{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8193c857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 11:55:09.947073: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 11:55:10.782572: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-12 11:55:10.782644: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-12 11:55:10.782650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import importlib\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import logomaker\n",
    "import time\n",
    "import tfomics\n",
    "from tfomics import impress, explain, moana\n",
    "\n",
    "from hominid_pipeline import utils, model_zoo, hominid, layers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20a44ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmaxND(a, axis=None):\n",
    "    amax = np.max(a, axis)\n",
    "    amin = np.min(a, axis)\n",
    "    return np.where(-amin > amax, amin, amax)\n",
    "\n",
    "\n",
    "def get_layer_output(model, index, X):\n",
    "    temp = tf.keras.Model(inputs=model.inputs, outputs=model.layers[index].output)\n",
    "    return temp.predict(X)\n",
    "\n",
    "def pearsonr(vector1, vector2):\n",
    "    m1 = np.mean(vector1)\n",
    "    m2 = np.mean(vector2)\n",
    "    \n",
    "    diff1 = vector1 - m1\n",
    "    diff2 = vector2 - m2\n",
    "    \n",
    "    top = np.sum(diff1 * diff2)\n",
    "    bottom = np.sum(np.power(diff1, 2)) * np.sum(np.power(diff2, 2))\n",
    "    bottom = np.sqrt(bottom)\n",
    "    \n",
    "    return top/bottom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_glifac(ax, correlation_matrix, filter_labels, vmin=-0.5, vmax=0.5):\n",
    "    ax.set_xticks(list(range(len(filter_labels))))\n",
    "    ax.set_yticks(list(range(len(filter_labels))))\n",
    "    ax.set_xticklabels(filter_labels, rotation=90)\n",
    "    ax.set_yticklabels(filter_labels)\n",
    "    c = ax.imshow(correlation_matrix, cmap='bwr_r', vmin=vmin, vmax=vmax)\n",
    "    return ax, c\n",
    "\n",
    "\n",
    "def correlation_matrix(model, c_index, mha_index, X, thresh=0.1, random_frac=0.5, limit=None, head_concat=np.max, symmetrize=absmaxND):\n",
    "    \n",
    "    \"\"\"\n",
    "    * model                  trained tensorflow model\n",
    "    * c_index                index of the convolutoinal layer (after pooling)\n",
    "    * mha_index              index of multi-head attention layer\n",
    "    * X                      test sequences\n",
    "    * thresh                 attention threshold\n",
    "    * random_frac            proportion of negative positions in the set of position interactions\n",
    "    * limit                  maximum number of position interactions processed; sometimes needed to avoid resource exhaustion\n",
    "    * head_concat            function for concatenating heads; e.g. np.max, np.mean\n",
    "    * symmetrize             function for symmetrizing the correlation matrix across diagonal\n",
    "    \"\"\"\n",
    "    \n",
    "    assert 0 <= random_frac < 1\n",
    "    \n",
    "    feature_maps = get_layer_output(model, c_index, X)\n",
    "    o, att_maps = get_layer_output(model, mha_index, X)\n",
    "    att_maps = head_concat(att_maps, axis=1)\n",
    "    \n",
    "    position_interactions = get_position_interactions(att_maps, thresh)\n",
    "    num_rands = int(random_frac/(1-random_frac))\n",
    "    random_interactions = [np.random.randint(len(att_maps), size=(num_rands, 1)), np.random.randint(att_maps.shape[1], size=(num_rands, 2))]\n",
    "    position_pairs = [np.vstack([position_interactions[0], random_interactions[0]]), np.vstack([position_interactions[1], random_interactions[1]])]\n",
    "    if limit is not None:\n",
    "        permutation = np.random.permutation(len(position_pairs[0]))\n",
    "        position_pairs = [position_pairs[0][permutation], position_pairs[1][permutation]]\n",
    "        position_pairs = [position_pairs[0][:limit], position_pairs[1][:limit]]\n",
    "    \n",
    "    filter_interactions = feature_maps[position_pairs].transpose([1, 2, 0])\n",
    "    correlation_matrix = correlation(filter_interactions[0], filter_interactions[1])\n",
    "    if symmetrize is not None:\n",
    "        correlation_matrix = symmetrize(np.array([correlation_matrix, correlation_matrix.transpose()]), axis=0)\n",
    "    correlation_matrix = np.nan_to_num(correlation_matrix)\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "    \n",
    "def get_position_interactions(att_maps, threshold=0.1):\n",
    "    position_interactions = np.array(np.where(att_maps >= threshold))\n",
    "    position_interactions = [position_interactions[[0]].transpose(), position_interactions[[1, 2]].transpose()]\n",
    "    return position_interactions\n",
    "    \n",
    "    \n",
    "def correlation(set1, set2, function=pearsonr):\n",
    "    combinations = np.indices(dimensions=(set1.shape[0], set2.shape[0])).transpose().reshape((-1, 2)).transpose()[::-1]\n",
    "    vector_mesh = [set1[combinations[0]], set2[combinations[1]]]\n",
    "    vector_mesh = np.array(vector_mesh).transpose([1, 0, 2])\n",
    "    correlations = []\n",
    "    for i in range(len(vector_mesh)):\n",
    "        r = function(vector_mesh[i][0], vector_mesh[i][1])\n",
    "        correlations.append(r)\n",
    "    correlations = np.array(correlations).reshape((len(set1), len(set2)))\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27b8b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/chandana/projects/hominid_pipeline/results\"\n",
    "hits = [\n",
    "    \"experiments/sweeps/tune_hominid_8f34a_00085_85_conv1_activation=exponential,conv1_attention_pool_size=23,conv1_batchnorm=True,conv1_channel_weight=se_2023-05-18_00-04-12\",\n",
    "    \"experiments/sweeps/tune_hominid_8f34a_00185_185_conv1_activation=relu,conv1_attention_pool_size=23,conv1_batchnorm=True,conv1_channel_weight=softconv_2023-05-18_06-11-38\",\n",
    "    \"experiments/model_variations/tune_hominid_8f34a_00058_58_conv1_activation=relu,conv1_attention_pool_size=30,conv1_batchnorm=False,conv1_channel_weight=softconv_2023-05-17_22-09-59/exponential\", # this one!\n",
    "    \"experiments/model_variations/tune_hominid_8f34a_00056_56_conv1_activation=relu,conv1_attention_pool_size=5,conv1_batchnorm=False,conv1_channel_weight=se,conv1__2023-05-17_22-05-53/exponential\",\n",
    "    \"experiments/model_variations/tune_hominid_8f34a_00058_58_conv1_activation=relu,conv1_attention_pool_size=30,conv1_batchnorm=False,conv1_channel_weight=softconv_2023-05-17_22-09-59/exponential/variations/variation_1\"    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c341e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chandana/projects/hominid_pipeline/results/experiments/sweeps/tune_hominid_8f34a_00185_185_conv1_activation=relu,conv1_attention_pool_size=23,conv1_batchnorm=True,conv1_channel_weight=softconv_2023-05-18_06-11-38'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 1\n",
    "save_path = f\"{working_dir}/{hits[index]}\"\n",
    "config_file = f\"{working_dir}/{hits[index]}/config.yaml\"\n",
    "config = hominid.load_config(config_file)\n",
    "\n",
    "tuner = hominid.HominidTuner(\n",
    "    config, \n",
    "    epochs=100, \n",
    "    tuning_mode=False, \n",
    "    save_path=save_path, \n",
    "    subsample=False\n",
    ")\n",
    "\n",
    "tuner.save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "776bbe6a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and dataset!\n",
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 249, 4)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1 (Conv1D)                 (None, 249, 96)      7392        ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 249, 96)      384         ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1_activation (Activation)  (None, 249, 96)      0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " softconv_conv (Conv1D)         (None, 249, 128)     12416       ['conv1_activation[0][0]']       \n",
      "                                                                                                  \n",
      " softconv_activation (Activatio  (None, 249, 128)    0           ['softconv_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv1_maxpool (MaxPooling1D)   (None, 31, 128)      0           ['softconv_activation[0][0]']    \n",
      "                                                                                                  \n",
      " conv1_dropout (Dropout)        (None, 31, 128)      0           ['conv1_maxpool[0][0]']          \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  ((None, 31, 192),   148032      ['conv1_dropout[0][0]',          \n",
      " dAttention)                     (None, 4, 31, 31))               'conv1_dropout[0][0]',          \n",
      "                                                                  'conv1_dropout[0][0]']          \n",
      "                                                                                                  \n",
      " mha_dropout (Dropout)          (None, 31, 192)      0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 5952)         0           ['mha_dropout[0][0]']            \n",
      "                                                                                                  \n",
      " dense_0 (Dense)                (None, 512)          3047936     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " bn_0 (BatchNormalization)      (None, 512)          2048        ['dense_0[0][0]']                \n",
      "                                                                                                  \n",
      " dense_activation_0 (Activation  (None, 512)         0           ['bn_0[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_dropout_0 (Dropout)      (None, 512)          0           ['dense_activation_0[0][0]']     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          262656      ['dense_dropout_0[0][0]']        \n",
      "                                                                                                  \n",
      " bn_1 (BatchNormalization)      (None, 512)          2048        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_activation_1 (Activation  (None, 512)         0           ['bn_1[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_dropout_1 (Dropout)      (None, 512)          0           ['dense_activation_1[0][0]']     \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 2)            1026        ['dense_dropout_1[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,483,938\n",
      "Trainable params: 3,481,698\n",
      "Non-trainable params: 2,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f31b821ab90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Loading model and dataset!\")\n",
    "\n",
    "x_test, y_test = tuner.data_processor.load_data(\"test\")\n",
    "\n",
    "# Build the model\n",
    "model = tuner.model_builder.build_model()\n",
    "\n",
    "model.compile(\n",
    "    tf.keras.optimizers.Adam(lr=0.001),\n",
    "    loss='mse',\n",
    "    metrics=[utils.Spearman, utils.pearson_r]\n",
    "    )\n",
    "print(model.summary())\n",
    "model.load_weights(f'{tuner.save_path}/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8b5557e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 2ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandana/miniforge3/envs/deepstarr/lib/python3.7/site-packages/ipykernel_launcher.py:65: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/home/chandana/miniforge3/envs/deepstarr/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 129.41733574867249\n"
     ]
    }
   ],
   "source": [
    "# for this model ONLY: challenges are that there is no pooling layer\n",
    "# so selected the concatenation layer for the conv_layer\n",
    "\n",
    "t1 = time.time()\n",
    "sample = x_test[:5000]\n",
    "lays = [type(i) for i in model.layers]\n",
    "c_index = lays.index(tf.keras.layers.MaxPool1D) # lays.index(keras.layers.core.tf_op_layer.TFOpLambda) #\n",
    "\n",
    "mha_index = lays.index(layers.MultiHeadAttention)\n",
    "correlation_map = correlation_matrix(\n",
    "                            model, \n",
    "                            c_index, \n",
    "                            mha_index, \n",
    "                            sample, \n",
    "                            thresh=0.1, \n",
    "                            random_frac=0.3, \n",
    "                            limit=150000\n",
    "                        )\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"Time taken: {t2-t1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34a0c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def absmaxND(a, axis=None):\n",
    "    amax = np.max(a, axis)\n",
    "    amin = np.min(a, axis)\n",
    "    return np.where(-amin > amax, amin, amax)\n",
    "\n",
    "\n",
    "def get_layer_output(model, index, X):\n",
    "    temp = tf.keras.Model(inputs=model.inputs, outputs=model.layers[index].output)\n",
    "    return temp.predict(X)\n",
    "\n",
    "\n",
    "def pearsonr(vector1, vector2):\n",
    "    diff1 = vector1 - np.mean(vector1)\n",
    "    diff2 = vector2 - np.mean(vector2)\n",
    "    top = np.sum(diff1 * diff2)\n",
    "    bottom = np.sqrt(np.sum(diff1 ** 2) * np.sum(diff2 ** 2))\n",
    "    return top / bottom\n",
    "\n",
    "\n",
    "def plot_glifac(ax, correlation_matrix, filter_labels, vmin=-0.5, vmax=0.5):\n",
    "    ax.set_xticks(np.arange(len(filter_labels)))\n",
    "    ax.set_yticks(np.arange(len(filter_labels)))\n",
    "    ax.set_xticklabels(filter_labels, rotation=90)\n",
    "    ax.set_yticklabels(filter_labels)\n",
    "    c = ax.imshow(correlation_matrix, cmap='bwr_r', vmin=vmin, vmax=vmax)\n",
    "    return ax, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b904e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _correlation_matrix(model, c_index, mha_index, X, thresh=0.1, random_frac=0.5, limit=None, head_concat=np.max,\n",
    "                       symmetrize=absmaxND):\n",
    "    assert 0 <= random_frac < 1\n",
    "\n",
    "    feature_maps = get_layer_output(model, c_index, X)\n",
    "    o, att_maps = get_layer_output(model, mha_index, X)\n",
    "    att_maps = head_concat(att_maps, axis=1)\n",
    "\n",
    "    position_interactions = _get_position_interactions(att_maps, thresh)\n",
    "    \n",
    "#     num_rands = int(random_frac / (1 - random_frac))\n",
    "#     random_interactions = [np.random.randint(len(att_maps), size=num_rands),\n",
    "#                            np.random.randint(att_maps.shape[1], size=(num_rands, 2))]\n",
    "    \n",
    "    \n",
    "#     print(num_rands)\n",
    "    \n",
    "#     position_pairs = [np.vstack([position_interactions[0], random_interactions[0]]),\n",
    "#                       np.vstack([position_interactions[1], random_interactions[1]])]\n",
    "#     if limit is not None:\n",
    "#         permutation = np.random.permutation(len(position_pairs[0]))\n",
    "#         position_pairs = [position_pairs[0][permutation], position_pairs[1][permutation]]\n",
    "#         position_pairs = [position_pairs[0][:limit], position_pairs[1][:limit]]\n",
    "\n",
    "#     filter_interactions = feature_maps[position_pairs].transpose([1, 2, 0])\n",
    "#     correlation_matrix = _correlation(filter_interactions[0], filter_interactions[1], pearsonr)\n",
    "#     if symmetrize is not None:\n",
    "#         correlation_matrix = symmetrize(np.array([correlation_matrix, correlation_matrix.transpose()]), axis=0)\n",
    "#     correlation_matrix = np.nan_to_num(correlation_matrix)\n",
    "\n",
    "#     return correlation_matrix\n",
    "    return 0\n",
    "\n",
    "\n",
    "def _get_position_interactions(att_maps, threshold=0.1):\n",
    "    position_interactions = np.argwhere(att_maps >= threshold)\n",
    "    position_interactions = [position_interactions[:, 0], position_interactions[:, 1:]]\n",
    "    return position_interactions\n",
    "\n",
    "\n",
    "def _correlation(set1, set2, function=pearsonr):\n",
    "    combinations = np.indices((set1.shape[0], set2.shape[0])).reshape((-1, 2))[:, ::-1]\n",
    "    vector_mesh = [set1[combinations[:, 0]], set2[combinations[:, 1]]]\n",
    "    correlations = np.array([function(vector_mesh[i][0], vector_mesh[i][1]) for i in range(len(vector_mesh))])\n",
    "    correlations = correlations.reshape((len(set1), len(set2)))\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e51e729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rands = int(random_frac/(1-random_frac))\n",
    "num_rands\n",
    "random_interactions = [np.random.randint(len(att_maps), size=(num_rands, 1)), \n",
    "                       np.random.randint(att_maps.shape[1], size=(num_rands, 2))]\n",
    "# position_pairs = [np.vstack([position_interactions[0], random_interactions[0]]), np.vstack([position_interactions[1], random_interactions[1]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9923cd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step\n",
      "157/157 [==============================] - 1s 3ms/step\n",
      "0\n",
      "Time taken: 1.400761604309082\n"
     ]
    }
   ],
   "source": [
    "# for this model ONLY: challenges are that there is no pooling layer\n",
    "# so selected the concatenation layer for the conv_layer\n",
    "\n",
    "t1 = time.time()\n",
    "sample = x_test[:5000]\n",
    "lays = [type(i) for i in model.layers]\n",
    "c_index = lays.index(tf.keras.layers.MaxPool1D) # lays.index(keras.layers.core.tf_op_layer.TFOpLambda) #\n",
    "\n",
    "mha_index = lays.index(layers.MultiHeadAttention)\n",
    "correlation_map_2 = _correlation_matrix(\n",
    "                            model, \n",
    "                            c_index, \n",
    "                            mha_index, \n",
    "                            sample, \n",
    "                            thresh=0.1, \n",
    "                            random_frac=0.3, \n",
    "                            limit=150000\n",
    "                        )\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"Time taken: {t2-t1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57879dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
