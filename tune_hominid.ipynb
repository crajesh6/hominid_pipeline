{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8758d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 10:27:16.729690: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-04 10:27:18.761006: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-04 10:27:18.761113: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-04 10:27:18.761121: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/chandana/miniforge3/envs/deepstarr/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: The module `ray.air.callbacks.wandb` has been moved to `ray.air.integrations.wandb` and the old location will be deprecated soon. Please adjust your imports to point to the new location. Example: Do a global search and replace `ray.air.callbacks.wandb` with `ray.air.integrations.wandb`.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import h5py, time\n",
    "import click\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from filelock import FileLock\n",
    "import json\n",
    "from ray.air import CheckpointConfig\n",
    "from ray import tune\n",
    "from ray.tune.integration.keras import TuneReportCallback\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.air.callbacks.wandb import WandbLoggerCallback\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scipy.stats import spearmanr\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "import model_zoo, utils\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562079c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c98776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "def Spearman(y_true, y_pred):\n",
    "     return ( tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32),\n",
    "                       tf.cast(y_true, tf.float32)], Tout = tf.float32) ) \n",
    "from keras import backend as K\n",
    "\n",
    "def pearson_r(y_true, y_pred):\n",
    "    # use smoothing for not resulting in NaN values\n",
    "    # pearson correlation coefficient\n",
    "    # https://github.com/WenYanger/Keras_Metrics\n",
    "    epsilon = 10e-5\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = K.sum(xm * ym)\n",
    "    x_square_sum = K.sum(xm * xm)\n",
    "    y_square_sum = K.sum(ym * ym)\n",
    "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
    "    r = r_num / (r_den + epsilon)\n",
    "    return K.mean(r)\n",
    "\n",
    "def load_deepstarr_data(\n",
    "        data_split: str,\n",
    "        data_dir='/home/chandana/projects/hominid_pipeline/data/deepstarr_data.h5',\n",
    "        subsample: bool = False\n",
    "    ) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"Load dataset\"\"\"\n",
    "\n",
    "    # load sequences and labels\n",
    "    with FileLock(os.path.expanduser(f\"{data_dir}.lock\")):\n",
    "        with h5py.File(data_dir, \"r\") as dataset:\n",
    "            x = np.array(dataset[f'x_{data_split}']).astype(np.float32)\n",
    "            y = np.array(dataset[f'y_{data_split}']).astype(np.float32).transpose()\n",
    "    if subsample:\n",
    "        if data_split == \"train\":\n",
    "            x = x[:80000]\n",
    "            y = y[:80000]\n",
    "        elif data_split == \"valid\":\n",
    "            x = x[:20000]\n",
    "            y = y[:20000]\n",
    "        else:\n",
    "            x = x[:10000]\n",
    "            y = y[:10000]\n",
    "    return x, y\n",
    "\n",
    "def hominid_pipeline(config):\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Load dataset\n",
    "    # ==============================================================================\n",
    "\n",
    "    x_train, y_train = load_deepstarr_data(\"train\", subsample=False)\n",
    "    x_valid, y_valid = load_deepstarr_data(\"valid\", subsample=False)\n",
    "    x_test, y_test = load_deepstarr_data(\"test\", subsample=False)        \n",
    "\n",
    "    N, L, A = x_train.shape\n",
    "    output_shape = y_train.shape[-1]\n",
    "\n",
    "    print(f\"Input shape: {N, L, A}. Output shape: {output_shape}\")\n",
    "\n",
    "    config[\"input_shape\"] = (L, A)\n",
    "    config[\"output_shape\"] = output_shape\n",
    "\n",
    "    print(output_shape)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Build model\n",
    "    # ==============================================================================\n",
    "\n",
    "    print(\"Building model...\")\n",
    "\n",
    "    model = model_zoo.base_model(**config)\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test, model\n",
    "\n",
    "def tune_hominid(config: dict):\n",
    "\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test, model = hominid_pipeline(config)\n",
    "\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(lr=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[Spearman, pearson_r]\n",
    "        )\n",
    "    model.summary()\n",
    "    \n",
    "    # Write to the Tune trial directory, not the shared working dir\n",
    "    tune_trial_dir = Path(session.get_trial_dir())\n",
    "\n",
    "    # train model\n",
    "    model.fit(\n",
    "          x_train, y_train,\n",
    "          epochs=60,\n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[\n",
    "              TuneReportCallback({\n",
    "                  \"pearson_r\": \"pearson_r\",\n",
    "                  \"val_pearson_r\": \"val_pearson_r\",              \n",
    "              }),\n",
    "        ]\n",
    "      )\n",
    "    model.save_weights(f'{tune_trial_dir}/weights')\n",
    "    \n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79fe199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"conv1_activation\": tune.choice([\"relu\"]),                     # activation on 1st layer conv \n",
    "    \"conv1_batchnorm\": tune.choice([False]),                       # batchnorm on 1st layer conv\n",
    "    \"conv1_channel_weight\": tune.choice([\"softconv\", \"se\", None]), # soft attention on channels (1st layer conv)\n",
    "    \"conv1_dropout\": 0.2,\n",
    "    \"conv1_filters\": tune.choice([64, 96, 128, 256, 512]), \n",
    "    \"conv1_kernel_size\": tune.choice([15, 19]),    \n",
    "    \"conv1_pool_type\": tune.choice([\"attention\", \"max_pool\"]),\n",
    "    \"conv1_max_pool\": tune.choice([4, 8, 10, 20]),                 # if conv1 pool = max pool\n",
    "    \"conv1_attention_pool_size\": tune.choice(range(40)),           # if conv1 pool = attention pool\n",
    "    \"conv1_type\": tune.choice([\"pw\", \"standard\"]),                 # additive vs pairwise 1st conv layer\n",
    "    \"dense_activation\": \"relu\",\n",
    "    \"dense_batchnorm\": True,\n",
    "    \"dense_dropout\": tune.choice([[0.3, 0.3], [0.4, 0.4], [0.5, 0.5]]), \n",
    "    \"dense_units\": tune.choice([[128, 128], [256, 128],[512, 256], [256, 256], [512, 512],[1024, 512]]),\n",
    "    \"mha_d_model\": tune.choice([96, 192]),\n",
    "    \"mha_dropout\": 0.1,\n",
    "    \"mha_head_type\": tune.choice([\"pool\", \"task_specific\"]),       # shared vs task specific atttention\n",
    "    \"mha_heads\": tune.choice([4, 8]),\n",
    "    \"mha_layernorm\": False,\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"output_shape\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f19f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization experiments with small initialization parameter (small standard deviations)\n",
    "# use a small initializer for all convolutions in dense layers (0.005, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058d639a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-05-04 10:32:35</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:23.69        </td></tr>\n",
       "<tr><td>Memory:      </td><td>472.7/503.8 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 320.000: None | Iter 80.000: None | Iter 20.000: None<br>Resources requested: 4.0/32 CPUs, 1.0/1 GPUs, 0.0/12.44 GiB heap, 0.0/6.22 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc                  </th><th>conv1_activation  </th><th style=\"text-align: right;\">   conv1_attention_pool\n",
       "_size</th><th>conv1_batchnorm  </th><th>conv1_channel_weight  </th><th style=\"text-align: right;\">  conv1_filters</th><th style=\"text-align: right;\">  conv1_kernel_size</th><th style=\"text-align: right;\">  conv1_max_pool</th><th>conv1_pool_type  </th><th>conv1_type  </th><th>dense_dropout  </th><th>dense_units  </th><th style=\"text-align: right;\">  mha_d_model</th><th>mha_head_type  </th><th style=\"text-align: right;\">  mha_heads</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  pearson_r</th><th style=\"text-align: right;\">  val_pearson_r</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_hominid_2db57_00000</td><td>RUNNING </td><td>143.48.44.146:1281183</td><td>relu              </td><td style=\"text-align: right;\"> 0</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">             96</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">              10</td><td>max_pool         </td><td>standard    </td><td>[0.3, 0.3]     </td><td>[512, 512]   </td><td style=\"text-align: right;\">          192</td><td>pool           </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         138.444</td><td style=\"text-align: right;\">   0.631274</td><td style=\"text-align: right;\">       0.563095</td></tr>\n",
       "<tr><td>tune_hominid_2db57_00001</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\"> 4</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">            128</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               4</td><td>attention        </td><td>pw          </td><td>[0.3, 0.3]     </td><td>[512, 256]   </td><td style=\"text-align: right;\">          192</td><td>pool           </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00002</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">35</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">             96</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">              20</td><td>max_pool         </td><td>standard    </td><td>[0.3, 0.3]     </td><td>[512, 256]   </td><td style=\"text-align: right;\">          192</td><td>pool           </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00003</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\"> 0</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">            512</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">               8</td><td>max_pool         </td><td>standard    </td><td>[0.3, 0.3]     </td><td>[512, 256]   </td><td style=\"text-align: right;\">           96</td><td>task_specific  </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00004</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">28</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">             96</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">               8</td><td>attention        </td><td>pw          </td><td>[0.5, 0.5]     </td><td>[128, 128]   </td><td style=\"text-align: right;\">           96</td><td>task_specific  </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00005</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\"> 8</td><td>False            </td><td>se                    </td><td style=\"text-align: right;\">             96</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">              10</td><td>max_pool         </td><td>pw          </td><td>[0.3, 0.3]     </td><td>[256, 256]   </td><td style=\"text-align: right;\">          192</td><td>task_specific  </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00006</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">27</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">            256</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               8</td><td>attention        </td><td>standard    </td><td>[0.5, 0.5]     </td><td>[256, 128]   </td><td style=\"text-align: right;\">          192</td><td>task_specific  </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00007</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">38</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">            256</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">               4</td><td>attention        </td><td>standard    </td><td>[0.5, 0.5]     </td><td>[256, 256]   </td><td style=\"text-align: right;\">           96</td><td>task_specific  </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00008</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\"> 4</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">             64</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               8</td><td>attention        </td><td>standard    </td><td>[0.4, 0.4]     </td><td>[256, 256]   </td><td style=\"text-align: right;\">          192</td><td>task_specific  </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00009</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">13</td><td>False            </td><td>se                    </td><td style=\"text-align: right;\">            512</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               4</td><td>attention        </td><td>pw          </td><td>[0.4, 0.4]     </td><td>[512, 256]   </td><td style=\"text-align: right;\">          192</td><td>task_specific  </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00010</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">38</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">            512</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">              10</td><td>attention        </td><td>pw          </td><td>[0.3, 0.3]     </td><td>[1024, 512]  </td><td style=\"text-align: right;\">           96</td><td>task_specific  </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00011</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\"> 6</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">            256</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">              10</td><td>max_pool         </td><td>standard    </td><td>[0.4, 0.4]     </td><td>[256, 128]   </td><td style=\"text-align: right;\">           96</td><td>task_specific  </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00012</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">28</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">             96</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">              10</td><td>attention        </td><td>pw          </td><td>[0.4, 0.4]     </td><td>[256, 256]   </td><td style=\"text-align: right;\">           96</td><td>pool           </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00013</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">16</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">            128</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               4</td><td>attention        </td><td>pw          </td><td>[0.3, 0.3]     </td><td>[256, 256]   </td><td style=\"text-align: right;\">          192</td><td>pool           </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00014</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">24</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">             64</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               8</td><td>attention        </td><td>pw          </td><td>[0.3, 0.3]     </td><td>[1024, 512]  </td><td style=\"text-align: right;\">          192</td><td>task_specific  </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00015</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">28</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">            512</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">              10</td><td>attention        </td><td>pw          </td><td>[0.5, 0.5]     </td><td>[1024, 512]  </td><td style=\"text-align: right;\">          192</td><td>task_specific  </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00016</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\"> 8</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">            256</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">              20</td><td>max_pool         </td><td>pw          </td><td>[0.4, 0.4]     </td><td>[1024, 512]  </td><td style=\"text-align: right;\">          192</td><td>task_specific  </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00017</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">26</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">             96</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               4</td><td>attention        </td><td>pw          </td><td>[0.5, 0.5]     </td><td>[128, 128]   </td><td style=\"text-align: right;\">          192</td><td>task_specific  </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00018</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">27</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">            256</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">               8</td><td>attention        </td><td>standard    </td><td>[0.4, 0.4]     </td><td>[512, 256]   </td><td style=\"text-align: right;\">          192</td><td>pool           </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00019</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">36</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">             64</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               4</td><td>max_pool         </td><td>pw          </td><td>[0.4, 0.4]     </td><td>[128, 128]   </td><td style=\"text-align: right;\">          192</td><td>pool           </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00020</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">33</td><td>False            </td><td>se                    </td><td style=\"text-align: right;\">            256</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">               4</td><td>attention        </td><td>standard    </td><td>[0.4, 0.4]     </td><td>[512, 512]   </td><td style=\"text-align: right;\">           96</td><td>pool           </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00021</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\"> 0</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">             96</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">              20</td><td>attention        </td><td>pw          </td><td>[0.5, 0.5]     </td><td>[256, 128]   </td><td style=\"text-align: right;\">           96</td><td>pool           </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00022</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">18</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">            512</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               8</td><td>attention        </td><td>standard    </td><td>[0.4, 0.4]     </td><td>[512, 256]   </td><td style=\"text-align: right;\">          192</td><td>pool           </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00023</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">16</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">            128</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               4</td><td>max_pool         </td><td>pw          </td><td>[0.5, 0.5]     </td><td>[512, 512]   </td><td style=\"text-align: right;\">           96</td><td>task_specific  </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00024</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">25</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">             96</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">               4</td><td>attention        </td><td>pw          </td><td>[0.4, 0.4]     </td><td>[512, 512]   </td><td style=\"text-align: right;\">           96</td><td>pool           </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00025</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">33</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">             96</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               4</td><td>max_pool         </td><td>pw          </td><td>[0.4, 0.4]     </td><td>[512, 256]   </td><td style=\"text-align: right;\">           96</td><td>pool           </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00026</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">28</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">            128</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">              20</td><td>attention        </td><td>pw          </td><td>[0.5, 0.5]     </td><td>[256, 256]   </td><td style=\"text-align: right;\">           96</td><td>task_specific  </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00027</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\"> 6</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">            256</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">              20</td><td>max_pool         </td><td>standard    </td><td>[0.5, 0.5]     </td><td>[512, 256]   </td><td style=\"text-align: right;\">           96</td><td>pool           </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00028</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">33</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">            512</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">              10</td><td>max_pool         </td><td>pw          </td><td>[0.5, 0.5]     </td><td>[256, 128]   </td><td style=\"text-align: right;\">          192</td><td>pool           </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00029</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\"> 2</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">            256</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">              10</td><td>max_pool         </td><td>pw          </td><td>[0.4, 0.4]     </td><td>[256, 128]   </td><td style=\"text-align: right;\">          192</td><td>pool           </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00030</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">12</td><td>False            </td><td>se                    </td><td style=\"text-align: right;\">            256</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               4</td><td>attention        </td><td>standard    </td><td>[0.3, 0.3]     </td><td>[512, 256]   </td><td style=\"text-align: right;\">          192</td><td>task_specific  </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00031</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">30</td><td>False            </td><td>se                    </td><td style=\"text-align: right;\">            512</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">              10</td><td>attention        </td><td>standard    </td><td>[0.4, 0.4]     </td><td>[1024, 512]  </td><td style=\"text-align: right;\">          192</td><td>task_specific  </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00032</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">29</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">             64</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">               4</td><td>attention        </td><td>pw          </td><td>[0.4, 0.4]     </td><td>[512, 256]   </td><td style=\"text-align: right;\">          192</td><td>pool           </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00033</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">19</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">            512</td><td style=\"text-align: right;\">                 15</td><td style=\"text-align: right;\">               8</td><td>attention        </td><td>pw          </td><td>[0.5, 0.5]     </td><td>[512, 512]   </td><td style=\"text-align: right;\">           96</td><td>task_specific  </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00034</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\">16</td><td>False            </td><td>                      </td><td style=\"text-align: right;\">            512</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">              20</td><td>max_pool         </td><td>pw          </td><td>[0.4, 0.4]     </td><td>[512, 512]   </td><td style=\"text-align: right;\">           96</td><td>task_specific  </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "<tr><td>tune_hominid_2db57_00035</td><td>PENDING </td><td>                     </td><td>relu              </td><td style=\"text-align: right;\"> 9</td><td>False            </td><td>softconv              </td><td style=\"text-align: right;\">             64</td><td style=\"text-align: right;\">                 19</td><td style=\"text-align: right;\">               8</td><td>max_pool         </td><td>pw          </td><td>[0.5, 0.5]     </td><td>[256, 128]   </td><td style=\"text-align: right;\">           96</td><td>pool           </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">           </td><td style=\"text-align: right;\">               </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 10:30:12,173\tINFO wandb.py:314 -- Already logged into W&B.\n",
      "\u001b[2m\u001b[36m(pid=1281183)\u001b[0m 2023-05-04 10:30:13.478811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=1281183)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=1281183)\u001b[0m 2023-05-04 10:30:14.310527: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=1281183)\u001b[0m 2023-05-04 10:30:14.310617: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=1281183)\u001b[0m 2023-05-04 10:30:14.310625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281186)\u001b[0m wandb: Currently logged in as: ckochath. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281186)\u001b[0m wandb: wandb version 0.15.1 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281186)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281186)\u001b[0m wandb: Tracking run with wandb version 0.13.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281186)\u001b[0m wandb: Run data is saved locally in /home/chandana/ray_results/tune_hominid_pipeline-full/tune_hominid_2db57_00000_0_conv1_activation=relu,conv1_attention_pool_size=0,conv1_batchnorm=False,conv1_channel_weight=None,conv1_2023-05-04_10-30-12/wandb/run-20230504_103014-2db57_00000\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281186)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281186)\u001b[0m wandb: Syncing run tune_hominid_2db57_00000\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281186)\u001b[0m wandb: â­ï¸ View project at https://wandb.ai/ckochath/raytune-hominid_pipeline-full\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281186)\u001b[0m wandb: ðŸš€ View run at https://wandb.ai/ckochath/raytune-hominid_pipeline-full/runs/2db57_00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Input shape: (402296, 249, 4). Output shape: 2\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2023-05-04 10:30:39.457006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2023-05-04 10:30:40.401608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14239 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m WARNING:tensorflow:From /home/chandana/miniforge3/envs/deepstarr/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2023-05-04 10:30:41.363164: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1602747264 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Model: \"model\"\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  input (InputLayer)             [(None, 249, 4)]     0           []                               \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  conv1 (Conv1D)                 (None, 249, 96)      7392        ['input[0][0]']                  \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  conv1_activation (Activation)  (None, 249, 96)      0           ['conv1[0][0]']                  \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  conv1_maxpool (MaxPooling1D)   (None, 24, 96)       0           ['conv1_activation[0][0]']       \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  conv1_dropout (Dropout)        (None, 24, 96)       0           ['conv1_maxpool[0][0]']          \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  multi_head_attention (MultiHea  ((None, 24, 192),   129600      ['conv1_dropout[0][0]',          \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  dAttention)                     (None, 8, 24, 24))               'conv1_dropout[0][0]',          \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                   'conv1_dropout[0][0]']          \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  mha_dropout (Dropout)          (None, 24, 192)      0           ['multi_head_attention[0][0]']   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  flatten (Flatten)              (None, 4608)         0           ['mha_dropout[0][0]']            \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  dense_0 (Dense)                (None, 512)          2359808     ['flatten[0][0]']                \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  bn_0 (BatchNormalization)      (None, 512)          2048        ['dense_0[0][0]']                \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  dense_activation_0 (Activation  (None, 512)         0           ['bn_0[0][0]']                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  )                                                                                                \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  dense_dropout_0 (Dropout)      (None, 512)          0           ['dense_activation_0[0][0]']     \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  dense_1 (Dense)                (None, 512)          262656      ['dense_dropout_0[0][0]']        \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  bn_1 (BatchNormalization)      (None, 512)          2048        ['dense_1[0][0]']                \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  dense_activation_1 (Activation  (None, 512)         0           ['bn_1[0][0]']                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  )                                                                                                \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  dense_dropout_1 (Dropout)      (None, 512)          0           ['dense_activation_1[0][0]']     \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m  output (Dense)                 (None, 2)            1026        ['dense_dropout_1[0][0]']        \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m                                                                                                   \n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Total params: 2,764,578\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Trainable params: 2,762,530\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Non-trainable params: 2,048\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m __________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2023-05-04 10:30:42.133454: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1602747264 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function Spearman at 0x7f87a4d37170> and will run it as-is.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Cause: Unknown node type <gast.gast.Import object at 0x7f86b437c5d0>\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function Spearman at 0x7f87a4d37170> and will run it as-is.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Cause: Unknown node type <gast.gast.Import object at 0x7f86b437c5d0>\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function pearson_r at 0x7f8ae4314ef0> and will run it as-is.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Cause: Unknown node type <gast.gast.Import object at 0x7f86b437c7d0>\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m WARNING:tensorflow:AutoGraph could not transform <function pearson_r at 0x7f8ae4314ef0> and will run it as-is.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Cause: Unknown node type <gast.gast.Import object at 0x7f86b437c7d0>\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2023-05-04 10:30:45.872538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2023-05-04 10:30:48.927208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2023-05-04 10:30:49.060708: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f819fafbed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2023-05-04 10:30:49.060773: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2023-05-04 10:30:49.086288: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m 2023-05-04 10:30:49.278444: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/3143 [..............................] - ETA: 36s - loss: 3.4937 - Spearman: 0.3185 - pearson_r: 0.1261    \n",
      "  12/3143 [..............................] - ETA: 36s - loss: 3.5503 - Spearman: 0.3119 - pearson_r: 0.0539\n",
      "  18/3143 [..............................] - ETA: 34s - loss: 3.5270 - Spearman: 0.3144 - pearson_r: 0.0564\n",
      "  28/3143 [..............................] - ETA: 33s - loss: 3.4376 - Spearman: 0.3278 - pearson_r: 0.0830\n",
      "  34/3143 [..............................] - ETA: 33s - loss: 3.4289 - Spearman: 0.3267 - pearson_r: 0.0778\n",
      "  44/3143 [..............................] - ETA: 33s - loss: 3.3408 - Spearman: 0.3335 - pearson_r: 0.0842\n",
      "  54/3143 [..............................] - ETA: 32s - loss: 3.2700 - Spearman: 0.3345 - pearson_r: 0.0837\n",
      "  65/3143 [..............................] - ETA: 32s - loss: 3.2449 - Spearman: 0.3345 - pearson_r: 0.0854\n",
      "  77/3143 [..............................] - ETA: 31s - loss: 3.2116 - Spearman: 0.3389 - pearson_r: 0.0901\n",
      "  89/3143 [..............................] - ETA: 31s - loss: 3.1795 - Spearman: 0.3411 - pearson_r: 0.0938\n",
      " 100/3143 [..............................] - ETA: 31s - loss: 3.1503 - Spearman: 0.3449 - pearson_r: 0.0970\n",
      " 111/3143 [>.............................] - ETA: 30s - loss: 3.0931 - Spearman: 0.3468 - pearson_r: 0.1034\n",
      " 123/3143 [>.............................] - ETA: 30s - loss: 3.0409 - Spearman: 0.3500 - pearson_r: 0.1096\n",
      " 129/3143 [>.............................] - ETA: 30s - loss: 3.0393 - Spearman: 0.3518 - pearson_r: 0.1127\n",
      " 135/3143 [>.............................] - ETA: 30s - loss: 3.0178 - Spearman: 0.3537 - pearson_r: 0.1154\n",
      " 145/3143 [>.............................] - ETA: 30s - loss: 2.9950 - Spearman: 0.3553 - pearson_r: 0.1178\n",
      " 151/3143 [>.............................] - ETA: 30s - loss: 2.9745 - Spearman: 0.3570 - pearson_r: 0.1219\n",
      " 163/3143 [>.............................] - ETA: 29s - loss: 2.9508 - Spearman: 0.3605 - pearson_r: 0.1278\n",
      " 173/3143 [>.............................] - ETA: 29s - loss: 2.9257 - Spearman: 0.3636 - pearson_r: 0.1331\n",
      " 183/3143 [>.............................] - ETA: 30s - loss: 2.9086 - Spearman: 0.3664 - pearson_r: 0.1365\n",
      " 193/3143 [>.............................] - ETA: 29s - loss: 2.8924 - Spearman: 0.3685 - pearson_r: 0.1394\n",
      " 203/3143 [>.............................] - ETA: 30s - loss: 2.8734 - Spearman: 0.3705 - pearson_r: 0.1413\n",
      " 213/3143 [=>............................] - ETA: 29s - loss: 2.8539 - Spearman: 0.3752 - pearson_r: 0.1475\n",
      " 224/3143 [=>............................] - ETA: 29s - loss: 2.8311 - Spearman: 0.3796 - pearson_r: 0.1527\n",
      " 235/3143 [=>............................] - ETA: 29s - loss: 2.8066 - Spearman: 0.3845 - pearson_r: 0.1583\n",
      " 245/3143 [=>............................] - ETA: 29s - loss: 2.7935 - Spearman: 0.3883 - pearson_r: 0.1634\n",
      " 256/3143 [=>............................] - ETA: 29s - loss: 2.7820 - Spearman: 0.3935 - pearson_r: 0.1696\n",
      " 267/3143 [=>............................] - ETA: 29s - loss: 2.7688 - Spearman: 0.3971 - pearson_r: 0.1742\n",
      " 278/3143 [=>............................] - ETA: 29s - loss: 2.7539 - Spearman: 0.4012 - pearson_r: 0.1793\n",
      " 289/3143 [=>............................] - ETA: 29s - loss: 2.7392 - Spearman: 0.4038 - pearson_r: 0.1834\n",
      " 301/3143 [=>............................] - ETA: 28s - loss: 2.7251 - Spearman: 0.4079 - pearson_r: 0.1888\n",
      " 307/3143 [=>............................] - ETA: 28s - loss: 2.7174 - Spearman: 0.4091 - pearson_r: 0.1909\n",
      " 319/3143 [==>...........................] - ETA: 28s - loss: 2.6998 - Spearman: 0.4122 - pearson_r: 0.1957\n",
      " 331/3143 [==>...........................] - ETA: 28s - loss: 2.6924 - Spearman: 0.4162 - pearson_r: 0.2007\n",
      " 342/3143 [==>...........................] - ETA: 28s - loss: 2.6791 - Spearman: 0.4197 - pearson_r: 0.2058\n",
      " 354/3143 [==>...........................] - ETA: 28s - loss: 2.6703 - Spearman: 0.4230 - pearson_r: 0.2115\n",
      " 365/3143 [==>...........................] - ETA: 28s - loss: 2.6545 - Spearman: 0.4267 - pearson_r: 0.2175\n",
      " 376/3143 [==>...........................] - ETA: 27s - loss: 2.6408 - Spearman: 0.4298 - pearson_r: 0.2222\n",
      " 386/3143 [==>...........................] - ETA: 27s - loss: 2.6366 - Spearman: 0.4320 - pearson_r: 0.2264\n",
      " 396/3143 [==>...........................] - ETA: 27s - loss: 2.6276 - Spearman: 0.4341 - pearson_r: 0.2297\n",
      " 406/3143 [==>...........................] - ETA: 27s - loss: 2.6211 - Spearman: 0.4355 - pearson_r: 0.2329\n",
      " 416/3143 [==>...........................] - ETA: 27s - loss: 2.6140 - Spearman: 0.4373 - pearson_r: 0.2365\n",
      " 426/3143 [===>..........................] - ETA: 27s - loss: 2.6050 - Spearman: 0.4390 - pearson_r: 0.2399\n",
      " 436/3143 [===>..........................] - ETA: 27s - loss: 2.5986 - Spearman: 0.4408 - pearson_r: 0.2435\n",
      " 447/3143 [===>..........................] - ETA: 27s - loss: 2.5886 - Spearman: 0.4429 - pearson_r: 0.2475\n",
      " 457/3143 [===>..........................] - ETA: 27s - loss: 2.5791 - Spearman: 0.4450 - pearson_r: 0.2513\n",
      " 467/3143 [===>..........................] - ETA: 27s - loss: 2.5739 - Spearman: 0.4466 - pearson_r: 0.2547\n",
      " 478/3143 [===>..........................] - ETA: 26s - loss: 2.5618 - Spearman: 0.4485 - pearson_r: 0.2590\n",
      " 488/3143 [===>..........................] - ETA: 26s - loss: 2.5547 - Spearman: 0.4503 - pearson_r: 0.2624\n",
      " 498/3143 [===>..........................] - ETA: 26s - loss: 2.5455 - Spearman: 0.4521 - pearson_r: 0.2659\n",
      " 508/3143 [===>..........................] - ETA: 26s - loss: 2.5350 - Spearman: 0.4534 - pearson_r: 0.2688\n",
      " 518/3143 [===>..........................] - ETA: 26s - loss: 2.5274 - Spearman: 0.4548 - pearson_r: 0.2720\n",
      " 529/3143 [====>.........................] - ETA: 26s - loss: 2.5183 - Spearman: 0.4568 - pearson_r: 0.2762\n",
      " 539/3143 [====>.........................] - ETA: 26s - loss: 2.5095 - Spearman: 0.4582 - pearson_r: 0.2786\n",
      " 549/3143 [====>.........................] - ETA: 26s - loss: 2.4988 - Spearman: 0.4591 - pearson_r: 0.2820\n",
      " 559/3143 [====>.........................] - ETA: 26s - loss: 2.4915 - Spearman: 0.4608 - pearson_r: 0.2851\n",
      " 570/3143 [====>.........................] - ETA: 26s - loss: 2.4836 - Spearman: 0.4627 - pearson_r: 0.2886\n",
      " 581/3143 [====>.........................] - ETA: 26s - loss: 2.4740 - Spearman: 0.4647 - pearson_r: 0.2919\n",
      " 591/3143 [====>.........................] - ETA: 25s - loss: 2.4657 - Spearman: 0.4658 - pearson_r: 0.2939\n",
      " 601/3143 [====>.........................] - ETA: 25s - loss: 2.4590 - Spearman: 0.4663 - pearson_r: 0.2962\n",
      " 611/3143 [====>.........................] - ETA: 25s - loss: 2.4524 - Spearman: 0.4677 - pearson_r: 0.2990\n",
      " 621/3143 [====>.........................] - ETA: 25s - loss: 2.4462 - Spearman: 0.4689 - pearson_r: 0.3012\n",
      " 630/3143 [=====>........................] - ETA: 25s - loss: 2.4410 - Spearman: 0.4696 - pearson_r: 0.3030\n",
      " 640/3143 [=====>........................] - ETA: 25s - loss: 2.4345 - Spearman: 0.4706 - pearson_r: 0.3054\n",
      " 650/3143 [=====>........................] - ETA: 25s - loss: 2.4296 - Spearman: 0.4716 - pearson_r: 0.3073\n",
      " 660/3143 [=====>........................] - ETA: 25s - loss: 2.4249 - Spearman: 0.4723 - pearson_r: 0.3091\n",
      " 670/3143 [=====>........................] - ETA: 25s - loss: 2.4183 - Spearman: 0.4730 - pearson_r: 0.3111\n",
      " 680/3143 [=====>........................] - ETA: 25s - loss: 2.4114 - Spearman: 0.4741 - pearson_r: 0.3136\n",
      " 691/3143 [=====>........................] - ETA: 25s - loss: 2.4037 - Spearman: 0.4757 - pearson_r: 0.3169\n",
      " 703/3143 [=====>........................] - ETA: 24s - loss: 2.3962 - Spearman: 0.4773 - pearson_r: 0.3192\n",
      " 715/3143 [=====>........................] - ETA: 24s - loss: 2.3908 - Spearman: 0.4784 - pearson_r: 0.3214\n",
      " 721/3143 [=====>........................] - ETA: 24s - loss: 2.3879 - Spearman: 0.4788 - pearson_r: 0.3226\n",
      " 727/3143 [=====>........................] - ETA: 24s - loss: 2.3852 - Spearman: 0.4792 - pearson_r: 0.3233\n",
      " 733/3143 [=====>........................] - ETA: 24s - loss: 2.3827 - Spearman: 0.4799 - pearson_r: 0.3245\n",
      " 745/3143 [======>.......................] - ETA: 24s - loss: 2.3763 - Spearman: 0.4809 - pearson_r: 0.3268\n",
      " 756/3143 [======>.......................] - ETA: 24s - loss: 2.3713 - Spearman: 0.4821 - pearson_r: 0.3288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 766/3143 [======>.......................] - ETA: 24s - loss: 2.3653 - Spearman: 0.4830 - pearson_r: 0.3308\n",
      " 778/3143 [======>.......................] - ETA: 24s - loss: 2.3600 - Spearman: 0.4841 - pearson_r: 0.3334\n",
      " 789/3143 [======>.......................] - ETA: 23s - loss: 2.3528 - Spearman: 0.4854 - pearson_r: 0.3359\n",
      " 801/3143 [======>.......................] - ETA: 23s - loss: 2.3467 - Spearman: 0.4864 - pearson_r: 0.3380\n",
      " 812/3143 [======>.......................] - ETA: 23s - loss: 2.3409 - Spearman: 0.4875 - pearson_r: 0.3403\n",
      " 822/3143 [======>.......................] - ETA: 23s - loss: 2.3358 - Spearman: 0.4885 - pearson_r: 0.3425\n",
      " 832/3143 [======>.......................] - ETA: 23s - loss: 2.3316 - Spearman: 0.4896 - pearson_r: 0.3444\n",
      " 843/3143 [=======>......................] - ETA: 23s - loss: 2.3271 - Spearman: 0.4908 - pearson_r: 0.3464\n",
      " 854/3143 [=======>......................] - ETA: 23s - loss: 2.3235 - Spearman: 0.4916 - pearson_r: 0.3477\n",
      " 859/3143 [=======>......................] - ETA: 23s - loss: 2.3216 - Spearman: 0.4920 - pearson_r: 0.3486\n",
      " 865/3143 [=======>......................] - ETA: 23s - loss: 2.3211 - Spearman: 0.4925 - pearson_r: 0.3497\n",
      " 871/3143 [=======>......................] - ETA: 23s - loss: 2.3180 - Spearman: 0.4932 - pearson_r: 0.3510\n",
      " 883/3143 [=======>......................] - ETA: 22s - loss: 2.3105 - Spearman: 0.4940 - pearson_r: 0.3527\n",
      " 895/3143 [=======>......................] - ETA: 22s - loss: 2.3062 - Spearman: 0.4946 - pearson_r: 0.3543\n",
      " 906/3143 [=======>......................] - ETA: 22s - loss: 2.3003 - Spearman: 0.4952 - pearson_r: 0.3557\n",
      " 917/3143 [=======>......................] - ETA: 22s - loss: 2.2972 - Spearman: 0.4959 - pearson_r: 0.3571\n",
      " 927/3143 [=======>......................] - ETA: 22s - loss: 2.2948 - Spearman: 0.4967 - pearson_r: 0.3586\n",
      " 938/3143 [=======>......................] - ETA: 22s - loss: 2.2902 - Spearman: 0.4977 - pearson_r: 0.3605\n",
      " 949/3143 [========>.....................] - ETA: 22s - loss: 2.2858 - Spearman: 0.4984 - pearson_r: 0.3621\n",
      " 959/3143 [========>.....................] - ETA: 22s - loss: 2.2820 - Spearman: 0.4993 - pearson_r: 0.3639\n",
      " 964/3143 [========>.....................] - ETA: 22s - loss: 2.2796 - Spearman: 0.4997 - pearson_r: 0.3645\n",
      " 969/3143 [========>.....................] - ETA: 22s - loss: 2.2772 - Spearman: 0.5001 - pearson_r: 0.3652\n",
      " 974/3143 [========>.....................] - ETA: 22s - loss: 2.2741 - Spearman: 0.5005 - pearson_r: 0.3660\n",
      " 985/3143 [========>.....................] - ETA: 21s - loss: 2.2703 - Spearman: 0.5011 - pearson_r: 0.3671\n",
      " 996/3143 [========>.....................] - ETA: 21s - loss: 2.2669 - Spearman: 0.5020 - pearson_r: 0.3685\n",
      "1007/3143 [========>.....................] - ETA: 21s - loss: 2.2615 - Spearman: 0.5026 - pearson_r: 0.3700\n",
      "1018/3143 [========>.....................] - ETA: 21s - loss: 2.2558 - Spearman: 0.5033 - pearson_r: 0.3717\n",
      "1028/3143 [========>.....................] - ETA: 21s - loss: 2.2520 - Spearman: 0.5040 - pearson_r: 0.3728\n",
      "1038/3143 [========>.....................] - ETA: 21s - loss: 2.2476 - Spearman: 0.5045 - pearson_r: 0.3741\n",
      "1048/3143 [=========>....................] - ETA: 21s - loss: 2.2436 - Spearman: 0.5054 - pearson_r: 0.3757\n",
      "1058/3143 [=========>....................] - ETA: 21s - loss: 2.2395 - Spearman: 0.5063 - pearson_r: 0.3772\n",
      "1069/3143 [=========>....................] - ETA: 21s - loss: 2.2349 - Spearman: 0.5070 - pearson_r: 0.3785\n",
      "1081/3143 [=========>....................] - ETA: 20s - loss: 2.2325 - Spearman: 0.5077 - pearson_r: 0.3800\n",
      "1091/3143 [=========>....................] - ETA: 20s - loss: 2.2296 - Spearman: 0.5083 - pearson_r: 0.3808\n",
      "1101/3143 [=========>....................] - ETA: 20s - loss: 2.2261 - Spearman: 0.5087 - pearson_r: 0.3818\n",
      "1111/3143 [=========>....................] - ETA: 20s - loss: 2.2225 - Spearman: 0.5093 - pearson_r: 0.3829\n",
      "1122/3143 [=========>....................] - ETA: 20s - loss: 2.2195 - Spearman: 0.5097 - pearson_r: 0.3841\n",
      "1133/3143 [=========>....................] - ETA: 20s - loss: 2.2153 - Spearman: 0.5105 - pearson_r: 0.3857\n",
      "1139/3143 [=========>....................] - ETA: 20s - loss: 2.2136 - Spearman: 0.5108 - pearson_r: 0.3864\n",
      "1150/3143 [=========>....................] - ETA: 20s - loss: 2.2092 - Spearman: 0.5115 - pearson_r: 0.3879\n",
      "1162/3143 [==========>...................] - ETA: 20s - loss: 2.2052 - Spearman: 0.5122 - pearson_r: 0.3894\n",
      "1173/3143 [==========>...................] - ETA: 20s - loss: 2.2024 - Spearman: 0.5127 - pearson_r: 0.3905\n",
      "1183/3143 [==========>...................] - ETA: 19s - loss: 2.2002 - Spearman: 0.5131 - pearson_r: 0.3915\n",
      "1194/3143 [==========>...................] - ETA: 19s - loss: 2.1985 - Spearman: 0.5137 - pearson_r: 0.3929\n",
      "1205/3143 [==========>...................] - ETA: 19s - loss: 2.1968 - Spearman: 0.5141 - pearson_r: 0.3936\n",
      "1215/3143 [==========>...................] - ETA: 19s - loss: 2.1944 - Spearman: 0.5147 - pearson_r: 0.3946\n",
      "1225/3143 [==========>...................] - ETA: 19s - loss: 2.1928 - Spearman: 0.5153 - pearson_r: 0.3955\n",
      "1236/3143 [==========>...................] - ETA: 19s - loss: 2.1894 - Spearman: 0.5160 - pearson_r: 0.3967\n",
      "1247/3143 [==========>...................] - ETA: 19s - loss: 2.1861 - Spearman: 0.5167 - pearson_r: 0.3980\n",
      "1259/3143 [===========>..................] - ETA: 19s - loss: 2.1823 - Spearman: 0.5173 - pearson_r: 0.3993\n",
      "1270/3143 [===========>..................] - ETA: 19s - loss: 2.1789 - Spearman: 0.5179 - pearson_r: 0.4004\n",
      "1281/3143 [===========>..................] - ETA: 18s - loss: 2.1767 - Spearman: 0.5184 - pearson_r: 0.4013\n",
      "1291/3143 [===========>..................] - ETA: 18s - loss: 2.1743 - Spearman: 0.5189 - pearson_r: 0.4023\n",
      "1297/3143 [===========>..................] - ETA: 18s - loss: 2.1729 - Spearman: 0.5192 - pearson_r: 0.4028\n",
      "1308/3143 [===========>..................] - ETA: 18s - loss: 2.1697 - Spearman: 0.5196 - pearson_r: 0.4038\n",
      "1313/3143 [===========>..................] - ETA: 18s - loss: 2.1683 - Spearman: 0.5199 - pearson_r: 0.4043\n",
      "1319/3143 [===========>..................] - ETA: 18s - loss: 2.1664 - Spearman: 0.5200 - pearson_r: 0.4047\n",
      "1324/3143 [===========>..................] - ETA: 18s - loss: 2.1652 - Spearman: 0.5202 - pearson_r: 0.4051\n",
      "1334/3143 [===========>..................] - ETA: 18s - loss: 2.1620 - Spearman: 0.5206 - pearson_r: 0.4061\n",
      "1344/3143 [===========>..................] - ETA: 18s - loss: 2.1596 - Spearman: 0.5210 - pearson_r: 0.4070\n",
      "1349/3143 [===========>..................] - ETA: 18s - loss: 2.1581 - Spearman: 0.5212 - pearson_r: 0.4073\n",
      "1357/3143 [===========>..................] - ETA: 18s - loss: 2.1556 - Spearman: 0.5216 - pearson_r: 0.4081\n",
      "1365/3143 [============>.................] - ETA: 18s - loss: 2.1539 - Spearman: 0.5220 - pearson_r: 0.4087\n",
      "1369/3143 [============>.................] - ETA: 18s - loss: 2.1534 - Spearman: 0.5223 - pearson_r: 0.4091\n",
      "1373/3143 [============>.................] - ETA: 18s - loss: 2.1521 - Spearman: 0.5225 - pearson_r: 0.4095\n",
      "1377/3143 [============>.................] - ETA: 18s - loss: 2.1506 - Spearman: 0.5227 - pearson_r: 0.4101\n",
      "1384/3143 [============>.................] - ETA: 18s - loss: 2.1494 - Spearman: 0.5231 - pearson_r: 0.4107\n",
      "1391/3143 [============>.................] - ETA: 18s - loss: 2.1482 - Spearman: 0.5233 - pearson_r: 0.4114\n",
      "1395/3143 [============>.................] - ETA: 18s - loss: 2.1475 - Spearman: 0.5236 - pearson_r: 0.4118\n",
      "1399/3143 [============>.................] - ETA: 18s - loss: 2.1468 - Spearman: 0.5237 - pearson_r: 0.4121\n",
      "1403/3143 [============>.................] - ETA: 18s - loss: 2.1458 - Spearman: 0.5239 - pearson_r: 0.4125\n",
      "1411/3143 [============>.................] - ETA: 18s - loss: 2.1442 - Spearman: 0.5242 - pearson_r: 0.4130\n",
      "1418/3143 [============>.................] - ETA: 18s - loss: 2.1418 - Spearman: 0.5246 - pearson_r: 0.4138\n",
      "1426/3143 [============>.................] - ETA: 17s - loss: 2.1406 - Spearman: 0.5250 - pearson_r: 0.4145\n",
      "1434/3143 [============>.................] - ETA: 17s - loss: 2.1389 - Spearman: 0.5254 - pearson_r: 0.4152\n",
      "1438/3143 [============>.................] - ETA: 17s - loss: 2.1377 - Spearman: 0.5255 - pearson_r: 0.4154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446/3143 [============>.................] - ETA: 17s - loss: 2.1362 - Spearman: 0.5258 - pearson_r: 0.4161\n",
      "1454/3143 [============>.................] - ETA: 17s - loss: 2.1340 - Spearman: 0.5262 - pearson_r: 0.4167\n",
      "1462/3143 [============>.................] - ETA: 17s - loss: 2.1316 - Spearman: 0.5266 - pearson_r: 0.4176\n",
      "1466/3143 [============>.................] - ETA: 17s - loss: 2.1312 - Spearman: 0.5269 - pearson_r: 0.4180\n",
      "1474/3143 [=============>................] - ETA: 17s - loss: 2.1291 - Spearman: 0.5272 - pearson_r: 0.4187\n",
      "1481/3143 [=============>................] - ETA: 17s - loss: 2.1276 - Spearman: 0.5274 - pearson_r: 0.4191\n",
      "1489/3143 [=============>................] - ETA: 17s - loss: 2.1267 - Spearman: 0.5276 - pearson_r: 0.4196\n",
      "1493/3143 [=============>................] - ETA: 17s - loss: 2.1259 - Spearman: 0.5278 - pearson_r: 0.4199\n",
      "1497/3143 [=============>................] - ETA: 17s - loss: 2.1249 - Spearman: 0.5281 - pearson_r: 0.4203\n",
      "1504/3143 [=============>................] - ETA: 17s - loss: 2.1232 - Spearman: 0.5285 - pearson_r: 0.4208\n",
      "1508/3143 [=============>................] - ETA: 17s - loss: 2.1227 - Spearman: 0.5287 - pearson_r: 0.4212\n",
      "1513/3143 [=============>................] - ETA: 17s - loss: 2.1213 - Spearman: 0.5289 - pearson_r: 0.4216\n",
      "1518/3143 [=============>................] - ETA: 17s - loss: 2.1205 - Spearman: 0.5291 - pearson_r: 0.4221\n",
      "1523/3143 [=============>................] - ETA: 17s - loss: 2.1191 - Spearman: 0.5294 - pearson_r: 0.4226\n",
      "1529/3143 [=============>................] - ETA: 17s - loss: 2.1174 - Spearman: 0.5298 - pearson_r: 0.4232\n",
      "1534/3143 [=============>................] - ETA: 17s - loss: 2.1164 - Spearman: 0.5301 - pearson_r: 0.4236\n",
      "1539/3143 [=============>................] - ETA: 17s - loss: 2.1156 - Spearman: 0.5302 - pearson_r: 0.4240\n",
      "1551/3143 [=============>................] - ETA: 17s - loss: 2.1130 - Spearman: 0.5307 - pearson_r: 0.4249\n",
      "1561/3143 [=============>................] - ETA: 16s - loss: 2.1110 - Spearman: 0.5308 - pearson_r: 0.4254\n",
      "1571/3143 [=============>................] - ETA: 16s - loss: 2.1090 - Spearman: 0.5313 - pearson_r: 0.4262\n",
      "1583/3143 [==============>...............] - ETA: 16s - loss: 2.1065 - Spearman: 0.5317 - pearson_r: 0.4270\n",
      "1594/3143 [==============>...............] - ETA: 16s - loss: 2.1044 - Spearman: 0.5322 - pearson_r: 0.4278\n",
      "1604/3143 [==============>...............] - ETA: 16s - loss: 2.1027 - Spearman: 0.5325 - pearson_r: 0.4286\n",
      "1614/3143 [==============>...............] - ETA: 16s - loss: 2.1010 - Spearman: 0.5327 - pearson_r: 0.4292\n",
      "1619/3143 [==============>...............] - ETA: 16s - loss: 2.1000 - Spearman: 0.5329 - pearson_r: 0.4296\n",
      "1624/3143 [==============>...............] - ETA: 16s - loss: 2.0992 - Spearman: 0.5331 - pearson_r: 0.4300\n",
      "1629/3143 [==============>...............] - ETA: 16s - loss: 2.0988 - Spearman: 0.5332 - pearson_r: 0.4303\n",
      "1635/3143 [==============>...............] - ETA: 16s - loss: 2.0976 - Spearman: 0.5334 - pearson_r: 0.4307\n",
      "1641/3143 [==============>...............] - ETA: 16s - loss: 2.0963 - Spearman: 0.5336 - pearson_r: 0.4311\n",
      "1651/3143 [==============>...............] - ETA: 15s - loss: 2.0952 - Spearman: 0.5340 - pearson_r: 0.4318\n",
      "1663/3143 [==============>...............] - ETA: 15s - loss: 2.0923 - Spearman: 0.5345 - pearson_r: 0.4327\n",
      "1673/3143 [==============>...............] - ETA: 15s - loss: 2.0908 - Spearman: 0.5347 - pearson_r: 0.4334\n",
      "1683/3143 [===============>..............] - ETA: 15s - loss: 2.0886 - Spearman: 0.5350 - pearson_r: 0.4340\n",
      "1694/3143 [===============>..............] - ETA: 15s - loss: 2.0869 - Spearman: 0.5353 - pearson_r: 0.4347\n",
      "1704/3143 [===============>..............] - ETA: 15s - loss: 2.0849 - Spearman: 0.5357 - pearson_r: 0.4354\n",
      "1714/3143 [===============>..............] - ETA: 15s - loss: 2.0826 - Spearman: 0.5359 - pearson_r: 0.4361\n",
      "1725/3143 [===============>..............] - ETA: 15s - loss: 2.0805 - Spearman: 0.5364 - pearson_r: 0.4369\n",
      "1735/3143 [===============>..............] - ETA: 15s - loss: 2.0782 - Spearman: 0.5368 - pearson_r: 0.4376\n",
      "1745/3143 [===============>..............] - ETA: 14s - loss: 2.0770 - Spearman: 0.5371 - pearson_r: 0.4382\n",
      "1755/3143 [===============>..............] - ETA: 14s - loss: 2.0756 - Spearman: 0.5374 - pearson_r: 0.4389\n",
      "1766/3143 [===============>..............] - ETA: 14s - loss: 2.0730 - Spearman: 0.5377 - pearson_r: 0.4396\n",
      "1777/3143 [===============>..............] - ETA: 14s - loss: 2.0713 - Spearman: 0.5381 - pearson_r: 0.4404\n",
      "1788/3143 [================>.............] - ETA: 14s - loss: 2.0694 - Spearman: 0.5385 - pearson_r: 0.4410\n",
      "1798/3143 [================>.............] - ETA: 14s - loss: 2.0681 - Spearman: 0.5388 - pearson_r: 0.4416\n",
      "1808/3143 [================>.............] - ETA: 14s - loss: 2.0668 - Spearman: 0.5390 - pearson_r: 0.4421\n",
      "1818/3143 [================>.............] - ETA: 14s - loss: 2.0651 - Spearman: 0.5391 - pearson_r: 0.4425\n",
      "1829/3143 [================>.............] - ETA: 14s - loss: 2.0625 - Spearman: 0.5393 - pearson_r: 0.4432\n",
      "1839/3143 [================>.............] - ETA: 13s - loss: 2.0613 - Spearman: 0.5396 - pearson_r: 0.4438\n",
      "1850/3143 [================>.............] - ETA: 13s - loss: 2.0601 - Spearman: 0.5399 - pearson_r: 0.4443\n",
      "1861/3143 [================>.............] - ETA: 13s - loss: 2.0587 - Spearman: 0.5401 - pearson_r: 0.4449\n",
      "1867/3143 [================>.............] - ETA: 13s - loss: 2.0582 - Spearman: 0.5402 - pearson_r: 0.4452\n",
      "1878/3143 [================>.............] - ETA: 13s - loss: 2.0561 - Spearman: 0.5407 - pearson_r: 0.4460\n",
      "1888/3143 [=================>............] - ETA: 13s - loss: 2.0540 - Spearman: 0.5410 - pearson_r: 0.4466\n",
      "1899/3143 [=================>............] - ETA: 13s - loss: 2.0525 - Spearman: 0.5413 - pearson_r: 0.4473\n",
      "1909/3143 [=================>............] - ETA: 13s - loss: 2.0514 - Spearman: 0.5416 - pearson_r: 0.4479\n",
      "1920/3143 [=================>............] - ETA: 13s - loss: 2.0500 - Spearman: 0.5417 - pearson_r: 0.4484\n",
      "1931/3143 [=================>............] - ETA: 12s - loss: 2.0479 - Spearman: 0.5419 - pearson_r: 0.4490\n",
      "1941/3143 [=================>............] - ETA: 12s - loss: 2.0459 - Spearman: 0.5421 - pearson_r: 0.4496\n",
      "1952/3143 [=================>............] - ETA: 12s - loss: 2.0443 - Spearman: 0.5425 - pearson_r: 0.4503\n",
      "1962/3143 [=================>............] - ETA: 12s - loss: 2.0426 - Spearman: 0.5427 - pearson_r: 0.4510\n",
      "1974/3143 [=================>............] - ETA: 12s - loss: 2.0413 - Spearman: 0.5430 - pearson_r: 0.4516\n",
      "1984/3143 [=================>............] - ETA: 12s - loss: 2.0400 - Spearman: 0.5434 - pearson_r: 0.4523\n",
      "1994/3143 [==================>...........] - ETA: 12s - loss: 2.0384 - Spearman: 0.5437 - pearson_r: 0.4530\n",
      "2000/3143 [==================>...........] - ETA: 12s - loss: 2.0376 - Spearman: 0.5438 - pearson_r: 0.4533\n",
      "2012/3143 [==================>...........] - ETA: 12s - loss: 2.0360 - Spearman: 0.5440 - pearson_r: 0.4540\n",
      "2023/3143 [==================>...........] - ETA: 11s - loss: 2.0343 - Spearman: 0.5441 - pearson_r: 0.4545\n",
      "2033/3143 [==================>...........] - ETA: 11s - loss: 2.0331 - Spearman: 0.5444 - pearson_r: 0.4551\n",
      "2043/3143 [==================>...........] - ETA: 11s - loss: 2.0321 - Spearman: 0.5447 - pearson_r: 0.4557\n",
      "2054/3143 [==================>...........] - ETA: 11s - loss: 2.0304 - Spearman: 0.5451 - pearson_r: 0.4564\n",
      "2066/3143 [==================>...........] - ETA: 11s - loss: 2.0288 - Spearman: 0.5456 - pearson_r: 0.4572\n",
      "2077/3143 [==================>...........] - ETA: 11s - loss: 2.0271 - Spearman: 0.5459 - pearson_r: 0.4579\n",
      "2087/3143 [==================>...........] - ETA: 11s - loss: 2.0265 - Spearman: 0.5461 - pearson_r: 0.4583\n",
      "2098/3143 [===================>..........] - ETA: 11s - loss: 2.0249 - Spearman: 0.5463 - pearson_r: 0.4589\n",
      "2108/3143 [===================>..........] - ETA: 10s - loss: 2.0238 - Spearman: 0.5465 - pearson_r: 0.4594\n",
      "2119/3143 [===================>..........] - ETA: 10s - loss: 2.0224 - Spearman: 0.5469 - pearson_r: 0.4600\n",
      "2130/3143 [===================>..........] - ETA: 10s - loss: 2.0211 - Spearman: 0.5472 - pearson_r: 0.4604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2136/3143 [===================>..........] - ETA: 10s - loss: 2.0204 - Spearman: 0.5473 - pearson_r: 0.4607\n",
      "2147/3143 [===================>..........] - ETA: 10s - loss: 2.0187 - Spearman: 0.5476 - pearson_r: 0.4613\n",
      "2157/3143 [===================>..........] - ETA: 10s - loss: 2.0177 - Spearman: 0.5477 - pearson_r: 0.4617\n",
      "2169/3143 [===================>..........] - ETA: 10s - loss: 2.0169 - Spearman: 0.5477 - pearson_r: 0.4620\n",
      "2179/3143 [===================>..........] - ETA: 10s - loss: 2.0151 - Spearman: 0.5479 - pearson_r: 0.4625\n",
      "2189/3143 [===================>..........] - ETA: 10s - loss: 2.0136 - Spearman: 0.5481 - pearson_r: 0.4630\n",
      "2199/3143 [===================>..........] - ETA: 9s - loss: 2.0120 - Spearman: 0.5483 - pearson_r: 0.4634 \n",
      "2209/3143 [====================>.........] - ETA: 9s - loss: 2.0109 - Spearman: 0.5486 - pearson_r: 0.4640\n",
      "2219/3143 [====================>.........] - ETA: 9s - loss: 2.0098 - Spearman: 0.5487 - pearson_r: 0.4644\n",
      "2229/3143 [====================>.........] - ETA: 9s - loss: 2.0080 - Spearman: 0.5488 - pearson_r: 0.4648\n",
      "2239/3143 [====================>.........] - ETA: 9s - loss: 2.0071 - Spearman: 0.5490 - pearson_r: 0.4651\n",
      "2249/3143 [====================>.........] - ETA: 9s - loss: 2.0057 - Spearman: 0.5493 - pearson_r: 0.4655\n",
      "2260/3143 [====================>.........] - ETA: 9s - loss: 2.0045 - Spearman: 0.5495 - pearson_r: 0.4661\n",
      "2270/3143 [====================>.........] - ETA: 9s - loss: 2.0029 - Spearman: 0.5498 - pearson_r: 0.4666\n",
      "2280/3143 [====================>.........] - ETA: 9s - loss: 2.0013 - Spearman: 0.5500 - pearson_r: 0.4671\n",
      "2291/3143 [====================>.........] - ETA: 9s - loss: 1.9998 - Spearman: 0.5501 - pearson_r: 0.4674\n",
      "2301/3143 [====================>.........] - ETA: 8s - loss: 1.9988 - Spearman: 0.5502 - pearson_r: 0.4678\n",
      "2312/3143 [=====================>........] - ETA: 8s - loss: 1.9970 - Spearman: 0.5505 - pearson_r: 0.4685\n",
      "2323/3143 [=====================>........] - ETA: 8s - loss: 1.9949 - Spearman: 0.5507 - pearson_r: 0.4690\n",
      "2333/3143 [=====================>........] - ETA: 8s - loss: 1.9936 - Spearman: 0.5509 - pearson_r: 0.4694\n",
      "2344/3143 [=====================>........] - ETA: 8s - loss: 1.9918 - Spearman: 0.5511 - pearson_r: 0.4699\n",
      "2355/3143 [=====================>........] - ETA: 8s - loss: 1.9899 - Spearman: 0.5514 - pearson_r: 0.4705\n",
      "2365/3143 [=====================>........] - ETA: 8s - loss: 1.9892 - Spearman: 0.5516 - pearson_r: 0.4708\n",
      "2375/3143 [=====================>........] - ETA: 8s - loss: 1.9881 - Spearman: 0.5518 - pearson_r: 0.4712\n",
      "2385/3143 [=====================>........] - ETA: 8s - loss: 1.9869 - Spearman: 0.5520 - pearson_r: 0.4716\n",
      "2395/3143 [=====================>........] - ETA: 7s - loss: 1.9864 - Spearman: 0.5522 - pearson_r: 0.4720\n",
      "2405/3143 [=====================>........] - ETA: 7s - loss: 1.9850 - Spearman: 0.5525 - pearson_r: 0.4725\n",
      "2416/3143 [======================>.......] - ETA: 7s - loss: 1.9839 - Spearman: 0.5527 - pearson_r: 0.4729\n",
      "2422/3143 [======================>.......] - ETA: 7s - loss: 1.9833 - Spearman: 0.5529 - pearson_r: 0.4732\n",
      "2428/3143 [======================>.......] - ETA: 7s - loss: 1.9828 - Spearman: 0.5530 - pearson_r: 0.4734\n",
      "2433/3143 [======================>.......] - ETA: 7s - loss: 1.9821 - Spearman: 0.5530 - pearson_r: 0.4736\n",
      "2438/3143 [======================>.......] - ETA: 7s - loss: 1.9817 - Spearman: 0.5531 - pearson_r: 0.4738\n",
      "2443/3143 [======================>.......] - ETA: 7s - loss: 1.9811 - Spearman: 0.5533 - pearson_r: 0.4741\n",
      "2449/3143 [======================>.......] - ETA: 7s - loss: 1.9805 - Spearman: 0.5534 - pearson_r: 0.4743\n",
      "2454/3143 [======================>.......] - ETA: 7s - loss: 1.9798 - Spearman: 0.5535 - pearson_r: 0.4745\n",
      "2460/3143 [======================>.......] - ETA: 7s - loss: 1.9793 - Spearman: 0.5536 - pearson_r: 0.4747\n",
      "2470/3143 [======================>.......] - ETA: 7s - loss: 1.9779 - Spearman: 0.5539 - pearson_r: 0.4751\n",
      "2481/3143 [======================>.......] - ETA: 6s - loss: 1.9768 - Spearman: 0.5541 - pearson_r: 0.4755\n",
      "2486/3143 [======================>.......] - ETA: 6s - loss: 1.9761 - Spearman: 0.5542 - pearson_r: 0.4757\n",
      "2491/3143 [======================>.......] - ETA: 6s - loss: 1.9757 - Spearman: 0.5543 - pearson_r: 0.4759\n",
      "2501/3143 [======================>.......] - ETA: 6s - loss: 1.9749 - Spearman: 0.5545 - pearson_r: 0.4763\n",
      "2506/3143 [======================>.......] - ETA: 6s - loss: 1.9742 - Spearman: 0.5545 - pearson_r: 0.4765\n",
      "2512/3143 [======================>.......] - ETA: 6s - loss: 1.9734 - Spearman: 0.5546 - pearson_r: 0.4767\n",
      "2518/3143 [=======================>......] - ETA: 6s - loss: 1.9728 - Spearman: 0.5547 - pearson_r: 0.4768\n",
      "2528/3143 [=======================>......] - ETA: 6s - loss: 1.9719 - Spearman: 0.5549 - pearson_r: 0.4774\n",
      "2538/3143 [=======================>......] - ETA: 6s - loss: 1.9706 - Spearman: 0.5552 - pearson_r: 0.4779\n",
      "2550/3143 [=======================>......] - ETA: 6s - loss: 1.9687 - Spearman: 0.5554 - pearson_r: 0.4785\n",
      "2562/3143 [=======================>......] - ETA: 6s - loss: 1.9678 - Spearman: 0.5557 - pearson_r: 0.4790\n",
      "2573/3143 [=======================>......] - ETA: 6s - loss: 1.9665 - Spearman: 0.5560 - pearson_r: 0.4795\n",
      "2585/3143 [=======================>......] - ETA: 5s - loss: 1.9650 - Spearman: 0.5562 - pearson_r: 0.4801\n",
      "2595/3143 [=======================>......] - ETA: 5s - loss: 1.9640 - Spearman: 0.5564 - pearson_r: 0.4804\n",
      "2605/3143 [=======================>......] - ETA: 5s - loss: 1.9630 - Spearman: 0.5567 - pearson_r: 0.4809\n",
      "2616/3143 [=======================>......] - ETA: 5s - loss: 1.9616 - Spearman: 0.5569 - pearson_r: 0.4813\n",
      "2627/3143 [========================>.....] - ETA: 5s - loss: 1.9610 - Spearman: 0.5572 - pearson_r: 0.4818\n",
      "2637/3143 [========================>.....] - ETA: 5s - loss: 1.9601 - Spearman: 0.5575 - pearson_r: 0.4822\n",
      "2648/3143 [========================>.....] - ETA: 5s - loss: 1.9588 - Spearman: 0.5576 - pearson_r: 0.4826\n",
      "2654/3143 [========================>.....] - ETA: 5s - loss: 1.9584 - Spearman: 0.5577 - pearson_r: 0.4829\n",
      "2666/3143 [========================>.....] - ETA: 5s - loss: 1.9570 - Spearman: 0.5579 - pearson_r: 0.4833\n",
      "2677/3143 [========================>.....] - ETA: 4s - loss: 1.9557 - Spearman: 0.5580 - pearson_r: 0.4837\n",
      "2689/3143 [========================>.....] - ETA: 4s - loss: 1.9544 - Spearman: 0.5582 - pearson_r: 0.4842\n",
      "2699/3143 [========================>.....] - ETA: 4s - loss: 1.9536 - Spearman: 0.5583 - pearson_r: 0.4845\n",
      "2709/3143 [========================>.....] - ETA: 4s - loss: 1.9527 - Spearman: 0.5584 - pearson_r: 0.4848\n",
      "2720/3143 [========================>.....] - ETA: 4s - loss: 1.9517 - Spearman: 0.5586 - pearson_r: 0.4852\n",
      "2732/3143 [=========================>....] - ETA: 4s - loss: 1.9502 - Spearman: 0.5587 - pearson_r: 0.4857\n",
      "2743/3143 [=========================>....] - ETA: 4s - loss: 1.9487 - Spearman: 0.5588 - pearson_r: 0.4860\n",
      "2754/3143 [=========================>....] - ETA: 4s - loss: 1.9477 - Spearman: 0.5589 - pearson_r: 0.4863\n",
      "2766/3143 [=========================>....] - ETA: 3s - loss: 1.9468 - Spearman: 0.5590 - pearson_r: 0.4867\n",
      "2772/3143 [=========================>....] - ETA: 3s - loss: 1.9467 - Spearman: 0.5591 - pearson_r: 0.4868\n",
      "2777/3143 [=========================>....] - ETA: 3s - loss: 1.9462 - Spearman: 0.5592 - pearson_r: 0.4870\n",
      "2783/3143 [=========================>....] - ETA: 3s - loss: 1.9460 - Spearman: 0.5593 - pearson_r: 0.4872\n",
      "2793/3143 [=========================>....] - ETA: 3s - loss: 1.9451 - Spearman: 0.5595 - pearson_r: 0.4877\n",
      "2798/3143 [=========================>....] - ETA: 3s - loss: 1.9446 - Spearman: 0.5596 - pearson_r: 0.4878\n",
      "2804/3143 [=========================>....] - ETA: 3s - loss: 1.9443 - Spearman: 0.5597 - pearson_r: 0.4880\n",
      "2815/3143 [=========================>....] - ETA: 3s - loss: 1.9431 - Spearman: 0.5599 - pearson_r: 0.4883\n",
      "2827/3143 [=========================>....] - ETA: 3s - loss: 1.9420 - Spearman: 0.5600 - pearson_r: 0.4888\n",
      "2838/3143 [==========================>...] - ETA: 3s - loss: 1.9411 - Spearman: 0.5602 - pearson_r: 0.4892\n",
      "2848/3143 [==========================>...] - ETA: 3s - loss: 1.9399 - Spearman: 0.5604 - pearson_r: 0.4896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2859/3143 [==========================>...] - ETA: 2s - loss: 1.9389 - Spearman: 0.5605 - pearson_r: 0.4899\n",
      "2870/3143 [==========================>...] - ETA: 2s - loss: 1.9374 - Spearman: 0.5605 - pearson_r: 0.4901\n",
      "2881/3143 [==========================>...] - ETA: 2s - loss: 1.9364 - Spearman: 0.5606 - pearson_r: 0.4903\n",
      "2892/3143 [==========================>...] - ETA: 2s - loss: 1.9356 - Spearman: 0.5607 - pearson_r: 0.4907\n",
      "2903/3143 [==========================>...] - ETA: 2s - loss: 1.9344 - Spearman: 0.5608 - pearson_r: 0.4909\n",
      "2913/3143 [==========================>...] - ETA: 2s - loss: 1.9335 - Spearman: 0.5609 - pearson_r: 0.4912\n",
      "2923/3143 [==========================>...] - ETA: 2s - loss: 1.9326 - Spearman: 0.5610 - pearson_r: 0.4914\n",
      "2933/3143 [==========================>...] - ETA: 2s - loss: 1.9320 - Spearman: 0.5612 - pearson_r: 0.4918\n",
      "2943/3143 [===========================>..] - ETA: 2s - loss: 1.9315 - Spearman: 0.5614 - pearson_r: 0.4922\n",
      "2954/3143 [===========================>..] - ETA: 1s - loss: 1.9309 - Spearman: 0.5617 - pearson_r: 0.4926\n",
      "2964/3143 [===========================>..] - ETA: 1s - loss: 1.9300 - Spearman: 0.5618 - pearson_r: 0.4930\n",
      "2970/3143 [===========================>..] - ETA: 1s - loss: 1.9294 - Spearman: 0.5620 - pearson_r: 0.4932\n",
      "2982/3143 [===========================>..] - ETA: 1s - loss: 1.9283 - Spearman: 0.5622 - pearson_r: 0.4936\n",
      "2992/3143 [===========================>..] - ETA: 1s - loss: 1.9273 - Spearman: 0.5624 - pearson_r: 0.4940\n",
      "3002/3143 [===========================>..] - ETA: 1s - loss: 1.9264 - Spearman: 0.5626 - pearson_r: 0.4944\n",
      "3012/3143 [===========================>..] - ETA: 1s - loss: 1.9255 - Spearman: 0.5627 - pearson_r: 0.4948\n",
      "3023/3143 [===========================>..] - ETA: 1s - loss: 1.9246 - Spearman: 0.5629 - pearson_r: 0.4951\n",
      "3034/3143 [===========================>..] - ETA: 1s - loss: 1.9235 - Spearman: 0.5631 - pearson_r: 0.4955\n",
      "3046/3143 [============================>.] - ETA: 1s - loss: 1.9227 - Spearman: 0.5632 - pearson_r: 0.4958\n",
      "3058/3143 [============================>.] - ETA: 0s - loss: 1.9213 - Spearman: 0.5634 - pearson_r: 0.4963\n",
      "3069/3143 [============================>.] - ETA: 0s - loss: 1.9203 - Spearman: 0.5636 - pearson_r: 0.4966\n",
      "3079/3143 [============================>.] - ETA: 0s - loss: 1.9197 - Spearman: 0.5637 - pearson_r: 0.4969\n",
      "3090/3143 [============================>.] - ETA: 0s - loss: 1.9190 - Spearman: 0.5638 - pearson_r: 0.4971\n",
      "3095/3143 [============================>.] - ETA: 0s - loss: 1.9187 - Spearman: 0.5638 - pearson_r: 0.4972\n",
      "3100/3143 [============================>.] - ETA: 0s - loss: 1.9182 - Spearman: 0.5640 - pearson_r: 0.4974\n",
      "3106/3143 [============================>.] - ETA: 0s - loss: 1.9178 - Spearman: 0.5640 - pearson_r: 0.4976\n",
      "3116/3143 [============================>.] - ETA: 0s - loss: 1.9167 - Spearman: 0.5642 - pearson_r: 0.4980\n",
      "3121/3143 [============================>.] - ETA: 0s - loss: 1.9162 - Spearman: 0.5643 - pearson_r: 0.4982\n",
      "3131/3143 [============================>.] - ETA: 0s - loss: 1.9151 - Spearman: 0.5644 - pearson_r: 0.4984\n",
      "3141/3143 [============================>.] - ETA: 0s - loss: 1.9146 - Spearman: 0.5644 - pearson_r: 0.4987\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip      </th><th style=\"text-align: right;\">  pearson_r</th><th style=\"text-align: right;\">    pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  val_pearson_r</th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_hominid_2db57_00000</td><td>2023-05-04_10-32-35</td><td>False </td><td>                </td><td>41eeb579ac8c43a8929452355b0b6745</td><td>citra     </td><td style=\"text-align: right;\">                         3</td><td>143.48.44.146</td><td style=\"text-align: right;\">   0.631274</td><td style=\"text-align: right;\">1281183</td><td style=\"text-align: right;\">             138.444</td><td style=\"text-align: right;\">           33.8047</td><td style=\"text-align: right;\">       138.444</td><td style=\"text-align: right;\"> 1683210755</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   3</td><td>2db57_00000</td><td style=\"text-align: right;\">       0.563095</td><td style=\"text-align: right;\">    0.0046525</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 10:31:27,795\tWARNING util.py:244 -- The `callbacks.on_trial_result` operation took 0.723 s, which may be a performance bottleneck.\n",
      "2023-05-04 10:31:27,796\tWARNING util.py:244 -- The `process_trial_result` operation took 0.724 s, which may be a performance bottleneck.\n",
      "2023-05-04 10:31:27,796\tWARNING util.py:244 -- Processing trial results took 0.724 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-05-04 10:31:27,796\tWARNING util.py:244 -- The `process_trial_result` operation took 0.725 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3143/3143 [==============================] - 45s 12ms/step - loss: 1.9145 - Spearman: 0.5644 - pearson_r: 0.4987 - val_loss: 1.7375 - val_Spearman: 0.5680 - val_pearson_r: 0.5070\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Epoch 2/60\n",
      "   6/3143 [..............................] - ETA: 32s - loss: 1.5454 - Spearman: 0.5980 - pearson_r: 0.5942\n",
      "  16/3143 [..............................] - ETA: 32s - loss: 1.6414 - Spearman: 0.6091 - pearson_r: 0.5975\n",
      "  26/3143 [..............................] - ETA: 32s - loss: 1.6484 - Spearman: 0.6077 - pearson_r: 0.6002\n",
      "  36/3143 [..............................] - ETA: 32s - loss: 1.6511 - Spearman: 0.6087 - pearson_r: 0.6027\n",
      "  46/3143 [..............................] - ETA: 32s - loss: 1.6189 - Spearman: 0.6132 - pearson_r: 0.6121\n",
      "  56/3143 [..............................] - ETA: 32s - loss: 1.6256 - Spearman: 0.6128 - pearson_r: 0.6059\n",
      "  66/3143 [..............................] - ETA: 32s - loss: 1.6143 - Spearman: 0.6134 - pearson_r: 0.6069\n",
      "  76/3143 [..............................] - ETA: 32s - loss: 1.6038 - Spearman: 0.6099 - pearson_r: 0.6049\n",
      "  87/3143 [..............................] - ETA: 32s - loss: 1.6054 - Spearman: 0.6076 - pearson_r: 0.6061\n",
      "  97/3143 [..............................] - ETA: 31s - loss: 1.6035 - Spearman: 0.6066 - pearson_r: 0.6075\n",
      " 107/3143 [>.............................] - ETA: 32s - loss: 1.5968 - Spearman: 0.6074 - pearson_r: 0.6074\n",
      " 117/3143 [>.............................] - ETA: 31s - loss: 1.6057 - Spearman: 0.6081 - pearson_r: 0.6054\n",
      " 128/3143 [>.............................] - ETA: 31s - loss: 1.6188 - Spearman: 0.6081 - pearson_r: 0.6033\n",
      " 138/3143 [>.............................] - ETA: 31s - loss: 1.6188 - Spearman: 0.6076 - pearson_r: 0.6028\n",
      " 148/3143 [>.............................] - ETA: 31s - loss: 1.6218 - Spearman: 0.6077 - pearson_r: 0.6022\n",
      " 154/3143 [>.............................] - ETA: 31s - loss: 1.6194 - Spearman: 0.6079 - pearson_r: 0.6036\n",
      " 164/3143 [>.............................] - ETA: 31s - loss: 1.6161 - Spearman: 0.6086 - pearson_r: 0.6041\n",
      " 175/3143 [>.............................] - ETA: 31s - loss: 1.6211 - Spearman: 0.6087 - pearson_r: 0.6027\n",
      " 186/3143 [>.............................] - ETA: 30s - loss: 1.6185 - Spearman: 0.6088 - pearson_r: 0.6033\n",
      " 198/3143 [>.............................] - ETA: 30s - loss: 1.6197 - Spearman: 0.6085 - pearson_r: 0.6040\n",
      " 208/3143 [>.............................] - ETA: 30s - loss: 1.6213 - Spearman: 0.6085 - pearson_r: 0.6034\n",
      " 219/3143 [=>............................] - ETA: 30s - loss: 1.6223 - Spearman: 0.6085 - pearson_r: 0.6027\n",
      " 229/3143 [=>............................] - ETA: 30s - loss: 1.6253 - Spearman: 0.6092 - pearson_r: 0.6020\n",
      " 239/3143 [=>............................] - ETA: 30s - loss: 1.6282 - Spearman: 0.6093 - pearson_r: 0.6009\n",
      " 250/3143 [=>............................] - ETA: 29s - loss: 1.6285 - Spearman: 0.6091 - pearson_r: 0.6003\n",
      " 261/3143 [=>............................] - ETA: 29s - loss: 1.6281 - Spearman: 0.6083 - pearson_r: 0.5995\n",
      " 272/3143 [=>............................] - ETA: 29s - loss: 1.6323 - Spearman: 0.6077 - pearson_r: 0.5990\n",
      " 283/3143 [=>............................] - ETA: 29s - loss: 1.6297 - Spearman: 0.6074 - pearson_r: 0.5987\n",
      " 295/3143 [=>............................] - ETA: 29s - loss: 1.6285 - Spearman: 0.6071 - pearson_r: 0.5985\n",
      " 301/3143 [=>............................] - ETA: 29s - loss: 1.6261 - Spearman: 0.6074 - pearson_r: 0.5986\n",
      " 306/3143 [=>............................] - ETA: 29s - loss: 1.6261 - Spearman: 0.6075 - pearson_r: 0.5987\n",
      " 312/3143 [=>............................] - ETA: 29s - loss: 1.6279 - Spearman: 0.6075 - pearson_r: 0.5986\n",
      " 317/3143 [==>...........................] - ETA: 29s - loss: 1.6284 - Spearman: 0.6072 - pearson_r: 0.5981\n",
      " 328/3143 [==>...........................] - ETA: 28s - loss: 1.6287 - Spearman: 0.6072 - pearson_r: 0.5976\n",
      " 338/3143 [==>...........................] - ETA: 28s - loss: 1.6290 - Spearman: 0.6069 - pearson_r: 0.5969\n",
      " 349/3143 [==>...........................] - ETA: 28s - loss: 1.6279 - Spearman: 0.6065 - pearson_r: 0.5968\n",
      " 355/3143 [==>...........................] - ETA: 28s - loss: 1.6286 - Spearman: 0.6066 - pearson_r: 0.5974\n",
      " 366/3143 [==>...........................] - ETA: 28s - loss: 1.6296 - Spearman: 0.6061 - pearson_r: 0.5970\n",
      " 371/3143 [==>...........................] - ETA: 28s - loss: 1.6303 - Spearman: 0.6059 - pearson_r: 0.5967\n",
      " 376/3143 [==>...........................] - ETA: 28s - loss: 1.6306 - Spearman: 0.6056 - pearson_r: 0.5959\n",
      " 382/3143 [==>...........................] - ETA: 28s - loss: 1.6319 - Spearman: 0.6055 - pearson_r: 0.5957\n",
      " 388/3143 [==>...........................] - ETA: 28s - loss: 1.6298 - Spearman: 0.6055 - pearson_r: 0.5958\n",
      " 393/3143 [==>...........................] - ETA: 28s - loss: 1.6289 - Spearman: 0.6056 - pearson_r: 0.5957\n",
      " 403/3143 [==>...........................] - ETA: 27s - loss: 1.6295 - Spearman: 0.6062 - pearson_r: 0.5963\n",
      " 408/3143 [==>...........................] - ETA: 27s - loss: 1.6309 - Spearman: 0.6064 - pearson_r: 0.5965\n",
      " 420/3143 [===>..........................] - ETA: 27s - loss: 1.6297 - Spearman: 0.6060 - pearson_r: 0.5962\n",
      " 431/3143 [===>..........................] - ETA: 27s - loss: 1.6314 - Spearman: 0.6058 - pearson_r: 0.5960\n",
      " 442/3143 [===>..........................] - ETA: 27s - loss: 1.6315 - Spearman: 0.6064 - pearson_r: 0.5962\n",
      " 452/3143 [===>..........................] - ETA: 27s - loss: 1.6333 - Spearman: 0.6068 - pearson_r: 0.5968\n",
      " 463/3143 [===>..........................] - ETA: 27s - loss: 1.6339 - Spearman: 0.6065 - pearson_r: 0.5963\n",
      " 473/3143 [===>..........................] - ETA: 27s - loss: 1.6332 - Spearman: 0.6066 - pearson_r: 0.5972\n",
      " 483/3143 [===>..........................] - ETA: 27s - loss: 1.6340 - Spearman: 0.6064 - pearson_r: 0.5970\n",
      " 493/3143 [===>..........................] - ETA: 27s - loss: 1.6334 - Spearman: 0.6065 - pearson_r: 0.5971\n",
      " 503/3143 [===>..........................] - ETA: 26s - loss: 1.6318 - Spearman: 0.6062 - pearson_r: 0.5973\n",
      " 513/3143 [===>..........................] - ETA: 26s - loss: 1.6320 - Spearman: 0.6063 - pearson_r: 0.5976\n",
      " 523/3143 [===>..........................] - ETA: 26s - loss: 1.6325 - Spearman: 0.6060 - pearson_r: 0.5975\n",
      " 533/3143 [====>.........................] - ETA: 26s - loss: 1.6338 - Spearman: 0.6060 - pearson_r: 0.5979\n",
      " 543/3143 [====>.........................] - ETA: 26s - loss: 1.6339 - Spearman: 0.6054 - pearson_r: 0.5974\n",
      " 553/3143 [====>.........................] - ETA: 26s - loss: 1.6344 - Spearman: 0.6052 - pearson_r: 0.5977\n",
      " 563/3143 [====>.........................] - ETA: 26s - loss: 1.6334 - Spearman: 0.6053 - pearson_r: 0.5980\n",
      " 573/3143 [====>.........................] - ETA: 26s - loss: 1.6325 - Spearman: 0.6055 - pearson_r: 0.5980\n",
      " 584/3143 [====>.........................] - ETA: 26s - loss: 1.6329 - Spearman: 0.6053 - pearson_r: 0.5980\n",
      " 594/3143 [====>.........................] - ETA: 26s - loss: 1.6323 - Spearman: 0.6061 - pearson_r: 0.5988\n",
      " 604/3143 [====>.........................] - ETA: 26s - loss: 1.6322 - Spearman: 0.6061 - pearson_r: 0.5985\n",
      " 614/3143 [====>.........................] - ETA: 25s - loss: 1.6318 - Spearman: 0.6059 - pearson_r: 0.5986\n",
      " 626/3143 [====>.........................] - ETA: 25s - loss: 1.6307 - Spearman: 0.6056 - pearson_r: 0.5984\n",
      " 637/3143 [=====>........................] - ETA: 25s - loss: 1.6315 - Spearman: 0.6059 - pearson_r: 0.5984\n",
      " 647/3143 [=====>........................] - ETA: 25s - loss: 1.6309 - Spearman: 0.6062 - pearson_r: 0.5988\n",
      " 652/3143 [=====>........................] - ETA: 25s - loss: 1.6318 - Spearman: 0.6063 - pearson_r: 0.5988\n",
      " 658/3143 [=====>........................] - ETA: 25s - loss: 1.6324 - Spearman: 0.6064 - pearson_r: 0.5988\n",
      " 664/3143 [=====>........................] - ETA: 25s - loss: 1.6331 - Spearman: 0.6065 - pearson_r: 0.5990\n",
      " 669/3143 [=====>........................] - ETA: 25s - loss: 1.6338 - Spearman: 0.6065 - pearson_r: 0.5986\n",
      " 674/3143 [=====>........................] - ETA: 25s - loss: 1.6338 - Spearman: 0.6066 - pearson_r: 0.5988\n",
      " 685/3143 [=====>........................] - ETA: 25s - loss: 1.6316 - Spearman: 0.6068 - pearson_r: 0.5993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 696/3143 [=====>........................] - ETA: 25s - loss: 1.6323 - Spearman: 0.6071 - pearson_r: 0.5992\n",
      " 706/3143 [=====>........................] - ETA: 25s - loss: 1.6319 - Spearman: 0.6075 - pearson_r: 0.5996\n",
      " 716/3143 [=====>........................] - ETA: 24s - loss: 1.6310 - Spearman: 0.6078 - pearson_r: 0.6003\n",
      " 727/3143 [=====>........................] - ETA: 24s - loss: 1.6312 - Spearman: 0.6081 - pearson_r: 0.6004\n",
      " 739/3143 [======>.......................] - ETA: 24s - loss: 1.6318 - Spearman: 0.6085 - pearson_r: 0.6003\n",
      " 750/3143 [======>.......................] - ETA: 24s - loss: 1.6339 - Spearman: 0.6084 - pearson_r: 0.6001\n",
      " 761/3143 [======>.......................] - ETA: 24s - loss: 1.6334 - Spearman: 0.6084 - pearson_r: 0.5999\n",
      " 772/3143 [======>.......................] - ETA: 24s - loss: 1.6325 - Spearman: 0.6086 - pearson_r: 0.6001\n",
      " 782/3143 [======>.......................] - ETA: 24s - loss: 1.6331 - Spearman: 0.6085 - pearson_r: 0.5997\n",
      " 792/3143 [======>.......................] - ETA: 24s - loss: 1.6342 - Spearman: 0.6086 - pearson_r: 0.5996\n",
      " 803/3143 [======>.......................] - ETA: 23s - loss: 1.6357 - Spearman: 0.6086 - pearson_r: 0.5997\n",
      " 814/3143 [======>.......................] - ETA: 23s - loss: 1.6352 - Spearman: 0.6087 - pearson_r: 0.6001\n",
      " 826/3143 [======>.......................] - ETA: 23s - loss: 1.6334 - Spearman: 0.6091 - pearson_r: 0.6005\n",
      " 837/3143 [======>.......................] - ETA: 23s - loss: 1.6332 - Spearman: 0.6093 - pearson_r: 0.6008\n",
      " 847/3143 [=======>......................] - ETA: 23s - loss: 1.6331 - Spearman: 0.6095 - pearson_r: 0.6010\n",
      " 857/3143 [=======>......................] - ETA: 23s - loss: 1.6336 - Spearman: 0.6095 - pearson_r: 0.6007\n",
      " 868/3143 [=======>......................] - ETA: 23s - loss: 1.6329 - Spearman: 0.6095 - pearson_r: 0.6008\n",
      " 878/3143 [=======>......................] - ETA: 23s - loss: 1.6333 - Spearman: 0.6096 - pearson_r: 0.6009\n",
      " 884/3143 [=======>......................] - ETA: 23s - loss: 1.6335 - Spearman: 0.6096 - pearson_r: 0.6009\n",
      " 895/3143 [=======>......................] - ETA: 22s - loss: 1.6332 - Spearman: 0.6095 - pearson_r: 0.6010\n",
      " 910/3143 [=======>......................] - ETA: 22s - loss: 1.6323 - Spearman: 0.6098 - pearson_r: 0.6014\n",
      " 920/3143 [=======>......................] - ETA: 22s - loss: 1.6319 - Spearman: 0.6098 - pearson_r: 0.6017\n",
      " 930/3143 [=======>......................] - ETA: 22s - loss: 1.6309 - Spearman: 0.6097 - pearson_r: 0.6017\n",
      " 941/3143 [=======>......................] - ETA: 22s - loss: 1.6312 - Spearman: 0.6098 - pearson_r: 0.6017\n",
      " 946/3143 [========>.....................] - ETA: 22s - loss: 1.6310 - Spearman: 0.6098 - pearson_r: 0.6016\n",
      " 957/3143 [========>.....................] - ETA: 22s - loss: 1.6310 - Spearman: 0.6097 - pearson_r: 0.6012\n",
      " 967/3143 [========>.....................] - ETA: 22s - loss: 1.6314 - Spearman: 0.6099 - pearson_r: 0.6014\n",
      " 977/3143 [========>.....................] - ETA: 22s - loss: 1.6313 - Spearman: 0.6099 - pearson_r: 0.6017\n",
      " 987/3143 [========>.....................] - ETA: 22s - loss: 1.6321 - Spearman: 0.6100 - pearson_r: 0.6016\n",
      " 992/3143 [========>.....................] - ETA: 22s - loss: 1.6315 - Spearman: 0.6100 - pearson_r: 0.6016\n",
      " 998/3143 [========>.....................] - ETA: 21s - loss: 1.6319 - Spearman: 0.6100 - pearson_r: 0.6016\n",
      "1008/3143 [========>.....................] - ETA: 21s - loss: 1.6304 - Spearman: 0.6101 - pearson_r: 0.6019\n",
      "1018/3143 [========>.....................] - ETA: 21s - loss: 1.6307 - Spearman: 0.6102 - pearson_r: 0.6020\n",
      "1029/3143 [========>.....................] - ETA: 21s - loss: 1.6289 - Spearman: 0.6103 - pearson_r: 0.6021\n",
      "1040/3143 [========>.....................] - ETA: 21s - loss: 1.6290 - Spearman: 0.6103 - pearson_r: 0.6024\n",
      "1050/3143 [=========>....................] - ETA: 21s - loss: 1.6286 - Spearman: 0.6105 - pearson_r: 0.6025\n",
      "1060/3143 [=========>....................] - ETA: 21s - loss: 1.6286 - Spearman: 0.6107 - pearson_r: 0.6028\n",
      "1070/3143 [=========>....................] - ETA: 21s - loss: 1.6284 - Spearman: 0.6106 - pearson_r: 0.6026\n",
      "1080/3143 [=========>....................] - ETA: 21s - loss: 1.6282 - Spearman: 0.6106 - pearson_r: 0.6026\n",
      "1090/3143 [=========>....................] - ETA: 21s - loss: 1.6275 - Spearman: 0.6105 - pearson_r: 0.6027\n",
      "1100/3143 [=========>....................] - ETA: 20s - loss: 1.6276 - Spearman: 0.6106 - pearson_r: 0.6026\n",
      "1111/3143 [=========>....................] - ETA: 20s - loss: 1.6266 - Spearman: 0.6107 - pearson_r: 0.6026\n",
      "1121/3143 [=========>....................] - ETA: 20s - loss: 1.6274 - Spearman: 0.6106 - pearson_r: 0.6025\n",
      "1131/3143 [=========>....................] - ETA: 20s - loss: 1.6272 - Spearman: 0.6106 - pearson_r: 0.6026\n",
      "1142/3143 [=========>....................] - ETA: 20s - loss: 1.6267 - Spearman: 0.6106 - pearson_r: 0.6026\n",
      "1152/3143 [=========>....................] - ETA: 20s - loss: 1.6264 - Spearman: 0.6106 - pearson_r: 0.6026\n",
      "1162/3143 [==========>...................] - ETA: 20s - loss: 1.6264 - Spearman: 0.6106 - pearson_r: 0.6027\n",
      "1172/3143 [==========>...................] - ETA: 20s - loss: 1.6260 - Spearman: 0.6104 - pearson_r: 0.6025\n",
      "1183/3143 [==========>...................] - ETA: 20s - loss: 1.6268 - Spearman: 0.6104 - pearson_r: 0.6025\n",
      "1193/3143 [==========>...................] - ETA: 20s - loss: 1.6266 - Spearman: 0.6106 - pearson_r: 0.6026\n",
      "1203/3143 [==========>...................] - ETA: 19s - loss: 1.6259 - Spearman: 0.6106 - pearson_r: 0.6027\n",
      "1213/3143 [==========>...................] - ETA: 19s - loss: 1.6267 - Spearman: 0.6107 - pearson_r: 0.6028\n",
      "1223/3143 [==========>...................] - ETA: 19s - loss: 1.6275 - Spearman: 0.6107 - pearson_r: 0.6025\n",
      "1233/3143 [==========>...................] - ETA: 19s - loss: 1.6275 - Spearman: 0.6109 - pearson_r: 0.6027\n",
      "1243/3143 [==========>...................] - ETA: 19s - loss: 1.6273 - Spearman: 0.6110 - pearson_r: 0.6027\n",
      "1254/3143 [==========>...................] - ETA: 19s - loss: 1.6274 - Spearman: 0.6109 - pearson_r: 0.6027\n",
      "1265/3143 [===========>..................] - ETA: 19s - loss: 1.6264 - Spearman: 0.6108 - pearson_r: 0.6028\n",
      "1275/3143 [===========>..................] - ETA: 19s - loss: 1.6259 - Spearman: 0.6109 - pearson_r: 0.6029\n",
      "1285/3143 [===========>..................] - ETA: 19s - loss: 1.6259 - Spearman: 0.6109 - pearson_r: 0.6029\n",
      "1295/3143 [===========>..................] - ETA: 19s - loss: 1.6258 - Spearman: 0.6108 - pearson_r: 0.6028\n",
      "1305/3143 [===========>..................] - ETA: 18s - loss: 1.6263 - Spearman: 0.6108 - pearson_r: 0.6028\n",
      "1316/3143 [===========>..................] - ETA: 18s - loss: 1.6255 - Spearman: 0.6109 - pearson_r: 0.6032\n",
      "1326/3143 [===========>..................] - ETA: 18s - loss: 1.6254 - Spearman: 0.6109 - pearson_r: 0.6032\n",
      "1338/3143 [===========>..................] - ETA: 18s - loss: 1.6254 - Spearman: 0.6108 - pearson_r: 0.6030\n",
      "1344/3143 [===========>..................] - ETA: 18s - loss: 1.6253 - Spearman: 0.6108 - pearson_r: 0.6029\n",
      "1355/3143 [===========>..................] - ETA: 18s - loss: 1.6254 - Spearman: 0.6107 - pearson_r: 0.6029\n",
      "1365/3143 [============>.................] - ETA: 18s - loss: 1.6255 - Spearman: 0.6107 - pearson_r: 0.6030\n",
      "1377/3143 [============>.................] - ETA: 18s - loss: 1.6245 - Spearman: 0.6108 - pearson_r: 0.6034\n",
      "1388/3143 [============>.................] - ETA: 18s - loss: 1.6238 - Spearman: 0.6109 - pearson_r: 0.6035\n",
      "1399/3143 [============>.................] - ETA: 17s - loss: 1.6240 - Spearman: 0.6107 - pearson_r: 0.6035\n",
      "1409/3143 [============>.................] - ETA: 17s - loss: 1.6241 - Spearman: 0.6106 - pearson_r: 0.6035\n",
      "1419/3143 [============>.................] - ETA: 17s - loss: 1.6239 - Spearman: 0.6107 - pearson_r: 0.6037\n",
      "1430/3143 [============>.................] - ETA: 17s - loss: 1.6235 - Spearman: 0.6107 - pearson_r: 0.6036\n",
      "1441/3143 [============>.................] - ETA: 17s - loss: 1.6235 - Spearman: 0.6107 - pearson_r: 0.6036\n",
      "1451/3143 [============>.................] - ETA: 17s - loss: 1.6238 - Spearman: 0.6105 - pearson_r: 0.6034\n",
      "1462/3143 [============>.................] - ETA: 17s - loss: 1.6246 - Spearman: 0.6107 - pearson_r: 0.6035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1474/3143 [=============>................] - ETA: 17s - loss: 1.6239 - Spearman: 0.6109 - pearson_r: 0.6037\n",
      "1485/3143 [=============>................] - ETA: 17s - loss: 1.6236 - Spearman: 0.6108 - pearson_r: 0.6038\n",
      "1491/3143 [=============>................] - ETA: 16s - loss: 1.6233 - Spearman: 0.6109 - pearson_r: 0.6039\n",
      "1502/3143 [=============>................] - ETA: 16s - loss: 1.6228 - Spearman: 0.6108 - pearson_r: 0.6040\n",
      "1514/3143 [=============>................] - ETA: 16s - loss: 1.6228 - Spearman: 0.6109 - pearson_r: 0.6041\n",
      "1525/3143 [=============>................] - ETA: 16s - loss: 1.6227 - Spearman: 0.6109 - pearson_r: 0.6041\n",
      "1537/3143 [=============>................] - ETA: 16s - loss: 1.6220 - Spearman: 0.6108 - pearson_r: 0.6042\n",
      "1547/3143 [=============>................] - ETA: 16s - loss: 1.6222 - Spearman: 0.6109 - pearson_r: 0.6041\n",
      "1557/3143 [=============>................] - ETA: 16s - loss: 1.6227 - Spearman: 0.6109 - pearson_r: 0.6041\n",
      "1569/3143 [=============>................] - ETA: 16s - loss: 1.6219 - Spearman: 0.6112 - pearson_r: 0.6044\n",
      "1580/3143 [==============>...............] - ETA: 16s - loss: 1.6222 - Spearman: 0.6112 - pearson_r: 0.6045\n",
      "1591/3143 [==============>...............] - ETA: 15s - loss: 1.6217 - Spearman: 0.6112 - pearson_r: 0.6045\n",
      "1603/3143 [==============>...............] - ETA: 15s - loss: 1.6218 - Spearman: 0.6111 - pearson_r: 0.6044\n",
      "1609/3143 [==============>...............] - ETA: 15s - loss: 1.6214 - Spearman: 0.6111 - pearson_r: 0.6043\n",
      "1614/3143 [==============>...............] - ETA: 15s - loss: 1.6215 - Spearman: 0.6112 - pearson_r: 0.6045\n",
      "1619/3143 [==============>...............] - ETA: 15s - loss: 1.6216 - Spearman: 0.6112 - pearson_r: 0.6046\n",
      "1630/3143 [==============>...............] - ETA: 15s - loss: 1.6217 - Spearman: 0.6114 - pearson_r: 0.6047\n",
      "1642/3143 [==============>...............] - ETA: 15s - loss: 1.6209 - Spearman: 0.6113 - pearson_r: 0.6045\n",
      "1652/3143 [==============>...............] - ETA: 15s - loss: 1.6209 - Spearman: 0.6114 - pearson_r: 0.6048\n",
      "1663/3143 [==============>...............] - ETA: 15s - loss: 1.6209 - Spearman: 0.6115 - pearson_r: 0.6049\n",
      "1673/3143 [==============>...............] - ETA: 15s - loss: 1.6211 - Spearman: 0.6116 - pearson_r: 0.6050\n",
      "1684/3143 [===============>..............] - ETA: 14s - loss: 1.6211 - Spearman: 0.6115 - pearson_r: 0.6049\n",
      "1694/3143 [===============>..............] - ETA: 14s - loss: 1.6210 - Spearman: 0.6116 - pearson_r: 0.6050\n",
      "1705/3143 [===============>..............] - ETA: 14s - loss: 1.6212 - Spearman: 0.6117 - pearson_r: 0.6051\n",
      "1715/3143 [===============>..............] - ETA: 14s - loss: 1.6210 - Spearman: 0.6117 - pearson_r: 0.6052\n",
      "1725/3143 [===============>..............] - ETA: 14s - loss: 1.6206 - Spearman: 0.6117 - pearson_r: 0.6053\n",
      "1736/3143 [===============>..............] - ETA: 14s - loss: 1.6199 - Spearman: 0.6118 - pearson_r: 0.6056\n",
      "1746/3143 [===============>..............] - ETA: 14s - loss: 1.6199 - Spearman: 0.6117 - pearson_r: 0.6055\n",
      "1756/3143 [===============>..............] - ETA: 14s - loss: 1.6203 - Spearman: 0.6117 - pearson_r: 0.6056\n",
      "1766/3143 [===============>..............] - ETA: 14s - loss: 1.6203 - Spearman: 0.6117 - pearson_r: 0.6056\n",
      "1776/3143 [===============>..............] - ETA: 14s - loss: 1.6199 - Spearman: 0.6116 - pearson_r: 0.6058\n",
      "1786/3143 [================>.............] - ETA: 13s - loss: 1.6201 - Spearman: 0.6116 - pearson_r: 0.6057\n",
      "1797/3143 [================>.............] - ETA: 13s - loss: 1.6200 - Spearman: 0.6117 - pearson_r: 0.6057\n",
      "1807/3143 [================>.............] - ETA: 13s - loss: 1.6202 - Spearman: 0.6117 - pearson_r: 0.6056\n",
      "1817/3143 [================>.............] - ETA: 13s - loss: 1.6198 - Spearman: 0.6119 - pearson_r: 0.6058\n",
      "1827/3143 [================>.............] - ETA: 13s - loss: 1.6197 - Spearman: 0.6118 - pearson_r: 0.6057\n",
      "1837/3143 [================>.............] - ETA: 13s - loss: 1.6194 - Spearman: 0.6117 - pearson_r: 0.6057\n",
      "1847/3143 [================>.............] - ETA: 13s - loss: 1.6193 - Spearman: 0.6117 - pearson_r: 0.6058\n",
      "1857/3143 [================>.............] - ETA: 13s - loss: 1.6196 - Spearman: 0.6116 - pearson_r: 0.6056\n",
      "1868/3143 [================>.............] - ETA: 13s - loss: 1.6192 - Spearman: 0.6115 - pearson_r: 0.6056\n",
      "1879/3143 [================>.............] - ETA: 12s - loss: 1.6187 - Spearman: 0.6114 - pearson_r: 0.6055\n",
      "1889/3143 [=================>............] - ETA: 12s - loss: 1.6185 - Spearman: 0.6113 - pearson_r: 0.6054\n",
      "1899/3143 [=================>............] - ETA: 12s - loss: 1.6186 - Spearman: 0.6113 - pearson_r: 0.6053\n",
      "1909/3143 [=================>............] - ETA: 12s - loss: 1.6180 - Spearman: 0.6113 - pearson_r: 0.6054\n",
      "1915/3143 [=================>............] - ETA: 12s - loss: 1.6178 - Spearman: 0.6113 - pearson_r: 0.6055\n",
      "1921/3143 [=================>............] - ETA: 12s - loss: 1.6180 - Spearman: 0.6113 - pearson_r: 0.6055\n",
      "1926/3143 [=================>............] - ETA: 12s - loss: 1.6181 - Spearman: 0.6113 - pearson_r: 0.6055\n",
      "1932/3143 [=================>............] - ETA: 12s - loss: 1.6180 - Spearman: 0.6113 - pearson_r: 0.6055\n",
      "1942/3143 [=================>............] - ETA: 12s - loss: 1.6176 - Spearman: 0.6114 - pearson_r: 0.6057\n",
      "1947/3143 [=================>............] - ETA: 12s - loss: 1.6176 - Spearman: 0.6114 - pearson_r: 0.6057\n",
      "1953/3143 [=================>............] - ETA: 12s - loss: 1.6173 - Spearman: 0.6114 - pearson_r: 0.6057\n",
      "1963/3143 [=================>............] - ETA: 12s - loss: 1.6171 - Spearman: 0.6115 - pearson_r: 0.6059\n",
      "1969/3143 [=================>............] - ETA: 12s - loss: 1.6170 - Spearman: 0.6115 - pearson_r: 0.6058\n",
      "1981/3143 [=================>............] - ETA: 11s - loss: 1.6175 - Spearman: 0.6113 - pearson_r: 0.6057\n",
      "1991/3143 [==================>...........] - ETA: 11s - loss: 1.6174 - Spearman: 0.6113 - pearson_r: 0.6058\n",
      "2001/3143 [==================>...........] - ETA: 11s - loss: 1.6170 - Spearman: 0.6114 - pearson_r: 0.6059\n",
      "2011/3143 [==================>...........] - ETA: 11s - loss: 1.6165 - Spearman: 0.6115 - pearson_r: 0.6059\n",
      "2022/3143 [==================>...........] - ETA: 11s - loss: 1.6163 - Spearman: 0.6115 - pearson_r: 0.6059\n",
      "2032/3143 [==================>...........] - ETA: 11s - loss: 1.6162 - Spearman: 0.6116 - pearson_r: 0.6060\n",
      "2042/3143 [==================>...........] - ETA: 11s - loss: 1.6164 - Spearman: 0.6116 - pearson_r: 0.6060\n",
      "2052/3143 [==================>...........] - ETA: 11s - loss: 1.6159 - Spearman: 0.6117 - pearson_r: 0.6061\n",
      "2063/3143 [==================>...........] - ETA: 11s - loss: 1.6154 - Spearman: 0.6117 - pearson_r: 0.6061\n",
      "2073/3143 [==================>...........] - ETA: 10s - loss: 1.6153 - Spearman: 0.6117 - pearson_r: 0.6062\n",
      "2083/3143 [==================>...........] - ETA: 10s - loss: 1.6148 - Spearman: 0.6117 - pearson_r: 0.6062\n",
      "2093/3143 [==================>...........] - ETA: 10s - loss: 1.6143 - Spearman: 0.6117 - pearson_r: 0.6063\n",
      "2104/3143 [===================>..........] - ETA: 10s - loss: 1.6141 - Spearman: 0.6118 - pearson_r: 0.6065\n",
      "2114/3143 [===================>..........] - ETA: 10s - loss: 1.6142 - Spearman: 0.6119 - pearson_r: 0.6066\n",
      "2124/3143 [===================>..........] - ETA: 10s - loss: 1.6141 - Spearman: 0.6118 - pearson_r: 0.6067\n",
      "2135/3143 [===================>..........] - ETA: 10s - loss: 1.6137 - Spearman: 0.6118 - pearson_r: 0.6067\n",
      "2145/3143 [===================>..........] - ETA: 10s - loss: 1.6142 - Spearman: 0.6119 - pearson_r: 0.6068\n",
      "2155/3143 [===================>..........] - ETA: 10s - loss: 1.6144 - Spearman: 0.6118 - pearson_r: 0.6068\n",
      "2161/3143 [===================>..........] - ETA: 10s - loss: 1.6141 - Spearman: 0.6117 - pearson_r: 0.6068\n",
      "2166/3143 [===================>..........] - ETA: 10s - loss: 1.6141 - Spearman: 0.6117 - pearson_r: 0.6067\n",
      "2176/3143 [===================>..........] - ETA: 9s - loss: 1.6135 - Spearman: 0.6117 - pearson_r: 0.6069\n",
      "2186/3143 [===================>..........] - ETA: 9s - loss: 1.6133 - Spearman: 0.6118 - pearson_r: 0.6070\n",
      "2196/3143 [===================>..........] - ETA: 9s - loss: 1.6134 - Spearman: 0.6117 - pearson_r: 0.6070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206/3143 [====================>.........] - ETA: 9s - loss: 1.6129 - Spearman: 0.6117 - pearson_r: 0.6071\n",
      "2216/3143 [====================>.........] - ETA: 9s - loss: 1.6127 - Spearman: 0.6117 - pearson_r: 0.6072\n",
      "2228/3143 [====================>.........] - ETA: 9s - loss: 1.6125 - Spearman: 0.6117 - pearson_r: 0.6071\n",
      "2238/3143 [====================>.........] - ETA: 9s - loss: 1.6124 - Spearman: 0.6118 - pearson_r: 0.6073\n",
      "2248/3143 [====================>.........] - ETA: 9s - loss: 1.6123 - Spearman: 0.6118 - pearson_r: 0.6073\n",
      "2259/3143 [====================>.........] - ETA: 9s - loss: 1.6124 - Spearman: 0.6119 - pearson_r: 0.6073\n",
      "2269/3143 [====================>.........] - ETA: 8s - loss: 1.6125 - Spearman: 0.6119 - pearson_r: 0.6074\n",
      "2279/3143 [====================>.........] - ETA: 8s - loss: 1.6127 - Spearman: 0.6119 - pearson_r: 0.6073\n",
      "2285/3143 [====================>.........] - ETA: 8s - loss: 1.6126 - Spearman: 0.6119 - pearson_r: 0.6073\n",
      "2291/3143 [====================>.........] - ETA: 8s - loss: 1.6130 - Spearman: 0.6120 - pearson_r: 0.6073\n",
      "2297/3143 [====================>.........] - ETA: 8s - loss: 1.6134 - Spearman: 0.6120 - pearson_r: 0.6073\n",
      "2307/3143 [=====================>........] - ETA: 8s - loss: 1.6135 - Spearman: 0.6120 - pearson_r: 0.6073\n",
      "2317/3143 [=====================>........] - ETA: 8s - loss: 1.6128 - Spearman: 0.6122 - pearson_r: 0.6075\n",
      "2328/3143 [=====================>........] - ETA: 8s - loss: 1.6121 - Spearman: 0.6123 - pearson_r: 0.6077\n",
      "2339/3143 [=====================>........] - ETA: 8s - loss: 1.6126 - Spearman: 0.6124 - pearson_r: 0.6078\n",
      "2349/3143 [=====================>........] - ETA: 8s - loss: 1.6128 - Spearman: 0.6124 - pearson_r: 0.6079\n",
      "2360/3143 [=====================>........] - ETA: 8s - loss: 1.6123 - Spearman: 0.6125 - pearson_r: 0.6080\n",
      "2370/3143 [=====================>........] - ETA: 7s - loss: 1.6125 - Spearman: 0.6125 - pearson_r: 0.6080\n",
      "2382/3143 [=====================>........] - ETA: 7s - loss: 1.6124 - Spearman: 0.6126 - pearson_r: 0.6080\n",
      "2392/3143 [=====================>........] - ETA: 7s - loss: 1.6122 - Spearman: 0.6126 - pearson_r: 0.6081\n",
      "2403/3143 [=====================>........] - ETA: 7s - loss: 1.6121 - Spearman: 0.6126 - pearson_r: 0.6081\n",
      "2415/3143 [======================>.......] - ETA: 7s - loss: 1.6119 - Spearman: 0.6126 - pearson_r: 0.6081\n",
      "2426/3143 [======================>.......] - ETA: 7s - loss: 1.6119 - Spearman: 0.6125 - pearson_r: 0.6080\n",
      "2437/3143 [======================>.......] - ETA: 7s - loss: 1.6120 - Spearman: 0.6126 - pearson_r: 0.6080\n",
      "2443/3143 [======================>.......] - ETA: 7s - loss: 1.6118 - Spearman: 0.6125 - pearson_r: 0.6081\n",
      "2454/3143 [======================>.......] - ETA: 7s - loss: 1.6111 - Spearman: 0.6126 - pearson_r: 0.6082\n",
      "2464/3143 [======================>.......] - ETA: 6s - loss: 1.6108 - Spearman: 0.6127 - pearson_r: 0.6083\n",
      "2474/3143 [======================>.......] - ETA: 6s - loss: 1.6108 - Spearman: 0.6127 - pearson_r: 0.6083\n",
      "2489/3143 [======================>.......] - ETA: 6s - loss: 1.6102 - Spearman: 0.6128 - pearson_r: 0.6085\n",
      "2494/3143 [======================>.......] - ETA: 6s - loss: 1.6100 - Spearman: 0.6128 - pearson_r: 0.6085\n",
      "2500/3143 [======================>.......] - ETA: 6s - loss: 1.6100 - Spearman: 0.6128 - pearson_r: 0.6084\n",
      "2505/3143 [======================>.......] - ETA: 6s - loss: 1.6099 - Spearman: 0.6128 - pearson_r: 0.6085\n",
      "2516/3143 [=======================>......] - ETA: 6s - loss: 1.6092 - Spearman: 0.6128 - pearson_r: 0.6085\n",
      "2521/3143 [=======================>......] - ETA: 6s - loss: 1.6090 - Spearman: 0.6128 - pearson_r: 0.6086\n",
      "2531/3143 [=======================>......] - ETA: 6s - loss: 1.6088 - Spearman: 0.6129 - pearson_r: 0.6086\n",
      "2537/3143 [=======================>......] - ETA: 6s - loss: 1.6089 - Spearman: 0.6128 - pearson_r: 0.6086\n",
      "2548/3143 [=======================>......] - ETA: 6s - loss: 1.6092 - Spearman: 0.6129 - pearson_r: 0.6087\n",
      "2560/3143 [=======================>......] - ETA: 5s - loss: 1.6092 - Spearman: 0.6129 - pearson_r: 0.6088\n",
      "2571/3143 [=======================>......] - ETA: 5s - loss: 1.6090 - Spearman: 0.6129 - pearson_r: 0.6088\n",
      "2582/3143 [=======================>......] - ETA: 5s - loss: 1.6092 - Spearman: 0.6128 - pearson_r: 0.6087\n",
      "2592/3143 [=======================>......] - ETA: 5s - loss: 1.6093 - Spearman: 0.6128 - pearson_r: 0.6087\n",
      "2603/3143 [=======================>......] - ETA: 5s - loss: 1.6091 - Spearman: 0.6128 - pearson_r: 0.6087\n",
      "2613/3143 [=======================>......] - ETA: 5s - loss: 1.6089 - Spearman: 0.6128 - pearson_r: 0.6088\n",
      "2623/3143 [========================>.....] - ETA: 5s - loss: 1.6084 - Spearman: 0.6128 - pearson_r: 0.6089\n",
      "2635/3143 [========================>.....] - ETA: 5s - loss: 1.6081 - Spearman: 0.6129 - pearson_r: 0.6090\n",
      "2646/3143 [========================>.....] - ETA: 5s - loss: 1.6081 - Spearman: 0.6130 - pearson_r: 0.6090\n",
      "2656/3143 [========================>.....] - ETA: 4s - loss: 1.6081 - Spearman: 0.6131 - pearson_r: 0.6091\n",
      "2668/3143 [========================>.....] - ETA: 4s - loss: 1.6081 - Spearman: 0.6131 - pearson_r: 0.6092\n",
      "2680/3143 [========================>.....] - ETA: 4s - loss: 1.6079 - Spearman: 0.6132 - pearson_r: 0.6092\n",
      "2692/3143 [========================>.....] - ETA: 4s - loss: 1.6073 - Spearman: 0.6131 - pearson_r: 0.6093\n",
      "2697/3143 [========================>.....] - ETA: 4s - loss: 1.6072 - Spearman: 0.6131 - pearson_r: 0.6094\n",
      "2703/3143 [========================>.....] - ETA: 4s - loss: 1.6070 - Spearman: 0.6131 - pearson_r: 0.6094\n",
      "2709/3143 [========================>.....] - ETA: 4s - loss: 1.6069 - Spearman: 0.6132 - pearson_r: 0.6094\n",
      "2720/3143 [========================>.....] - ETA: 4s - loss: 1.6067 - Spearman: 0.6132 - pearson_r: 0.6094\n",
      "2731/3143 [=========================>....] - ETA: 4s - loss: 1.6062 - Spearman: 0.6132 - pearson_r: 0.6095\n",
      "2742/3143 [=========================>....] - ETA: 4s - loss: 1.6060 - Spearman: 0.6133 - pearson_r: 0.6095\n",
      "2753/3143 [=========================>....] - ETA: 3s - loss: 1.6058 - Spearman: 0.6134 - pearson_r: 0.6096\n",
      "2764/3143 [=========================>....] - ETA: 3s - loss: 1.6058 - Spearman: 0.6134 - pearson_r: 0.6096\n",
      "2774/3143 [=========================>....] - ETA: 3s - loss: 1.6057 - Spearman: 0.6134 - pearson_r: 0.6097\n",
      "2784/3143 [=========================>....] - ETA: 3s - loss: 1.6055 - Spearman: 0.6134 - pearson_r: 0.6097\n",
      "2795/3143 [=========================>....] - ETA: 3s - loss: 1.6051 - Spearman: 0.6135 - pearson_r: 0.6098\n",
      "2806/3143 [=========================>....] - ETA: 3s - loss: 1.6050 - Spearman: 0.6135 - pearson_r: 0.6098\n",
      "2816/3143 [=========================>....] - ETA: 3s - loss: 1.6048 - Spearman: 0.6135 - pearson_r: 0.6097\n",
      "2826/3143 [=========================>....] - ETA: 3s - loss: 1.6047 - Spearman: 0.6135 - pearson_r: 0.6098\n",
      "2838/3143 [==========================>...] - ETA: 3s - loss: 1.6048 - Spearman: 0.6135 - pearson_r: 0.6099\n",
      "2848/3143 [==========================>...] - ETA: 3s - loss: 1.6045 - Spearman: 0.6135 - pearson_r: 0.6099\n",
      "2859/3143 [==========================>...] - ETA: 2s - loss: 1.6042 - Spearman: 0.6135 - pearson_r: 0.6099\n",
      "2869/3143 [==========================>...] - ETA: 2s - loss: 1.6040 - Spearman: 0.6135 - pearson_r: 0.6099\n",
      "2880/3143 [==========================>...] - ETA: 2s - loss: 1.6039 - Spearman: 0.6135 - pearson_r: 0.6101\n",
      "2890/3143 [==========================>...] - ETA: 2s - loss: 1.6036 - Spearman: 0.6135 - pearson_r: 0.6101\n",
      "2901/3143 [==========================>...] - ETA: 2s - loss: 1.6036 - Spearman: 0.6135 - pearson_r: 0.6101\n",
      "2907/3143 [==========================>...] - ETA: 2s - loss: 1.6037 - Spearman: 0.6135 - pearson_r: 0.6100\n",
      "2919/3143 [==========================>...] - ETA: 2s - loss: 1.6038 - Spearman: 0.6136 - pearson_r: 0.6102\n",
      "2929/3143 [==========================>...] - ETA: 2s - loss: 1.6037 - Spearman: 0.6137 - pearson_r: 0.6102\n",
      "2940/3143 [===========================>..] - ETA: 2s - loss: 1.6034 - Spearman: 0.6137 - pearson_r: 0.6103\n",
      "2951/3143 [===========================>..] - ETA: 1s - loss: 1.6030 - Spearman: 0.6137 - pearson_r: 0.6104\n",
      "2962/3143 [===========================>..] - ETA: 1s - loss: 1.6025 - Spearman: 0.6138 - pearson_r: 0.6104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973/3143 [===========================>..] - ETA: 1s - loss: 1.6023 - Spearman: 0.6137 - pearson_r: 0.6104\n",
      "2983/3143 [===========================>..] - ETA: 1s - loss: 1.6022 - Spearman: 0.6137 - pearson_r: 0.6105\n",
      "2993/3143 [===========================>..] - ETA: 1s - loss: 1.6023 - Spearman: 0.6138 - pearson_r: 0.6105\n",
      "3003/3143 [===========================>..] - ETA: 1s - loss: 1.6017 - Spearman: 0.6139 - pearson_r: 0.6106\n",
      "3013/3143 [===========================>..] - ETA: 1s - loss: 1.6016 - Spearman: 0.6139 - pearson_r: 0.6107\n",
      "3023/3143 [===========================>..] - ETA: 1s - loss: 1.6016 - Spearman: 0.6140 - pearson_r: 0.6108\n",
      "3034/3143 [===========================>..] - ETA: 1s - loss: 1.6013 - Spearman: 0.6140 - pearson_r: 0.6108\n",
      "3045/3143 [============================>.] - ETA: 1s - loss: 1.6010 - Spearman: 0.6139 - pearson_r: 0.6108\n",
      "3055/3143 [============================>.] - ETA: 0s - loss: 1.6008 - Spearman: 0.6139 - pearson_r: 0.6108\n",
      "3066/3143 [============================>.] - ETA: 0s - loss: 1.6006 - Spearman: 0.6139 - pearson_r: 0.6109\n",
      "3076/3143 [============================>.] - ETA: 0s - loss: 1.6010 - Spearman: 0.6139 - pearson_r: 0.6109\n",
      "3086/3143 [============================>.] - ETA: 0s - loss: 1.6009 - Spearman: 0.6139 - pearson_r: 0.6108\n",
      "3091/3143 [============================>.] - ETA: 0s - loss: 1.6009 - Spearman: 0.6139 - pearson_r: 0.6109\n",
      "3097/3143 [============================>.] - ETA: 0s - loss: 1.6011 - Spearman: 0.6139 - pearson_r: 0.6109\n",
      "3107/3143 [============================>.] - ETA: 0s - loss: 1.6009 - Spearman: 0.6139 - pearson_r: 0.6110\n",
      "3117/3143 [============================>.] - ETA: 0s - loss: 1.6006 - Spearman: 0.6139 - pearson_r: 0.6110\n",
      "3127/3143 [============================>.] - ETA: 0s - loss: 1.6008 - Spearman: 0.6140 - pearson_r: 0.6111\n",
      "3138/3143 [============================>.] - ETA: 0s - loss: 1.6002 - Spearman: 0.6140 - pearson_r: 0.6112\n",
      "3143/3143 [==============================] - 34s 11ms/step - loss: 1.6002 - Spearman: 0.6140 - pearson_r: 0.6112 - val_loss: 1.4876 - val_Spearman: 0.5890 - val_pearson_r: 0.5470\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Epoch 3/60\n",
      "   1/3143 [..............................] - ETA: 40s - loss: 1.7888 - Spearman: 0.6382 - pearson_r: 0.6028\n",
      "  11/3143 [..............................] - ETA: 34s - loss: 1.5078 - Spearman: 0.5969 - pearson_r: 0.6267\n",
      "  21/3143 [..............................] - ETA: 33s - loss: 1.5892 - Spearman: 0.6002 - pearson_r: 0.6106\n",
      "  32/3143 [..............................] - ETA: 32s - loss: 1.5646 - Spearman: 0.6029 - pearson_r: 0.6104\n",
      "  43/3143 [..............................] - ETA: 31s - loss: 1.5840 - Spearman: 0.6116 - pearson_r: 0.6163\n",
      "  54/3143 [..............................] - ETA: 31s - loss: 1.5930 - Spearman: 0.6107 - pearson_r: 0.6132\n",
      "  64/3143 [..............................] - ETA: 31s - loss: 1.5905 - Spearman: 0.6068 - pearson_r: 0.6117\n",
      "  75/3143 [..............................] - ETA: 31s - loss: 1.5812 - Spearman: 0.6052 - pearson_r: 0.6113\n",
      "  86/3143 [..............................] - ETA: 31s - loss: 1.5817 - Spearman: 0.6071 - pearson_r: 0.6132\n",
      "  97/3143 [..............................] - ETA: 30s - loss: 1.5684 - Spearman: 0.6093 - pearson_r: 0.6154\n",
      " 108/3143 [>.............................] - ETA: 30s - loss: 1.5564 - Spearman: 0.6075 - pearson_r: 0.6149\n",
      " 119/3143 [>.............................] - ETA: 30s - loss: 1.5608 - Spearman: 0.6068 - pearson_r: 0.6155\n",
      " 129/3143 [>.............................] - ETA: 30s - loss: 1.5564 - Spearman: 0.6091 - pearson_r: 0.6175\n",
      " 140/3143 [>.............................] - ETA: 30s - loss: 1.5544 - Spearman: 0.6102 - pearson_r: 0.6192\n",
      " 150/3143 [>.............................] - ETA: 30s - loss: 1.5594 - Spearman: 0.6083 - pearson_r: 0.6185\n",
      " 160/3143 [>.............................] - ETA: 30s - loss: 1.5554 - Spearman: 0.6089 - pearson_r: 0.6191\n",
      " 171/3143 [>.............................] - ETA: 30s - loss: 1.5571 - Spearman: 0.6098 - pearson_r: 0.6195\n",
      " 181/3143 [>.............................] - ETA: 30s - loss: 1.5585 - Spearman: 0.6101 - pearson_r: 0.6176\n",
      " 191/3143 [>.............................] - ETA: 30s - loss: 1.5638 - Spearman: 0.6110 - pearson_r: 0.6181\n",
      " 202/3143 [>.............................] - ETA: 30s - loss: 1.5581 - Spearman: 0.6120 - pearson_r: 0.6191\n",
      " 212/3143 [=>............................] - ETA: 29s - loss: 1.5534 - Spearman: 0.6115 - pearson_r: 0.6191\n",
      " 222/3143 [=>............................] - ETA: 29s - loss: 1.5517 - Spearman: 0.6120 - pearson_r: 0.6202\n",
      " 228/3143 [=>............................] - ETA: 29s - loss: 1.5487 - Spearman: 0.6129 - pearson_r: 0.6214\n",
      " 240/3143 [=>............................] - ETA: 29s - loss: 1.5469 - Spearman: 0.6138 - pearson_r: 0.6221\n",
      " 251/3143 [=>............................] - ETA: 29s - loss: 1.5489 - Spearman: 0.6146 - pearson_r: 0.6225\n",
      " 261/3143 [=>............................] - ETA: 29s - loss: 1.5476 - Spearman: 0.6146 - pearson_r: 0.6227\n",
      " 271/3143 [=>............................] - ETA: 29s - loss: 1.5464 - Spearman: 0.6150 - pearson_r: 0.6226\n",
      " 282/3143 [=>............................] - ETA: 29s - loss: 1.5437 - Spearman: 0.6154 - pearson_r: 0.6230\n",
      " 293/3143 [=>............................] - ETA: 29s - loss: 1.5455 - Spearman: 0.6159 - pearson_r: 0.6230\n",
      " 304/3143 [=>............................] - ETA: 28s - loss: 1.5429 - Spearman: 0.6152 - pearson_r: 0.6233\n",
      " 316/3143 [==>...........................] - ETA: 28s - loss: 1.5453 - Spearman: 0.6145 - pearson_r: 0.6227\n",
      " 326/3143 [==>...........................] - ETA: 28s - loss: 1.5440 - Spearman: 0.6148 - pearson_r: 0.6234\n",
      " 337/3143 [==>...........................] - ETA: 28s - loss: 1.5441 - Spearman: 0.6150 - pearson_r: 0.6224\n",
      " 347/3143 [==>...........................] - ETA: 28s - loss: 1.5412 - Spearman: 0.6150 - pearson_r: 0.6222\n",
      " 358/3143 [==>...........................] - ETA: 28s - loss: 1.5409 - Spearman: 0.6150 - pearson_r: 0.6233\n",
      " 369/3143 [==>...........................] - ETA: 28s - loss: 1.5415 - Spearman: 0.6143 - pearson_r: 0.6225\n",
      " 379/3143 [==>...........................] - ETA: 28s - loss: 1.5421 - Spearman: 0.6134 - pearson_r: 0.6220\n",
      " 389/3143 [==>...........................] - ETA: 28s - loss: 1.5414 - Spearman: 0.6131 - pearson_r: 0.6222\n",
      " 399/3143 [==>...........................] - ETA: 27s - loss: 1.5442 - Spearman: 0.6133 - pearson_r: 0.6219\n",
      " 410/3143 [==>...........................] - ETA: 27s - loss: 1.5432 - Spearman: 0.6132 - pearson_r: 0.6228\n",
      " 420/3143 [===>..........................] - ETA: 27s - loss: 1.5406 - Spearman: 0.6135 - pearson_r: 0.6238\n",
      " 430/3143 [===>..........................] - ETA: 27s - loss: 1.5407 - Spearman: 0.6137 - pearson_r: 0.6243\n",
      " 440/3143 [===>..........................] - ETA: 27s - loss: 1.5410 - Spearman: 0.6134 - pearson_r: 0.6240\n",
      " 451/3143 [===>..........................] - ETA: 27s - loss: 1.5413 - Spearman: 0.6132 - pearson_r: 0.6242\n",
      " 456/3143 [===>..........................] - ETA: 27s - loss: 1.5416 - Spearman: 0.6130 - pearson_r: 0.6239\n",
      " 462/3143 [===>..........................] - ETA: 27s - loss: 1.5424 - Spearman: 0.6130 - pearson_r: 0.6240\n",
      " 472/3143 [===>..........................] - ETA: 27s - loss: 1.5435 - Spearman: 0.6133 - pearson_r: 0.6244\n",
      " 482/3143 [===>..........................] - ETA: 27s - loss: 1.5428 - Spearman: 0.6136 - pearson_r: 0.6243\n",
      " 493/3143 [===>..........................] - ETA: 27s - loss: 1.5411 - Spearman: 0.6140 - pearson_r: 0.6248\n",
      " 498/3143 [===>..........................] - ETA: 27s - loss: 1.5411 - Spearman: 0.6140 - pearson_r: 0.6246\n",
      " 510/3143 [===>..........................] - ETA: 26s - loss: 1.5421 - Spearman: 0.6142 - pearson_r: 0.6247\n",
      " 522/3143 [===>..........................] - ETA: 26s - loss: 1.5408 - Spearman: 0.6140 - pearson_r: 0.6244\n",
      " 532/3143 [====>.........................] - ETA: 26s - loss: 1.5391 - Spearman: 0.6141 - pearson_r: 0.6247\n",
      " 542/3143 [====>.........................] - ETA: 26s - loss: 1.5392 - Spearman: 0.6139 - pearson_r: 0.6249\n",
      " 552/3143 [====>.........................] - ETA: 26s - loss: 1.5389 - Spearman: 0.6138 - pearson_r: 0.6250\n",
      " 563/3143 [====>.........................] - ETA: 26s - loss: 1.5396 - Spearman: 0.6140 - pearson_r: 0.6253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 573/3143 [====>.........................] - ETA: 26s - loss: 1.5416 - Spearman: 0.6144 - pearson_r: 0.6255\n",
      " 584/3143 [====>.........................] - ETA: 26s - loss: 1.5419 - Spearman: 0.6143 - pearson_r: 0.6255\n",
      " 595/3143 [====>.........................] - ETA: 26s - loss: 1.5429 - Spearman: 0.6143 - pearson_r: 0.6256\n",
      " 605/3143 [====>.........................] - ETA: 25s - loss: 1.5430 - Spearman: 0.6143 - pearson_r: 0.6256\n",
      " 615/3143 [====>.........................] - ETA: 25s - loss: 1.5430 - Spearman: 0.6145 - pearson_r: 0.6257\n",
      " 626/3143 [====>.........................] - ETA: 25s - loss: 1.5435 - Spearman: 0.6147 - pearson_r: 0.6259\n",
      " 636/3143 [=====>........................] - ETA: 25s - loss: 1.5436 - Spearman: 0.6147 - pearson_r: 0.6260\n",
      " 646/3143 [=====>........................] - ETA: 25s - loss: 1.5441 - Spearman: 0.6148 - pearson_r: 0.6258\n",
      " 657/3143 [=====>........................] - ETA: 25s - loss: 1.5439 - Spearman: 0.6149 - pearson_r: 0.6260\n",
      " 667/3143 [=====>........................] - ETA: 25s - loss: 1.5443 - Spearman: 0.6149 - pearson_r: 0.6260\n",
      " 677/3143 [=====>........................] - ETA: 25s - loss: 1.5444 - Spearman: 0.6150 - pearson_r: 0.6259\n",
      " 687/3143 [=====>........................] - ETA: 25s - loss: 1.5439 - Spearman: 0.6153 - pearson_r: 0.6262\n",
      " 697/3143 [=====>........................] - ETA: 25s - loss: 1.5449 - Spearman: 0.6157 - pearson_r: 0.6261\n",
      " 708/3143 [=====>........................] - ETA: 24s - loss: 1.5455 - Spearman: 0.6159 - pearson_r: 0.6265\n",
      " 718/3143 [=====>........................] - ETA: 24s - loss: 1.5448 - Spearman: 0.6160 - pearson_r: 0.6265\n",
      " 728/3143 [=====>........................] - ETA: 24s - loss: 1.5457 - Spearman: 0.6162 - pearson_r: 0.6266\n",
      " 738/3143 [======>.......................] - ETA: 24s - loss: 1.5460 - Spearman: 0.6165 - pearson_r: 0.6265\n",
      " 748/3143 [======>.......................] - ETA: 24s - loss: 1.5471 - Spearman: 0.6164 - pearson_r: 0.6264\n",
      " 758/3143 [======>.......................] - ETA: 24s - loss: 1.5472 - Spearman: 0.6165 - pearson_r: 0.6265\n",
      " 769/3143 [======>.......................] - ETA: 24s - loss: 1.5469 - Spearman: 0.6163 - pearson_r: 0.6263\n",
      " 779/3143 [======>.......................] - ETA: 24s - loss: 1.5463 - Spearman: 0.6165 - pearson_r: 0.6267\n",
      " 790/3143 [======>.......................] - ETA: 24s - loss: 1.5460 - Spearman: 0.6166 - pearson_r: 0.6270\n",
      " 800/3143 [======>.......................] - ETA: 24s - loss: 1.5463 - Spearman: 0.6166 - pearson_r: 0.6266\n",
      " 810/3143 [======>.......................] - ETA: 23s - loss: 1.5467 - Spearman: 0.6169 - pearson_r: 0.6269\n",
      " 821/3143 [======>.......................] - ETA: 23s - loss: 1.5471 - Spearman: 0.6169 - pearson_r: 0.6268\n",
      " 833/3143 [======>.......................] - ETA: 23s - loss: 1.5460 - Spearman: 0.6170 - pearson_r: 0.6270\n",
      " 843/3143 [=======>......................] - ETA: 23s - loss: 1.5454 - Spearman: 0.6173 - pearson_r: 0.6274\n",
      " 853/3143 [=======>......................] - ETA: 23s - loss: 1.5454 - Spearman: 0.6174 - pearson_r: 0.6272\n",
      " 863/3143 [=======>......................] - ETA: 23s - loss: 1.5453 - Spearman: 0.6174 - pearson_r: 0.6270\n",
      " 874/3143 [=======>......................] - ETA: 23s - loss: 1.5451 - Spearman: 0.6175 - pearson_r: 0.6272\n",
      " 885/3143 [=======>......................] - ETA: 23s - loss: 1.5461 - Spearman: 0.6175 - pearson_r: 0.6271\n",
      " 891/3143 [=======>......................] - ETA: 23s - loss: 1.5465 - Spearman: 0.6177 - pearson_r: 0.6272\n",
      " 902/3143 [=======>......................] - ETA: 22s - loss: 1.5463 - Spearman: 0.6178 - pearson_r: 0.6274\n",
      " 912/3143 [=======>......................] - ETA: 22s - loss: 1.5461 - Spearman: 0.6176 - pearson_r: 0.6271\n",
      " 923/3143 [=======>......................] - ETA: 22s - loss: 1.5460 - Spearman: 0.6177 - pearson_r: 0.6273\n",
      " 933/3143 [=======>......................] - ETA: 22s - loss: 1.5459 - Spearman: 0.6176 - pearson_r: 0.6272\n",
      " 938/3143 [=======>......................] - ETA: 22s - loss: 1.5448 - Spearman: 0.6175 - pearson_r: 0.6272\n",
      " 944/3143 [========>.....................] - ETA: 22s - loss: 1.5442 - Spearman: 0.6174 - pearson_r: 0.6272\n",
      " 955/3143 [========>.....................] - ETA: 22s - loss: 1.5435 - Spearman: 0.6173 - pearson_r: 0.6270\n",
      " 966/3143 [========>.....................] - ETA: 22s - loss: 1.5433 - Spearman: 0.6173 - pearson_r: 0.6267\n",
      " 977/3143 [========>.....................] - ETA: 22s - loss: 1.5428 - Spearman: 0.6173 - pearson_r: 0.6269\n",
      " 988/3143 [========>.....................] - ETA: 22s - loss: 1.5432 - Spearman: 0.6175 - pearson_r: 0.6272\n",
      " 999/3143 [========>.....................] - ETA: 21s - loss: 1.5442 - Spearman: 0.6176 - pearson_r: 0.6270\n",
      "1010/3143 [========>.....................] - ETA: 21s - loss: 1.5431 - Spearman: 0.6178 - pearson_r: 0.6272\n",
      "1020/3143 [========>.....................] - ETA: 21s - loss: 1.5421 - Spearman: 0.6177 - pearson_r: 0.6273\n",
      "1030/3143 [========>.....................] - ETA: 21s - loss: 1.5419 - Spearman: 0.6178 - pearson_r: 0.6274\n",
      "1040/3143 [========>.....................] - ETA: 21s - loss: 1.5431 - Spearman: 0.6179 - pearson_r: 0.6272\n",
      "1051/3143 [=========>....................] - ETA: 21s - loss: 1.5433 - Spearman: 0.6179 - pearson_r: 0.6271\n",
      "1061/3143 [=========>....................] - ETA: 21s - loss: 1.5442 - Spearman: 0.6176 - pearson_r: 0.6271\n",
      "1072/3143 [=========>....................] - ETA: 21s - loss: 1.5453 - Spearman: 0.6175 - pearson_r: 0.6270\n",
      "1083/3143 [=========>....................] - ETA: 21s - loss: 1.5448 - Spearman: 0.6177 - pearson_r: 0.6272\n",
      "1095/3143 [=========>....................] - ETA: 20s - loss: 1.5453 - Spearman: 0.6176 - pearson_r: 0.6270\n",
      "1106/3143 [=========>....................] - ETA: 20s - loss: 1.5447 - Spearman: 0.6177 - pearson_r: 0.6273\n",
      "1111/3143 [=========>....................] - ETA: 20s - loss: 1.5454 - Spearman: 0.6175 - pearson_r: 0.6269\n",
      "1117/3143 [=========>....................] - ETA: 20s - loss: 1.5460 - Spearman: 0.6176 - pearson_r: 0.6269\n",
      "1123/3143 [=========>....................] - ETA: 20s - loss: 1.5459 - Spearman: 0.6175 - pearson_r: 0.6268\n",
      "1134/3143 [=========>....................] - ETA: 20s - loss: 1.5454 - Spearman: 0.6176 - pearson_r: 0.6269\n",
      "1144/3143 [=========>....................] - ETA: 20s - loss: 1.5455 - Spearman: 0.6178 - pearson_r: 0.6270\n",
      "1155/3143 [==========>...................] - ETA: 20s - loss: 1.5466 - Spearman: 0.6180 - pearson_r: 0.6271\n",
      "1166/3143 [==========>...................] - ETA: 20s - loss: 1.5455 - Spearman: 0.6179 - pearson_r: 0.6271\n",
      "1176/3143 [==========>...................] - ETA: 20s - loss: 1.5457 - Spearman: 0.6178 - pearson_r: 0.6271\n",
      "1187/3143 [==========>...................] - ETA: 19s - loss: 1.5458 - Spearman: 0.6180 - pearson_r: 0.6271\n",
      "1197/3143 [==========>...................] - ETA: 19s - loss: 1.5466 - Spearman: 0.6184 - pearson_r: 0.6274\n",
      "1207/3143 [==========>...................] - ETA: 19s - loss: 1.5464 - Spearman: 0.6186 - pearson_r: 0.6272\n",
      "1217/3143 [==========>...................] - ETA: 19s - loss: 1.5461 - Spearman: 0.6186 - pearson_r: 0.6273\n",
      "1227/3143 [==========>...................] - ETA: 19s - loss: 1.5456 - Spearman: 0.6186 - pearson_r: 0.6273\n",
      "1239/3143 [==========>...................] - ETA: 19s - loss: 1.5456 - Spearman: 0.6187 - pearson_r: 0.6274\n",
      "1249/3143 [==========>...................] - ETA: 19s - loss: 1.5455 - Spearman: 0.6187 - pearson_r: 0.6276\n",
      "1259/3143 [===========>..................] - ETA: 19s - loss: 1.5453 - Spearman: 0.6186 - pearson_r: 0.6276\n",
      "1269/3143 [===========>..................] - ETA: 19s - loss: 1.5456 - Spearman: 0.6185 - pearson_r: 0.6275\n",
      "1278/3143 [===========>..................] - ETA: 19s - loss: 1.5458 - Spearman: 0.6184 - pearson_r: 0.6275\n",
      "1288/3143 [===========>..................] - ETA: 18s - loss: 1.5459 - Spearman: 0.6183 - pearson_r: 0.6275\n",
      "1298/3143 [===========>..................] - ETA: 18s - loss: 1.5454 - Spearman: 0.6183 - pearson_r: 0.6274\n",
      "1309/3143 [===========>..................] - ETA: 18s - loss: 1.5452 - Spearman: 0.6185 - pearson_r: 0.6277\n",
      "1319/3143 [===========>..................] - ETA: 18s - loss: 1.5456 - Spearman: 0.6186 - pearson_r: 0.6276\n",
      "1329/3143 [===========>..................] - ETA: 18s - loss: 1.5448 - Spearman: 0.6185 - pearson_r: 0.6276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339/3143 [===========>..................] - ETA: 18s - loss: 1.5448 - Spearman: 0.6185 - pearson_r: 0.6277\n",
      "1349/3143 [===========>..................] - ETA: 18s - loss: 1.5445 - Spearman: 0.6185 - pearson_r: 0.6277\n",
      "1360/3143 [===========>..................] - ETA: 18s - loss: 1.5444 - Spearman: 0.6184 - pearson_r: 0.6278\n",
      "1370/3143 [============>.................] - ETA: 18s - loss: 1.5448 - Spearman: 0.6183 - pearson_r: 0.6276\n",
      "1380/3143 [============>.................] - ETA: 18s - loss: 1.5456 - Spearman: 0.6181 - pearson_r: 0.6276\n",
      "1385/3143 [============>.................] - ETA: 18s - loss: 1.5454 - Spearman: 0.6182 - pearson_r: 0.6276\n",
      "1390/3143 [============>.................] - ETA: 17s - loss: 1.5454 - Spearman: 0.6182 - pearson_r: 0.6276\n",
      "1395/3143 [============>.................] - ETA: 17s - loss: 1.5450 - Spearman: 0.6182 - pearson_r: 0.6276\n",
      "1401/3143 [============>.................] - ETA: 17s - loss: 1.5449 - Spearman: 0.6183 - pearson_r: 0.6276\n",
      "1411/3143 [============>.................] - ETA: 17s - loss: 1.5453 - Spearman: 0.6184 - pearson_r: 0.6277\n",
      "1422/3143 [============>.................] - ETA: 17s - loss: 1.5462 - Spearman: 0.6186 - pearson_r: 0.6278\n",
      "1432/3143 [============>.................] - ETA: 17s - loss: 1.5460 - Spearman: 0.6187 - pearson_r: 0.6278\n",
      "1442/3143 [============>.................] - ETA: 17s - loss: 1.5456 - Spearman: 0.6187 - pearson_r: 0.6278\n",
      "1453/3143 [============>.................] - ETA: 17s - loss: 1.5450 - Spearman: 0.6187 - pearson_r: 0.6280\n",
      "1464/3143 [============>.................] - ETA: 17s - loss: 1.5445 - Spearman: 0.6186 - pearson_r: 0.6279\n",
      "1470/3143 [=============>................] - ETA: 17s - loss: 1.5441 - Spearman: 0.6187 - pearson_r: 0.6279\n",
      "1481/3143 [=============>................] - ETA: 17s - loss: 1.5440 - Spearman: 0.6187 - pearson_r: 0.6280\n",
      "1492/3143 [=============>................] - ETA: 16s - loss: 1.5438 - Spearman: 0.6188 - pearson_r: 0.6280\n",
      "1502/3143 [=============>................] - ETA: 16s - loss: 1.5434 - Spearman: 0.6188 - pearson_r: 0.6280\n",
      "1512/3143 [=============>................] - ETA: 16s - loss: 1.5441 - Spearman: 0.6190 - pearson_r: 0.6280\n",
      "1523/3143 [=============>................] - ETA: 16s - loss: 1.5442 - Spearman: 0.6191 - pearson_r: 0.6281\n",
      "1534/3143 [=============>................] - ETA: 16s - loss: 1.5439 - Spearman: 0.6190 - pearson_r: 0.6281\n",
      "1544/3143 [=============>................] - ETA: 16s - loss: 1.5440 - Spearman: 0.6189 - pearson_r: 0.6281\n",
      "1556/3143 [=============>................] - ETA: 16s - loss: 1.5439 - Spearman: 0.6191 - pearson_r: 0.6281\n",
      "1567/3143 [=============>................] - ETA: 16s - loss: 1.5437 - Spearman: 0.6193 - pearson_r: 0.6282\n",
      "1579/3143 [==============>...............] - ETA: 16s - loss: 1.5432 - Spearman: 0.6194 - pearson_r: 0.6283\n",
      "1590/3143 [==============>...............] - ETA: 15s - loss: 1.5433 - Spearman: 0.6193 - pearson_r: 0.6284\n",
      "1600/3143 [==============>...............] - ETA: 15s - loss: 1.5433 - Spearman: 0.6193 - pearson_r: 0.6284\n",
      "1610/3143 [==============>...............] - ETA: 15s - loss: 1.5437 - Spearman: 0.6193 - pearson_r: 0.6285\n",
      "1620/3143 [==============>...............] - ETA: 15s - loss: 1.5439 - Spearman: 0.6193 - pearson_r: 0.6285\n",
      "1631/3143 [==============>...............] - ETA: 15s - loss: 1.5441 - Spearman: 0.6193 - pearson_r: 0.6285\n",
      "1641/3143 [==============>...............] - ETA: 15s - loss: 1.5441 - Spearman: 0.6192 - pearson_r: 0.6283\n",
      "1652/3143 [==============>...............] - ETA: 15s - loss: 1.5432 - Spearman: 0.6191 - pearson_r: 0.6282\n",
      "1662/3143 [==============>...............] - ETA: 15s - loss: 1.5423 - Spearman: 0.6192 - pearson_r: 0.6285\n",
      "1672/3143 [==============>...............] - ETA: 15s - loss: 1.5422 - Spearman: 0.6192 - pearson_r: 0.6285\n",
      "1678/3143 [===============>..............] - ETA: 15s - loss: 1.5424 - Spearman: 0.6192 - pearson_r: 0.6284\n",
      "1684/3143 [===============>..............] - ETA: 14s - loss: 1.5418 - Spearman: 0.6192 - pearson_r: 0.6285\n",
      "1690/3143 [===============>..............] - ETA: 14s - loss: 1.5415 - Spearman: 0.6193 - pearson_r: 0.6286\n",
      "1695/3143 [===============>..............] - ETA: 14s - loss: 1.5415 - Spearman: 0.6193 - pearson_r: 0.6286\n",
      "1705/3143 [===============>..............] - ETA: 14s - loss: 1.5414 - Spearman: 0.6194 - pearson_r: 0.6288\n",
      "1715/3143 [===============>..............] - ETA: 14s - loss: 1.5414 - Spearman: 0.6195 - pearson_r: 0.6289\n",
      "1721/3143 [===============>..............] - ETA: 14s - loss: 1.5413 - Spearman: 0.6195 - pearson_r: 0.6288\n",
      "1727/3143 [===============>..............] - ETA: 14s - loss: 1.5418 - Spearman: 0.6196 - pearson_r: 0.6288\n",
      "1737/3143 [===============>..............] - ETA: 14s - loss: 1.5415 - Spearman: 0.6196 - pearson_r: 0.6289\n",
      "1747/3143 [===============>..............] - ETA: 14s - loss: 1.5417 - Spearman: 0.6197 - pearson_r: 0.6291\n",
      "1757/3143 [===============>..............] - ETA: 14s - loss: 1.5422 - Spearman: 0.6197 - pearson_r: 0.6289\n",
      "1767/3143 [===============>..............] - ETA: 14s - loss: 1.5423 - Spearman: 0.6196 - pearson_r: 0.6289\n",
      "1778/3143 [===============>..............] - ETA: 13s - loss: 1.5415 - Spearman: 0.6197 - pearson_r: 0.6290\n",
      "1789/3143 [================>.............] - ETA: 13s - loss: 1.5413 - Spearman: 0.6199 - pearson_r: 0.6292\n",
      "1795/3143 [================>.............] - ETA: 13s - loss: 1.5412 - Spearman: 0.6199 - pearson_r: 0.6292\n",
      "1807/3143 [================>.............] - ETA: 13s - loss: 1.5412 - Spearman: 0.6200 - pearson_r: 0.6293\n",
      "1819/3143 [================>.............] - ETA: 13s - loss: 1.5413 - Spearman: 0.6199 - pearson_r: 0.6292\n",
      "1830/3143 [================>.............] - ETA: 13s - loss: 1.5412 - Spearman: 0.6199 - pearson_r: 0.6293\n",
      "1842/3143 [================>.............] - ETA: 13s - loss: 1.5402 - Spearman: 0.6199 - pearson_r: 0.6293\n",
      "1852/3143 [================>.............] - ETA: 13s - loss: 1.5399 - Spearman: 0.6199 - pearson_r: 0.6292\n",
      "1863/3143 [================>.............] - ETA: 13s - loss: 1.5400 - Spearman: 0.6200 - pearson_r: 0.6293\n",
      "1874/3143 [================>.............] - ETA: 12s - loss: 1.5397 - Spearman: 0.6202 - pearson_r: 0.6293\n",
      "1885/3143 [================>.............] - ETA: 12s - loss: 1.5399 - Spearman: 0.6203 - pearson_r: 0.6294\n",
      "1896/3143 [=================>............] - ETA: 12s - loss: 1.5396 - Spearman: 0.6204 - pearson_r: 0.6295\n",
      "1902/3143 [=================>............] - ETA: 12s - loss: 1.5396 - Spearman: 0.6203 - pearson_r: 0.6293\n",
      "1914/3143 [=================>............] - ETA: 12s - loss: 1.5397 - Spearman: 0.6202 - pearson_r: 0.6293\n",
      "1926/3143 [=================>............] - ETA: 12s - loss: 1.5396 - Spearman: 0.6202 - pearson_r: 0.6294\n",
      "1936/3143 [=================>............] - ETA: 12s - loss: 1.5396 - Spearman: 0.6202 - pearson_r: 0.6294\n",
      "1948/3143 [=================>............] - ETA: 12s - loss: 1.5395 - Spearman: 0.6202 - pearson_r: 0.6294\n",
      "1960/3143 [=================>............] - ETA: 12s - loss: 1.5393 - Spearman: 0.6203 - pearson_r: 0.6294\n",
      "1970/3143 [=================>............] - ETA: 11s - loss: 1.5392 - Spearman: 0.6205 - pearson_r: 0.6296\n",
      "1981/3143 [=================>............] - ETA: 11s - loss: 1.5393 - Spearman: 0.6205 - pearson_r: 0.6296\n",
      "1993/3143 [==================>...........] - ETA: 11s - loss: 1.5396 - Spearman: 0.6205 - pearson_r: 0.6295\n",
      "1999/3143 [==================>...........] - ETA: 11s - loss: 1.5394 - Spearman: 0.6205 - pearson_r: 0.6295\n",
      "2005/3143 [==================>...........] - ETA: 11s - loss: 1.5393 - Spearman: 0.6206 - pearson_r: 0.6296\n",
      "2011/3143 [==================>...........] - ETA: 11s - loss: 1.5391 - Spearman: 0.6206 - pearson_r: 0.6296\n",
      "2022/3143 [==================>...........] - ETA: 11s - loss: 1.5388 - Spearman: 0.6207 - pearson_r: 0.6297\n",
      "2034/3143 [==================>...........] - ETA: 11s - loss: 1.5388 - Spearman: 0.6208 - pearson_r: 0.6296\n",
      "2045/3143 [==================>...........] - ETA: 11s - loss: 1.5394 - Spearman: 0.6210 - pearson_r: 0.6297\n",
      "2057/3143 [==================>...........] - ETA: 11s - loss: 1.5399 - Spearman: 0.6211 - pearson_r: 0.6297\n",
      "2069/3143 [==================>...........] - ETA: 10s - loss: 1.5399 - Spearman: 0.6211 - pearson_r: 0.6297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080/3143 [==================>...........] - ETA: 10s - loss: 1.5396 - Spearman: 0.6212 - pearson_r: 0.6298\n",
      "2091/3143 [==================>...........] - ETA: 10s - loss: 1.5390 - Spearman: 0.6211 - pearson_r: 0.6299\n",
      "2102/3143 [===================>..........] - ETA: 10s - loss: 1.5391 - Spearman: 0.6212 - pearson_r: 0.6299\n",
      "2112/3143 [===================>..........] - ETA: 10s - loss: 1.5389 - Spearman: 0.6213 - pearson_r: 0.6301\n",
      "2122/3143 [===================>..........] - ETA: 10s - loss: 1.5392 - Spearman: 0.6213 - pearson_r: 0.6300\n",
      "2132/3143 [===================>..........] - ETA: 10s - loss: 1.5389 - Spearman: 0.6214 - pearson_r: 0.6302\n",
      "2144/3143 [===================>..........] - ETA: 10s - loss: 1.5388 - Spearman: 0.6214 - pearson_r: 0.6302\n",
      "2154/3143 [===================>..........] - ETA: 10s - loss: 1.5395 - Spearman: 0.6213 - pearson_r: 0.6301\n",
      "2164/3143 [===================>..........] - ETA: 9s - loss: 1.5391 - Spearman: 0.6214 - pearson_r: 0.6302 \n",
      "2169/3143 [===================>..........] - ETA: 9s - loss: 1.5392 - Spearman: 0.6214 - pearson_r: 0.6301\n",
      "2180/3143 [===================>..........] - ETA: 9s - loss: 1.5393 - Spearman: 0.6214 - pearson_r: 0.6300\n",
      "2195/3143 [===================>..........] - ETA: 9s - loss: 1.5395 - Spearman: 0.6214 - pearson_r: 0.6298\n",
      "2200/3143 [===================>..........] - ETA: 9s - loss: 1.5396 - Spearman: 0.6214 - pearson_r: 0.6298\n",
      "2205/3143 [====================>.........] - ETA: 9s - loss: 1.5393 - Spearman: 0.6214 - pearson_r: 0.6299\n",
      "2216/3143 [====================>.........] - ETA: 9s - loss: 1.5390 - Spearman: 0.6214 - pearson_r: 0.6300\n",
      "2222/3143 [====================>.........] - ETA: 9s - loss: 1.5388 - Spearman: 0.6214 - pearson_r: 0.6301\n",
      "2233/3143 [====================>.........] - ETA: 9s - loss: 1.5390 - Spearman: 0.6214 - pearson_r: 0.6301\n",
      "2244/3143 [====================>.........] - ETA: 9s - loss: 1.5387 - Spearman: 0.6214 - pearson_r: 0.6301\n",
      "2255/3143 [====================>.........] - ETA: 9s - loss: 1.5387 - Spearman: 0.6213 - pearson_r: 0.6301\n",
      "2266/3143 [====================>.........] - ETA: 8s - loss: 1.5387 - Spearman: 0.6213 - pearson_r: 0.6301\n",
      "2276/3143 [====================>.........] - ETA: 8s - loss: 1.5388 - Spearman: 0.6213 - pearson_r: 0.6301\n",
      "2286/3143 [====================>.........] - ETA: 8s - loss: 1.5388 - Spearman: 0.6212 - pearson_r: 0.6301\n",
      "2296/3143 [====================>.........] - ETA: 8s - loss: 1.5392 - Spearman: 0.6212 - pearson_r: 0.6300\n",
      "2306/3143 [=====================>........] - ETA: 8s - loss: 1.5395 - Spearman: 0.6212 - pearson_r: 0.6300\n",
      "2318/3143 [=====================>........] - ETA: 8s - loss: 1.5393 - Spearman: 0.6211 - pearson_r: 0.6299\n",
      "2329/3143 [=====================>........] - ETA: 8s - loss: 1.5392 - Spearman: 0.6211 - pearson_r: 0.6299\n",
      "2340/3143 [=====================>........] - ETA: 8s - loss: 1.5391 - Spearman: 0.6212 - pearson_r: 0.6301\n",
      "2352/3143 [=====================>........] - ETA: 8s - loss: 1.5386 - Spearman: 0.6212 - pearson_r: 0.6302\n",
      "2363/3143 [=====================>........] - ETA: 7s - loss: 1.5381 - Spearman: 0.6212 - pearson_r: 0.6303\n",
      "2368/3143 [=====================>........] - ETA: 7s - loss: 1.5382 - Spearman: 0.6212 - pearson_r: 0.6303\n",
      "2379/3143 [=====================>........] - ETA: 7s - loss: 1.5379 - Spearman: 0.6212 - pearson_r: 0.6302\n",
      "2384/3143 [=====================>........] - ETA: 7s - loss: 1.5379 - Spearman: 0.6213 - pearson_r: 0.6302\n",
      "2389/3143 [=====================>........] - ETA: 7s - loss: 1.5378 - Spearman: 0.6213 - pearson_r: 0.6303\n",
      "2400/3143 [=====================>........] - ETA: 7s - loss: 1.5381 - Spearman: 0.6213 - pearson_r: 0.6302\n",
      "2411/3143 [======================>.......] - ETA: 7s - loss: 1.5382 - Spearman: 0.6214 - pearson_r: 0.6303\n",
      "2421/3143 [======================>.......] - ETA: 7s - loss: 1.5378 - Spearman: 0.6214 - pearson_r: 0.6303\n",
      "2433/3143 [======================>.......] - ETA: 7s - loss: 1.5379 - Spearman: 0.6214 - pearson_r: 0.6303\n",
      "2445/3143 [======================>.......] - ETA: 7s - loss: 1.5383 - Spearman: 0.6215 - pearson_r: 0.6303\n",
      "2457/3143 [======================>.......] - ETA: 6s - loss: 1.5386 - Spearman: 0.6216 - pearson_r: 0.6304\n",
      "2469/3143 [======================>.......] - ETA: 6s - loss: 1.5386 - Spearman: 0.6216 - pearson_r: 0.6303\n",
      "2481/3143 [======================>.......] - ETA: 6s - loss: 1.5387 - Spearman: 0.6216 - pearson_r: 0.6304\n",
      "2487/3143 [======================>.......] - ETA: 6s - loss: 1.5384 - Spearman: 0.6217 - pearson_r: 0.6304\n",
      "2499/3143 [======================>.......] - ETA: 6s - loss: 1.5390 - Spearman: 0.6216 - pearson_r: 0.6304\n",
      "2510/3143 [======================>.......] - ETA: 6s - loss: 1.5389 - Spearman: 0.6216 - pearson_r: 0.6304\n",
      "2522/3143 [=======================>......] - ETA: 6s - loss: 1.5388 - Spearman: 0.6216 - pearson_r: 0.6304\n",
      "2533/3143 [=======================>......] - ETA: 6s - loss: 1.5387 - Spearman: 0.6216 - pearson_r: 0.6304\n",
      "2544/3143 [=======================>......] - ETA: 6s - loss: 1.5386 - Spearman: 0.6215 - pearson_r: 0.6304\n",
      "2554/3143 [=======================>......] - ETA: 5s - loss: 1.5387 - Spearman: 0.6216 - pearson_r: 0.6304\n",
      "2565/3143 [=======================>......] - ETA: 5s - loss: 1.5385 - Spearman: 0.6216 - pearson_r: 0.6305\n",
      "2577/3143 [=======================>......] - ETA: 5s - loss: 1.5378 - Spearman: 0.6215 - pearson_r: 0.6305\n",
      "2589/3143 [=======================>......] - ETA: 5s - loss: 1.5378 - Spearman: 0.6216 - pearson_r: 0.6306\n",
      "2595/3143 [=======================>......] - ETA: 5s - loss: 1.5375 - Spearman: 0.6216 - pearson_r: 0.6306\n",
      "2607/3143 [=======================>......] - ETA: 5s - loss: 1.5375 - Spearman: 0.6216 - pearson_r: 0.6307\n",
      "2619/3143 [=======================>......] - ETA: 5s - loss: 1.5373 - Spearman: 0.6218 - pearson_r: 0.6308\n",
      "2629/3143 [========================>.....] - ETA: 5s - loss: 1.5372 - Spearman: 0.6218 - pearson_r: 0.6308\n",
      "2640/3143 [========================>.....] - ETA: 5s - loss: 1.5369 - Spearman: 0.6218 - pearson_r: 0.6309\n",
      "2651/3143 [========================>.....] - ETA: 4s - loss: 1.5366 - Spearman: 0.6219 - pearson_r: 0.6310\n",
      "2662/3143 [========================>.....] - ETA: 4s - loss: 1.5366 - Spearman: 0.6219 - pearson_r: 0.6309\n",
      "2673/3143 [========================>.....] - ETA: 4s - loss: 1.5366 - Spearman: 0.6219 - pearson_r: 0.6310\n",
      "2685/3143 [========================>.....] - ETA: 4s - loss: 1.5366 - Spearman: 0.6220 - pearson_r: 0.6310\n",
      "2695/3143 [========================>.....] - ETA: 4s - loss: 1.5365 - Spearman: 0.6220 - pearson_r: 0.6310\n",
      "2705/3143 [========================>.....] - ETA: 4s - loss: 1.5363 - Spearman: 0.6221 - pearson_r: 0.6310\n",
      "2715/3143 [========================>.....] - ETA: 4s - loss: 1.5365 - Spearman: 0.6221 - pearson_r: 0.6311\n",
      "2725/3143 [=========================>....] - ETA: 4s - loss: 1.5365 - Spearman: 0.6221 - pearson_r: 0.6310\n",
      "2737/3143 [=========================>....] - ETA: 4s - loss: 1.5363 - Spearman: 0.6221 - pearson_r: 0.6310\n",
      "2748/3143 [=========================>....] - ETA: 4s - loss: 1.5361 - Spearman: 0.6221 - pearson_r: 0.6310\n",
      "2758/3143 [=========================>....] - ETA: 3s - loss: 1.5358 - Spearman: 0.6220 - pearson_r: 0.6310\n",
      "2770/3143 [=========================>....] - ETA: 3s - loss: 1.5355 - Spearman: 0.6220 - pearson_r: 0.6311\n",
      "2782/3143 [=========================>....] - ETA: 3s - loss: 1.5361 - Spearman: 0.6220 - pearson_r: 0.6310\n",
      "2787/3143 [=========================>....] - ETA: 3s - loss: 1.5360 - Spearman: 0.6221 - pearson_r: 0.6311\n",
      "2793/3143 [=========================>....] - ETA: 3s - loss: 1.5360 - Spearman: 0.6221 - pearson_r: 0.6310\n",
      "2799/3143 [=========================>....] - ETA: 3s - loss: 1.5358 - Spearman: 0.6221 - pearson_r: 0.6310\n",
      "2811/3143 [=========================>....] - ETA: 3s - loss: 1.5359 - Spearman: 0.6220 - pearson_r: 0.6310\n",
      "2823/3143 [=========================>....] - ETA: 3s - loss: 1.5356 - Spearman: 0.6220 - pearson_r: 0.6310\n",
      "2835/3143 [==========================>...] - ETA: 3s - loss: 1.5355 - Spearman: 0.6220 - pearson_r: 0.6310\n",
      "2846/3143 [==========================>...] - ETA: 3s - loss: 1.5352 - Spearman: 0.6220 - pearson_r: 0.6311\n",
      "2858/3143 [==========================>...] - ETA: 2s - loss: 1.5354 - Spearman: 0.6221 - pearson_r: 0.6312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870/3143 [==========================>...] - ETA: 2s - loss: 1.5356 - Spearman: 0.6221 - pearson_r: 0.6312\n",
      "2881/3143 [==========================>...] - ETA: 2s - loss: 1.5355 - Spearman: 0.6221 - pearson_r: 0.6312\n",
      "2891/3143 [==========================>...] - ETA: 2s - loss: 1.5358 - Spearman: 0.6220 - pearson_r: 0.6311\n",
      "2901/3143 [==========================>...] - ETA: 2s - loss: 1.5357 - Spearman: 0.6220 - pearson_r: 0.6311\n",
      "2912/3143 [==========================>...] - ETA: 2s - loss: 1.5353 - Spearman: 0.6220 - pearson_r: 0.6311\n",
      "2923/3143 [==========================>...] - ETA: 2s - loss: 1.5352 - Spearman: 0.6220 - pearson_r: 0.6311\n",
      "2934/3143 [===========================>..] - ETA: 2s - loss: 1.5351 - Spearman: 0.6221 - pearson_r: 0.6312\n",
      "2946/3143 [===========================>..] - ETA: 1s - loss: 1.5350 - Spearman: 0.6221 - pearson_r: 0.6312\n",
      "2952/3143 [===========================>..] - ETA: 1s - loss: 1.5353 - Spearman: 0.6221 - pearson_r: 0.6312\n",
      "2958/3143 [===========================>..] - ETA: 1s - loss: 1.5353 - Spearman: 0.6221 - pearson_r: 0.6312\n",
      "2964/3143 [===========================>..] - ETA: 1s - loss: 1.5352 - Spearman: 0.6221 - pearson_r: 0.6312\n",
      "2969/3143 [===========================>..] - ETA: 1s - loss: 1.5350 - Spearman: 0.6221 - pearson_r: 0.6311\n",
      "2975/3143 [===========================>..] - ETA: 1s - loss: 1.5349 - Spearman: 0.6221 - pearson_r: 0.6312\n",
      "2986/3143 [===========================>..] - ETA: 1s - loss: 1.5344 - Spearman: 0.6221 - pearson_r: 0.6312\n",
      "2998/3143 [===========================>..] - ETA: 1s - loss: 1.5344 - Spearman: 0.6222 - pearson_r: 0.6312\n",
      "3009/3143 [===========================>..] - ETA: 1s - loss: 1.5344 - Spearman: 0.6222 - pearson_r: 0.6313\n",
      "3019/3143 [===========================>..] - ETA: 1s - loss: 1.5350 - Spearman: 0.6222 - pearson_r: 0.6312\n",
      "3029/3143 [===========================>..] - ETA: 1s - loss: 1.5349 - Spearman: 0.6221 - pearson_r: 0.6311\n",
      "3041/3143 [============================>.] - ETA: 1s - loss: 1.5348 - Spearman: 0.6223 - pearson_r: 0.6312\n",
      "3052/3143 [============================>.] - ETA: 0s - loss: 1.5349 - Spearman: 0.6223 - pearson_r: 0.6313\n",
      "3063/3143 [============================>.] - ETA: 0s - loss: 1.5353 - Spearman: 0.6223 - pearson_r: 0.6313\n",
      "3073/3143 [============================>.] - ETA: 0s - loss: 1.5353 - Spearman: 0.6223 - pearson_r: 0.6312\n",
      "3084/3143 [============================>.] - ETA: 0s - loss: 1.5352 - Spearman: 0.6224 - pearson_r: 0.6313\n",
      "3094/3143 [============================>.] - ETA: 0s - loss: 1.5352 - Spearman: 0.6224 - pearson_r: 0.6313\n",
      "3105/3143 [============================>.] - ETA: 0s - loss: 1.5355 - Spearman: 0.6224 - pearson_r: 0.6313\n",
      "3115/3143 [============================>.] - ETA: 0s - loss: 1.5353 - Spearman: 0.6224 - pearson_r: 0.6312\n",
      "3126/3143 [============================>.] - ETA: 0s - loss: 1.5354 - Spearman: 0.6224 - pearson_r: 0.6313\n",
      "3137/3143 [============================>.] - ETA: 0s - loss: 1.5353 - Spearman: 0.6225 - pearson_r: 0.6312\n",
      "3142/3143 [============================>.] - ETA: 0s - loss: 1.5354 - Spearman: 0.6225 - pearson_r: 0.6313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 10:32:35,171\tWARNING tune.py:147 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-05-04 10:32:35,892\tERROR tune.py:794 -- Trials did not complete: [tune_hominid_2db57_00000, tune_hominid_2db57_00001, tune_hominid_2db57_00002, tune_hominid_2db57_00003, tune_hominid_2db57_00004, tune_hominid_2db57_00005, tune_hominid_2db57_00006, tune_hominid_2db57_00007, tune_hominid_2db57_00008, tune_hominid_2db57_00009, tune_hominid_2db57_00010, tune_hominid_2db57_00011, tune_hominid_2db57_00012, tune_hominid_2db57_00013, tune_hominid_2db57_00014, tune_hominid_2db57_00015, tune_hominid_2db57_00016, tune_hominid_2db57_00017, tune_hominid_2db57_00018, tune_hominid_2db57_00019, tune_hominid_2db57_00020, tune_hominid_2db57_00021, tune_hominid_2db57_00022, tune_hominid_2db57_00023, tune_hominid_2db57_00024, tune_hominid_2db57_00025, tune_hominid_2db57_00026, tune_hominid_2db57_00027, tune_hominid_2db57_00028, tune_hominid_2db57_00029, tune_hominid_2db57_00030, tune_hominid_2db57_00031, tune_hominid_2db57_00032, tune_hominid_2db57_00033, tune_hominid_2db57_00034, tune_hominid_2db57_00035]\n",
      "2023-05-04 10:32:35,893\tINFO tune.py:799 -- Total run time: 143.74 seconds (143.69 seconds for the tuning loop).\n",
      "2023-05-04 10:32:35,894\tWARNING tune.py:805 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281186)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3143/3143 [==============================] - 34s 11ms/step - loss: 1.5354 - Spearman: 0.6225 - pearson_r: 0.6313 - val_loss: 1.4366 - val_Spearman: 0.5994 - val_pearson_r: 0.5631\n",
      "\u001b[2m\u001b[36m(tune_hominid pid=1281183)\u001b[0m Epoch 4/60\n",
      "   1/3143 [..............................] - ETA: 34s - loss: 1.2442 - Spearman: 0.6277 - pearson_r: 0.7164\n",
      "   6/3143 [..............................] - ETA: 35s - loss: 1.5836 - Spearman: 0.6205 - pearson_r: 0.6557\n",
      "  11/3143 [..............................] - ETA: 35s - loss: 1.5612 - Spearman: 0.6241 - pearson_r: 0.6511\n",
      "Best hyperparameters found were:  {'conv1_activation': 'relu', 'conv1_batchnorm': False, 'conv1_channel_weight': None, 'conv1_dropout': 0.2, 'conv1_filters': 96, 'conv1_kernel_size': 19, 'conv1_pool_type': 'max_pool', 'conv1_max_pool': 10, 'conv1_attention_pool_size': 0, 'conv1_type': 'standard', 'dense_activation': 'relu', 'dense_batchnorm': True, 'dense_dropout': [0.3, 0.3], 'dense_units': [512, 512], 'mha_d_model': 192, 'mha_dropout': 0.1, 'mha_head_type': 'pool', 'mha_heads': 8, 'mha_layernorm': False, 'output_activation': 'linear', 'output_shape': None}\n"
     ]
    }
   ],
   "source": [
    "smoke_test = False  # For testing purposes: set this to False to run the full experiment\n",
    "analysis = tune.run(\n",
    "    tune_hominid,\n",
    "    name=\"tune_hominid_pipeline-full\",\n",
    "    callbacks=[WandbLoggerCallback(\n",
    "        project=\"raytune-hominid_pipeline-full\",\n",
    "        log_config=True,\n",
    "        upload_checkpoints=True,\n",
    "        save_checkpoints=True\n",
    "    )],\n",
    "    scheduler=AsyncHyperBandScheduler(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=400,\n",
    "        grace_period=20\n",
    "    ),\n",
    "    metric=\"val_pearson_r\",\n",
    "    mode=\"max\",\n",
    "    stop={\n",
    "        \"val_pearson_r\": 0.9,\n",
    "        \"training_iteration\": 5 if smoke_test else 100\n",
    "    },\n",
    "    num_samples=1 if smoke_test else 100,\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 4,\n",
    "        \"gpu\": 1\n",
    "    },\n",
    "    config=config,\n",
    "\n",
    ")\n",
    "\n",
    "RESULTS_DIR = '/home/chandana/projects/hominid_pipeline/temp/'\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "        analysis.best_config,\n",
    "        orient='index'\n",
    ").to_csv(\n",
    "        RESULTS_DIR + 'tune_best.csv',\n",
    "        header=True,\n",
    "        index=True\n",
    "        )\n",
    "analysis.results_df.to_csv(RESULTS_DIR + 'tune_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc343283",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_zoo.base_model(**config)\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test, model\n",
    "\n",
    "def tune_hominid(config: dict):\n",
    "\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test, model = hominid_pipeline(config)\n",
    "\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(lr=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[Spearman, pearson_r]\n",
    "        )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9373039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/chandana/projects/hominid_pipeline/temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94cc5c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (80000, 249, 4). Output shape: 2\n",
      "2\n",
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 17:01:09.708727: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 17:01:10.237977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14239 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/home/chandana/projects/hominid_pipeline/model_zoo.py:11: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  self.pool_size = pool_size\n",
      "/home/chandana/projects/hominid_pipeline/model_zoo.py:12: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"attention_pooling\" (type AttentionPooling).\n\nin user code:\n\n    File \"/home/chandana/projects/hominid_pipeline/model_zoo.py\", line 28, in call  *\n        raw_weights = self.dense(inputs)\n    File \"/home/chandana/miniforge3/envs/deepstarr/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'conv1d' (type Conv1D).\n    \n    Negative dimension size caused by subtracting 1 from 0 for '{{node attention_pooling/conv1d/Conv1D/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](attention_pooling/conv1d/Conv1D/Reshape, attention_pooling/conv1d/Conv1D/ExpandDims_1)' with input shapes: [?,1,0,128], [1,1,128,128].\n    \n    Call arguments received by layer 'conv1d' (type Conv1D):\n      â€¢ inputs=tf.Tensor(shape=(None, 0, 0, 128), dtype=float32)\n\n\nCall arguments received by layer \"attention_pooling\" (type AttentionPooling):\n  â€¢ inputs=tf.Tensor(shape=(None, 249, 128), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1120103/2175324414.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# wandb.init(project='raytune-hominid_pipeline', name='test_2', config=config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhominid_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m model.compile(\n",
      "\u001b[0;32m/tmp/ipykernel_1120103/2423438135.py\u001b[0m in \u001b[0;36mhominid_pipeline\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hominid_pipeline/model_zoo.py\u001b[0m in \u001b[0;36mbase_model\u001b[0;34m(conv1_activation, conv1_batchnorm, conv1_channel_weight, conv1_dropout, conv1_filters, conv1_kernel_size, conv1_pool_type, conv1_attention_pool_size, conv1_max_pool, conv1_type, dense_activation, dense_batchnorm, dense_dropout, dense_units, input_shape, mha_d_model, mha_dropout, mha_head_type, mha_heads, mha_layernorm, output_activation, output_shape)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softconv_activation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconv1_pool_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'attention'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionPooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_attention_pool_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_max_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1_maxpool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/deepstarr/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hominid_pipeline/model_zoo.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCropping1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mraw_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0matt_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"attention_pooling\" (type AttentionPooling).\n\nin user code:\n\n    File \"/home/chandana/projects/hominid_pipeline/model_zoo.py\", line 28, in call  *\n        raw_weights = self.dense(inputs)\n    File \"/home/chandana/miniforge3/envs/deepstarr/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'conv1d' (type Conv1D).\n    \n    Negative dimension size caused by subtracting 1 from 0 for '{{node attention_pooling/conv1d/Conv1D/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](attention_pooling/conv1d/Conv1D/Reshape, attention_pooling/conv1d/Conv1D/ExpandDims_1)' with input shapes: [?,1,0,128], [1,1,128,128].\n    \n    Call arguments received by layer 'conv1d' (type Conv1D):\n      â€¢ inputs=tf.Tensor(shape=(None, 0, 0, 128), dtype=float32)\n\n\nCall arguments received by layer \"attention_pooling\" (type AttentionPooling):\n  â€¢ inputs=tf.Tensor(shape=(None, 249, 128), dtype=float32)"
     ]
    }
   ],
   "source": [
    "importlib.reload(model_zoo)\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"conv1_activation\": np.random.choice([\"relu\"]), # relu\n",
    "    \"conv1_batchnorm\": np.random.choice([False]), \n",
    "    \"conv1_channel_weight\": np.random.choice([\"softconv\", \"se\"]),\n",
    "    \"conv1_dropout\": 0.2,\n",
    "    \"conv1_filters\": np.random.choice([256, 512]), # go to 256, 512\n",
    "    \"conv1_kernel_size\": int(np.random.choice([15, 19])), # go to 11 15 19\n",
    "    \"conv1_max_pool\": 10,\n",
    "    \"conv1_pool_type\": \"attention\", \n",
    "    \"conv1_attention_pool_size\": np.random.choice(range(40)), # --> tune.choice(0-40)\n",
    "    \"conv1_type\": np.random.choice([\"pw\", \"standard\"]),\n",
    "    \"dense_activation\": \"relu\",\n",
    "    \"dense_batchnorm\": True,\n",
    "    \"dense_dropout\": [0.3, 0.3], # go to 0.30.3 to 0.5 0.5\n",
    "    \"dense_units\": [128, 256],\n",
    "    \"input_shape\": None,\n",
    "    \"mha_d_model\": np.random.choice([96, 192]),\n",
    "    \"mha_dropout\": 0.1,\n",
    "    \"mha_head_type\": np.random.choice([\"pool\", \"task_specific\"]),\n",
    "    \"mha_heads\": np.random.choice([4, 8]),\n",
    "    \"mha_layernorm\": False,\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"output_shape\": None\n",
    "}\n",
    "\n",
    "# wandb.init(project='raytune-hominid_pipeline', name='test_2', config=config)\n",
    "\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test, model = hominid_pipeline(config)\n",
    "\n",
    "model.compile(\n",
    "    tf.keras.optimizers.Adam(lr=0.001),\n",
    "    loss='mse',\n",
    "    metrics=[Spearman, pearson_r]\n",
    "    )\n",
    "model.summary()\n",
    "\n",
    "# train model\n",
    "model.fit(\n",
    "      x_train, y_train,\n",
    "      epochs=1,\n",
    "      batch_size=128,\n",
    "      shuffle=True,\n",
    "      validation_data=(x_valid, y_valid),\n",
    "#     callbacks=[\n",
    "#           wandb.keras.WandbCallback(\n",
    "#               save_model=(True)\n",
    "#           )]\n",
    ")\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c31e31fe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'conv1_activation': 'relu', 'conv1_batchnorm': False, 'conv1_channel_weight': 'softconv', 'conv1_dropout': 0.2, 'conv1_filters': 128, 'conv1_kernel_size': 19, 'conv1_pool_type': 'attention', 'conv1_max_pool': 20, 'conv1_attention_pool_size': 23, 'conv1_type': 'pw', 'dense_activation': 'relu', 'dense_batchnorm': True, 'dense_dropout': [0.3, 0.3], 'dense_units': [256, 128], 'mha_d_model': 192, 'mha_dropout': 0.1, 'mha_head_type': 'task_specific', 'mha_heads': 8, 'mha_layernorm': False, 'output_activation': 'linear', 'output_shape': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb6e2bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(analysis.results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97cd990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'conv1_activation': 'relu', 'conv1_batchnorm': False, 'conv1_channel_weight': 'softconv', 'conv1_dropout': 0.2, 'conv1_filters': 256, 'conv1_kernel_size': 15, 'conv1_pool_type': 'max_pool', 'conv1_max_pool': 8, 'conv1_attention_pool_size': 15, 'conv1_type': 'standard', 'dense_activation': 'relu', 'dense_batchnorm': True, 'dense_dropout': [0.3, 0.3], 'dense_units': [512, 256], 'mha_d_model': 192, 'mha_dropout': 0.1, 'mha_head_type': 'pool', 'mha_heads': 8, 'mha_layernorm': False, 'output_activation': 'linear', 'output_shape': None}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789a48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756712b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65434d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85dc9baf",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb2ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba4c635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>State</th>\n",
       "      <th>Notes</th>\n",
       "      <th>User</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Created</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Sweep</th>\n",
       "      <th>conv1_activation</th>\n",
       "      <th>conv1_batchnorm</th>\n",
       "      <th>...</th>\n",
       "      <th>trial_log_path</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>pearson_r</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>time_this_iter_s</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timesteps_since_restore</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>warmup_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tune_hominid_2e9a0_00005</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T18:46:14.000Z</td>\n",
       "      <td>2557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.888635</td>\n",
       "      <td>2550.752711</td>\n",
       "      <td>42.304018</td>\n",
       "      <td>2550.752711</td>\n",
       "      <td>1680809328</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.005102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tune_hominid_2e9a0_00046</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T08:13:10.000Z</td>\n",
       "      <td>1443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.888559</td>\n",
       "      <td>1436.601748</td>\n",
       "      <td>23.748997</td>\n",
       "      <td>1436.601748</td>\n",
       "      <td>1680856630</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.005109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tune_hominid_2e9a0_00022</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T00:49:25.000Z</td>\n",
       "      <td>4241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.886563</td>\n",
       "      <td>4234.652682</td>\n",
       "      <td>70.478082</td>\n",
       "      <td>4234.652682</td>\n",
       "      <td>1680832803</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.005227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tune_hominid_2e9a0_00007</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T19:56:51.000Z</td>\n",
       "      <td>3070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.885639</td>\n",
       "      <td>3064.092025</td>\n",
       "      <td>50.814463</td>\n",
       "      <td>3064.092025</td>\n",
       "      <td>1680814079</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.005103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tune_hominid_2e9a0_00036</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T04:54:07.000Z</td>\n",
       "      <td>4173</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.885317</td>\n",
       "      <td>4166.478738</td>\n",
       "      <td>69.190704</td>\n",
       "      <td>4166.478738</td>\n",
       "      <td>1680847417</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.004926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tune_hominid_2e9a0_00049</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T08:54:30.000Z</td>\n",
       "      <td>1611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.885305</td>\n",
       "      <td>1604.369986</td>\n",
       "      <td>26.542195</td>\n",
       "      <td>1604.369986</td>\n",
       "      <td>1680859277</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.006063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tune_hominid_2e9a0_00020</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T23:52:14.000Z</td>\n",
       "      <td>2829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.881746</td>\n",
       "      <td>2823.065363</td>\n",
       "      <td>46.830002</td>\n",
       "      <td>2823.065363</td>\n",
       "      <td>1680827960</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.005236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tune_hominid_2e9a0_00006</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T19:28:54.000Z</td>\n",
       "      <td>1674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.878790</td>\n",
       "      <td>1667.439738</td>\n",
       "      <td>27.505669</td>\n",
       "      <td>1667.439738</td>\n",
       "      <td>1680811005</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.004855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tune_hominid_2e9a0_00026</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T02:32:52.000Z</td>\n",
       "      <td>2182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.876017</td>\n",
       "      <td>2174.928653</td>\n",
       "      <td>36.118381</td>\n",
       "      <td>2174.928653</td>\n",
       "      <td>1680836951</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.005168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tune_hominid_2e9a0_00016</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T22:45:32.000Z</td>\n",
       "      <td>1813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.873120</td>\n",
       "      <td>1806.488476</td>\n",
       "      <td>29.868063</td>\n",
       "      <td>1806.488476</td>\n",
       "      <td>1680822941</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.004921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tune_hominid_2e9a0_00037</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T06:03:43.000Z</td>\n",
       "      <td>1527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.872167</td>\n",
       "      <td>1520.935762</td>\n",
       "      <td>25.124688</td>\n",
       "      <td>1520.935762</td>\n",
       "      <td>1680848947</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.005152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tune_hominid_2e9a0_00003</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T17:06:21.000Z</td>\n",
       "      <td>4684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.869140</td>\n",
       "      <td>4676.966161</td>\n",
       "      <td>77.622760</td>\n",
       "      <td>4676.966161</td>\n",
       "      <td>1680805461</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.004710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tune_hominid_2e9a0_00004</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T18:24:28.000Z</td>\n",
       "      <td>1303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.843212</td>\n",
       "      <td>1298.102059</td>\n",
       "      <td>21.565726</td>\n",
       "      <td>1298.102059</td>\n",
       "      <td>1680806769</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.005467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tune_hominid_2e9a0_00014</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T22:20:29.000Z</td>\n",
       "      <td>627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.826601</td>\n",
       "      <td>620.144239</td>\n",
       "      <td>30.410017</td>\n",
       "      <td>620.144239</td>\n",
       "      <td>1680820253</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.006064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tune_hominid_2e9a0_00031</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T03:47:20.000Z</td>\n",
       "      <td>569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.825475</td>\n",
       "      <td>562.840743</td>\n",
       "      <td>27.692188</td>\n",
       "      <td>562.840743</td>\n",
       "      <td>1680839807</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tune_hominid_2e9a0_00011</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T21:39:01.000Z</td>\n",
       "      <td>976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.824746</td>\n",
       "      <td>969.997311</td>\n",
       "      <td>47.947724</td>\n",
       "      <td>969.997311</td>\n",
       "      <td>1680818115</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.006332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tune_hominid_2e9a0_00025</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T02:24:36.000Z</td>\n",
       "      <td>494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.823369</td>\n",
       "      <td>487.806197</td>\n",
       "      <td>23.776015</td>\n",
       "      <td>487.806197</td>\n",
       "      <td>1680834767</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.006481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tune_hominid_2e9a0_00008</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T20:48:04.000Z</td>\n",
       "      <td>760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.820063</td>\n",
       "      <td>753.712867</td>\n",
       "      <td>37.318596</td>\n",
       "      <td>753.712867</td>\n",
       "      <td>1680814841</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tune_hominid_2e9a0_00038</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T06:29:13.000Z</td>\n",
       "      <td>577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.818451</td>\n",
       "      <td>570.872665</td>\n",
       "      <td>27.958805</td>\n",
       "      <td>570.872665</td>\n",
       "      <td>1680849528</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tune_hominid_2e9a0_00042</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T07:15:51.000Z</td>\n",
       "      <td>519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.818424</td>\n",
       "      <td>513.100456</td>\n",
       "      <td>25.082859</td>\n",
       "      <td>513.100456</td>\n",
       "      <td>1680852267</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tune_hominid_2e9a0_00045</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T07:55:34.000Z</td>\n",
       "      <td>1053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.815022</td>\n",
       "      <td>1046.768218</td>\n",
       "      <td>51.672004</td>\n",
       "      <td>1046.768218</td>\n",
       "      <td>1680855184</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tune_hominid_2e9a0_00047</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T08:37:16.000Z</td>\n",
       "      <td>588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.812648</td>\n",
       "      <td>580.902604</td>\n",
       "      <td>28.531430</td>\n",
       "      <td>580.902604</td>\n",
       "      <td>1680857221</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tune_hominid_2e9a0_00018</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T23:26:32.000Z</td>\n",
       "      <td>589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.810611</td>\n",
       "      <td>582.539415</td>\n",
       "      <td>28.513494</td>\n",
       "      <td>582.539415</td>\n",
       "      <td>1680824178</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tune_hominid_2e9a0_00021</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T00:39:27.000Z</td>\n",
       "      <td>594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.807471</td>\n",
       "      <td>588.742311</td>\n",
       "      <td>28.904625</td>\n",
       "      <td>588.742311</td>\n",
       "      <td>1680828559</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tune_hominid_2e9a0_00032</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T03:56:52.000Z</td>\n",
       "      <td>1051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.801176</td>\n",
       "      <td>1045.627580</td>\n",
       "      <td>51.477784</td>\n",
       "      <td>1045.627580</td>\n",
       "      <td>1680840861</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tune_hominid_2e9a0_00019</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T23:36:24.000Z</td>\n",
       "      <td>947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.779853</td>\n",
       "      <td>939.758700</td>\n",
       "      <td>46.244713</td>\n",
       "      <td>939.758700</td>\n",
       "      <td>1680825128</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tune_hominid_2e9a0_00043</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T07:24:34.000Z</td>\n",
       "      <td>1195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.775980</td>\n",
       "      <td>1188.448543</td>\n",
       "      <td>58.768454</td>\n",
       "      <td>1188.448543</td>\n",
       "      <td>1680853466</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tune_hominid_2e9a0_00012</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T21:55:21.000Z</td>\n",
       "      <td>590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.773541</td>\n",
       "      <td>583.721735</td>\n",
       "      <td>28.622427</td>\n",
       "      <td>583.721735</td>\n",
       "      <td>1680818708</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tune_hominid_2e9a0_00017</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T23:15:48.000Z</td>\n",
       "      <td>641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.772441</td>\n",
       "      <td>634.796849</td>\n",
       "      <td>31.141842</td>\n",
       "      <td>634.796849</td>\n",
       "      <td>1680823586</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tune_hominid_2e9a0_00040</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T06:47:39.000Z</td>\n",
       "      <td>585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.771202</td>\n",
       "      <td>579.572993</td>\n",
       "      <td>28.427624</td>\n",
       "      <td>579.572993</td>\n",
       "      <td>1680850642</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tune_hominid_2e9a0_00048</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T08:47:08.000Z</td>\n",
       "      <td>438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.769857</td>\n",
       "      <td>432.248281</td>\n",
       "      <td>21.127588</td>\n",
       "      <td>432.248281</td>\n",
       "      <td>1680857663</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tune_hominid_2e9a0_00044</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T07:44:32.000Z</td>\n",
       "      <td>659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.767811</td>\n",
       "      <td>651.651195</td>\n",
       "      <td>32.042486</td>\n",
       "      <td>651.651195</td>\n",
       "      <td>1680854127</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tune_hominid_2e9a0_00001</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T16:18:56.000Z</td>\n",
       "      <td>1615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.647773</td>\n",
       "      <td>1608.184009</td>\n",
       "      <td>26.622830</td>\n",
       "      <td>1608.184009</td>\n",
       "      <td>1680799548</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.004786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tune_hominid_2e9a0_00024</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T02:09:39.000Z</td>\n",
       "      <td>894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.643570</td>\n",
       "      <td>887.164389</td>\n",
       "      <td>43.533321</td>\n",
       "      <td>887.164389</td>\n",
       "      <td>1680834269</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tune_hominid_2e9a0_00041</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T06:57:28.000Z</td>\n",
       "      <td>1099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.620266</td>\n",
       "      <td>1093.643809</td>\n",
       "      <td>53.500575</td>\n",
       "      <td>1093.643809</td>\n",
       "      <td>1680851745</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tune_hominid_2e9a0_00039</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T06:38:53.000Z</td>\n",
       "      <td>523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.596205</td>\n",
       "      <td>516.120235</td>\n",
       "      <td>25.335435</td>\n",
       "      <td>516.120235</td>\n",
       "      <td>1680850053</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>tune_hominid_2e9a0_00028</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T03:19:47.000Z</td>\n",
       "      <td>530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.549606</td>\n",
       "      <td>523.296041</td>\n",
       "      <td>25.595889</td>\n",
       "      <td>523.296041</td>\n",
       "      <td>1680838114</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tune_hominid_2e9a0_00029</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T03:28:39.000Z</td>\n",
       "      <td>554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.508631</td>\n",
       "      <td>547.702003</td>\n",
       "      <td>26.876401</td>\n",
       "      <td>547.702003</td>\n",
       "      <td>1680838670</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tune_hominid_2e9a0_00035</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T04:45:40.000Z</td>\n",
       "      <td>505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.496781</td>\n",
       "      <td>498.477462</td>\n",
       "      <td>24.476357</td>\n",
       "      <td>498.477462</td>\n",
       "      <td>1680843242</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tune_hominid_2e9a0_00023</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T02:00:09.000Z</td>\n",
       "      <td>567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.471141</td>\n",
       "      <td>560.938578</td>\n",
       "      <td>27.535063</td>\n",
       "      <td>560.938578</td>\n",
       "      <td>1680833373</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tune_hominid_2e9a0_00002</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T16:45:54.000Z</td>\n",
       "      <td>1224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.454841</td>\n",
       "      <td>1217.675331</td>\n",
       "      <td>60.111696</td>\n",
       "      <td>1217.675331</td>\n",
       "      <td>1680800775</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.006577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tune_hominid_2e9a0_00034</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T04:30:49.000Z</td>\n",
       "      <td>889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.390706</td>\n",
       "      <td>882.185165</td>\n",
       "      <td>43.338058</td>\n",
       "      <td>882.185165</td>\n",
       "      <td>1680842734</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>tune_hominid_2e9a0_00030</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T03:37:56.000Z</td>\n",
       "      <td>562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.389506</td>\n",
       "      <td>555.786399</td>\n",
       "      <td>27.167193</td>\n",
       "      <td>555.786399</td>\n",
       "      <td>1680839235</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tune_hominid_2e9a0_00015</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T22:30:59.000Z</td>\n",
       "      <td>869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.377785</td>\n",
       "      <td>862.685724</td>\n",
       "      <td>42.455498</td>\n",
       "      <td>862.685724</td>\n",
       "      <td>1680821126</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tune_hominid_2e9a0_00027</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T03:09:17.000Z</td>\n",
       "      <td>627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.319371</td>\n",
       "      <td>620.068781</td>\n",
       "      <td>30.472592</td>\n",
       "      <td>620.068781</td>\n",
       "      <td>1680837580</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tune_hominid_2e9a0_00010</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T21:15:14.000Z</td>\n",
       "      <td>1425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.306518</td>\n",
       "      <td>1418.605744</td>\n",
       "      <td>70.340443</td>\n",
       "      <td>1418.605744</td>\n",
       "      <td>1680817136</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tune_hominid_2e9a0_00033</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07T04:14:26.000Z</td>\n",
       "      <td>980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.290626</td>\n",
       "      <td>973.195126</td>\n",
       "      <td>48.318849</td>\n",
       "      <td>973.195126</td>\n",
       "      <td>1680841842</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tune_hominid_2e9a0_00009</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T21:00:48.000Z</td>\n",
       "      <td>862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.281040</td>\n",
       "      <td>856.980942</td>\n",
       "      <td>42.365385</td>\n",
       "      <td>856.980942</td>\n",
       "      <td>1680815708</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.007184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tune_hominid_2e9a0_00013</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T22:05:15.000Z</td>\n",
       "      <td>911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.062240</td>\n",
       "      <td>904.774534</td>\n",
       "      <td>44.663335</td>\n",
       "      <td>904.774534</td>\n",
       "      <td>1680819623</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.004964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>tune_hominid_2e9a0_00000</td>\n",
       "      <td>finished</td>\n",
       "      <td>-</td>\n",
       "      <td>ckochath</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-06T15:32:05.000Z</td>\n",
       "      <td>2808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exponential</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>/home/chandana/ray_results/tune_hominid/tune_h...</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2801.542135</td>\n",
       "      <td>46.035861</td>\n",
       "      <td>2801.542135</td>\n",
       "      <td>1680797930</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.004841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name     State Notes      User  Tags  \\\n",
       "0   tune_hominid_2e9a0_00005  finished     -  ckochath   NaN   \n",
       "1   tune_hominid_2e9a0_00046  finished     -  ckochath   NaN   \n",
       "2   tune_hominid_2e9a0_00022  finished     -  ckochath   NaN   \n",
       "3   tune_hominid_2e9a0_00007  finished     -  ckochath   NaN   \n",
       "4   tune_hominid_2e9a0_00036  finished     -  ckochath   NaN   \n",
       "5   tune_hominid_2e9a0_00049  finished     -  ckochath   NaN   \n",
       "6   tune_hominid_2e9a0_00020  finished     -  ckochath   NaN   \n",
       "7   tune_hominid_2e9a0_00006  finished     -  ckochath   NaN   \n",
       "8   tune_hominid_2e9a0_00026  finished     -  ckochath   NaN   \n",
       "9   tune_hominid_2e9a0_00016  finished     -  ckochath   NaN   \n",
       "10  tune_hominid_2e9a0_00037  finished     -  ckochath   NaN   \n",
       "11  tune_hominid_2e9a0_00003  finished     -  ckochath   NaN   \n",
       "12  tune_hominid_2e9a0_00004  finished     -  ckochath   NaN   \n",
       "13  tune_hominid_2e9a0_00014  finished     -  ckochath   NaN   \n",
       "14  tune_hominid_2e9a0_00031  finished     -  ckochath   NaN   \n",
       "15  tune_hominid_2e9a0_00011  finished     -  ckochath   NaN   \n",
       "16  tune_hominid_2e9a0_00025  finished     -  ckochath   NaN   \n",
       "17  tune_hominid_2e9a0_00008  finished     -  ckochath   NaN   \n",
       "18  tune_hominid_2e9a0_00038  finished     -  ckochath   NaN   \n",
       "19  tune_hominid_2e9a0_00042  finished     -  ckochath   NaN   \n",
       "20  tune_hominid_2e9a0_00045  finished     -  ckochath   NaN   \n",
       "21  tune_hominid_2e9a0_00047  finished     -  ckochath   NaN   \n",
       "22  tune_hominid_2e9a0_00018  finished     -  ckochath   NaN   \n",
       "23  tune_hominid_2e9a0_00021  finished     -  ckochath   NaN   \n",
       "24  tune_hominid_2e9a0_00032  finished     -  ckochath   NaN   \n",
       "25  tune_hominid_2e9a0_00019  finished     -  ckochath   NaN   \n",
       "26  tune_hominid_2e9a0_00043  finished     -  ckochath   NaN   \n",
       "27  tune_hominid_2e9a0_00012  finished     -  ckochath   NaN   \n",
       "28  tune_hominid_2e9a0_00017  finished     -  ckochath   NaN   \n",
       "29  tune_hominid_2e9a0_00040  finished     -  ckochath   NaN   \n",
       "30  tune_hominid_2e9a0_00048  finished     -  ckochath   NaN   \n",
       "31  tune_hominid_2e9a0_00044  finished     -  ckochath   NaN   \n",
       "32  tune_hominid_2e9a0_00001  finished     -  ckochath   NaN   \n",
       "33  tune_hominid_2e9a0_00024  finished     -  ckochath   NaN   \n",
       "34  tune_hominid_2e9a0_00041  finished     -  ckochath   NaN   \n",
       "35  tune_hominid_2e9a0_00039  finished     -  ckochath   NaN   \n",
       "36  tune_hominid_2e9a0_00028  finished     -  ckochath   NaN   \n",
       "37  tune_hominid_2e9a0_00029  finished     -  ckochath   NaN   \n",
       "38  tune_hominid_2e9a0_00035  finished     -  ckochath   NaN   \n",
       "39  tune_hominid_2e9a0_00023  finished     -  ckochath   NaN   \n",
       "40  tune_hominid_2e9a0_00002  finished     -  ckochath   NaN   \n",
       "41  tune_hominid_2e9a0_00034  finished     -  ckochath   NaN   \n",
       "42  tune_hominid_2e9a0_00030  finished     -  ckochath   NaN   \n",
       "43  tune_hominid_2e9a0_00015  finished     -  ckochath   NaN   \n",
       "44  tune_hominid_2e9a0_00027  finished     -  ckochath   NaN   \n",
       "45  tune_hominid_2e9a0_00010  finished     -  ckochath   NaN   \n",
       "46  tune_hominid_2e9a0_00033  finished     -  ckochath   NaN   \n",
       "47  tune_hominid_2e9a0_00009  finished     -  ckochath   NaN   \n",
       "48  tune_hominid_2e9a0_00013  finished     -  ckochath   NaN   \n",
       "49  tune_hominid_2e9a0_00000  finished     -  ckochath   NaN   \n",
       "\n",
       "                     Created  Runtime  Sweep conv1_activation  \\\n",
       "0   2023-04-06T18:46:14.000Z     2557    NaN             relu   \n",
       "1   2023-04-07T08:13:10.000Z     1443    NaN             relu   \n",
       "2   2023-04-07T00:49:25.000Z     4241    NaN             relu   \n",
       "3   2023-04-06T19:56:51.000Z     3070    NaN             relu   \n",
       "4   2023-04-07T04:54:07.000Z     4173    NaN             relu   \n",
       "5   2023-04-07T08:54:30.000Z     1611    NaN             relu   \n",
       "6   2023-04-06T23:52:14.000Z     2829    NaN             relu   \n",
       "7   2023-04-06T19:28:54.000Z     1674    NaN             relu   \n",
       "8   2023-04-07T02:32:52.000Z     2182    NaN             relu   \n",
       "9   2023-04-06T22:45:32.000Z     1813    NaN             relu   \n",
       "10  2023-04-07T06:03:43.000Z     1527    NaN             relu   \n",
       "11  2023-04-06T17:06:21.000Z     4684    NaN             relu   \n",
       "12  2023-04-06T18:24:28.000Z     1303    NaN      exponential   \n",
       "13  2023-04-06T22:20:29.000Z      627    NaN             relu   \n",
       "14  2023-04-07T03:47:20.000Z      569    NaN             relu   \n",
       "15  2023-04-06T21:39:01.000Z      976    NaN             relu   \n",
       "16  2023-04-07T02:24:36.000Z      494    NaN             relu   \n",
       "17  2023-04-06T20:48:04.000Z      760    NaN             relu   \n",
       "18  2023-04-07T06:29:13.000Z      577    NaN             relu   \n",
       "19  2023-04-07T07:15:51.000Z      519    NaN             relu   \n",
       "20  2023-04-07T07:55:34.000Z     1053    NaN             relu   \n",
       "21  2023-04-07T08:37:16.000Z      588    NaN             relu   \n",
       "22  2023-04-06T23:26:32.000Z      589    NaN             relu   \n",
       "23  2023-04-07T00:39:27.000Z      594    NaN             relu   \n",
       "24  2023-04-07T03:56:52.000Z     1051    NaN             relu   \n",
       "25  2023-04-06T23:36:24.000Z      947    NaN      exponential   \n",
       "26  2023-04-07T07:24:34.000Z     1195    NaN             relu   \n",
       "27  2023-04-06T21:55:21.000Z      590    NaN      exponential   \n",
       "28  2023-04-06T23:15:48.000Z      641    NaN             relu   \n",
       "29  2023-04-07T06:47:39.000Z      585    NaN      exponential   \n",
       "30  2023-04-07T08:47:08.000Z      438    NaN      exponential   \n",
       "31  2023-04-07T07:44:32.000Z      659    NaN      exponential   \n",
       "32  2023-04-06T16:18:56.000Z     1615    NaN      exponential   \n",
       "33  2023-04-07T02:09:39.000Z      894    NaN      exponential   \n",
       "34  2023-04-07T06:57:28.000Z     1099    NaN      exponential   \n",
       "35  2023-04-07T06:38:53.000Z      523    NaN      exponential   \n",
       "36  2023-04-07T03:19:47.000Z      530    NaN      exponential   \n",
       "37  2023-04-07T03:28:39.000Z      554    NaN      exponential   \n",
       "38  2023-04-07T04:45:40.000Z      505    NaN      exponential   \n",
       "39  2023-04-07T02:00:09.000Z      567    NaN      exponential   \n",
       "40  2023-04-06T16:45:54.000Z     1224    NaN      exponential   \n",
       "41  2023-04-07T04:30:49.000Z      889    NaN      exponential   \n",
       "42  2023-04-07T03:37:56.000Z      562    NaN      exponential   \n",
       "43  2023-04-06T22:30:59.000Z      869    NaN      exponential   \n",
       "44  2023-04-07T03:09:17.000Z      627    NaN      exponential   \n",
       "45  2023-04-06T21:15:14.000Z     1425    NaN      exponential   \n",
       "46  2023-04-07T04:14:26.000Z      980    NaN      exponential   \n",
       "47  2023-04-06T21:00:48.000Z      862    NaN      exponential   \n",
       "48  2023-04-06T22:05:15.000Z      911    NaN      exponential   \n",
       "49  2023-04-06T15:32:05.000Z     2808    NaN      exponential   \n",
       "\n",
       "    conv1_batchnorm  ...                                     trial_log_path  \\\n",
       "0              True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "1             False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "2             False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "3             False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "4             False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "5              True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "6              True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "7              True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "8              True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "9             False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "10             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "11             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "12            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "13             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "14             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "15             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "16            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "17            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "18            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "19             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "20            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "21             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "22            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "23            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "24            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "25            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "26             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "27            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "28             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "29            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "30            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "31            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "32             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "33            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "34            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "35             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "36             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "37             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "38            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "39             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "40             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "41            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "42             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "43             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "44             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "45            False  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "46             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "47             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "48             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "49             True  ...  /home/chandana/ray_results/tune_hominid/tune_h...   \n",
       "\n",
       "    iterations_since_restore  pearson_r  time_since_restore  time_this_iter_s  \\\n",
       "0                         60   0.888635         2550.752711         42.304018   \n",
       "1                         60   0.888559         1436.601748         23.748997   \n",
       "2                         60   0.886563         4234.652682         70.478082   \n",
       "3                         60   0.885639         3064.092025         50.814463   \n",
       "4                         60   0.885317         4166.478738         69.190704   \n",
       "5                         60   0.885305         1604.369986         26.542195   \n",
       "6                         60   0.881746         2823.065363         46.830002   \n",
       "7                         60   0.878790         1667.439738         27.505669   \n",
       "8                         60   0.876017         2174.928653         36.118381   \n",
       "9                         60   0.873120         1806.488476         29.868063   \n",
       "10                        60   0.872167         1520.935762         25.124688   \n",
       "11                        60   0.869140         4676.966161         77.622760   \n",
       "12                        60   0.843212         1298.102059         21.565726   \n",
       "13                        20   0.826601          620.144239         30.410017   \n",
       "14                        20   0.825475          562.840743         27.692188   \n",
       "15                        20   0.824746          969.997311         47.947724   \n",
       "16                        20   0.823369          487.806197         23.776015   \n",
       "17                        20   0.820063          753.712867         37.318596   \n",
       "18                        20   0.818451          570.872665         27.958805   \n",
       "19                        20   0.818424          513.100456         25.082859   \n",
       "20                        20   0.815022         1046.768218         51.672004   \n",
       "21                        20   0.812648          580.902604         28.531430   \n",
       "22                        20   0.810611          582.539415         28.513494   \n",
       "23                        20   0.807471          588.742311         28.904625   \n",
       "24                        20   0.801176         1045.627580         51.477784   \n",
       "25                        20   0.779853          939.758700         46.244713   \n",
       "26                        20   0.775980         1188.448543         58.768454   \n",
       "27                        20   0.773541          583.721735         28.622427   \n",
       "28                        20   0.772441          634.796849         31.141842   \n",
       "29                        20   0.771202          579.572993         28.427624   \n",
       "30                        20   0.769857          432.248281         21.127588   \n",
       "31                        20   0.767811          651.651195         32.042486   \n",
       "32                        60   0.647773         1608.184009         26.622830   \n",
       "33                        20   0.643570          887.164389         43.533321   \n",
       "34                        20   0.620266         1093.643809         53.500575   \n",
       "35                        20   0.596205          516.120235         25.335435   \n",
       "36                        20   0.549606          523.296041         25.595889   \n",
       "37                        20   0.508631          547.702003         26.876401   \n",
       "38                        20   0.496781          498.477462         24.476357   \n",
       "39                        20   0.471141          560.938578         27.535063   \n",
       "40                        20   0.454841         1217.675331         60.111696   \n",
       "41                        20   0.390706          882.185165         43.338058   \n",
       "42                        20   0.389506          555.786399         27.167193   \n",
       "43                        20   0.377785          862.685724         42.455498   \n",
       "44                        20   0.319371          620.068781         30.472592   \n",
       "45                        20   0.306518         1418.605744         70.340443   \n",
       "46                        20   0.290626          973.195126         48.318849   \n",
       "47                        20   0.281040          856.980942         42.365385   \n",
       "48                        20   0.062240          904.774534         44.663335   \n",
       "49                        60        NaN         2801.542135         46.035861   \n",
       "\n",
       "   time_total_s   timestamp timesteps_since_restore training_iteration  \\\n",
       "0   2550.752711  1680809328                       0                 60   \n",
       "1   1436.601748  1680856630                       0                 60   \n",
       "2   4234.652682  1680832803                       0                 60   \n",
       "3   3064.092025  1680814079                       0                 60   \n",
       "4   4166.478738  1680847417                       0                 60   \n",
       "5   1604.369986  1680859277                       0                 60   \n",
       "6   2823.065363  1680827960                       0                 60   \n",
       "7   1667.439738  1680811005                       0                 60   \n",
       "8   2174.928653  1680836951                       0                 60   \n",
       "9   1806.488476  1680822941                       0                 60   \n",
       "10  1520.935762  1680848947                       0                 60   \n",
       "11  4676.966161  1680805461                       0                 60   \n",
       "12  1298.102059  1680806769                       0                 60   \n",
       "13   620.144239  1680820253                       0                 20   \n",
       "14   562.840743  1680839807                       0                 20   \n",
       "15   969.997311  1680818115                       0                 20   \n",
       "16   487.806197  1680834767                       0                 20   \n",
       "17   753.712867  1680814841                       0                 20   \n",
       "18   570.872665  1680849528                       0                 20   \n",
       "19   513.100456  1680852267                       0                 20   \n",
       "20  1046.768218  1680855184                       0                 20   \n",
       "21   580.902604  1680857221                       0                 20   \n",
       "22   582.539415  1680824178                       0                 20   \n",
       "23   588.742311  1680828559                       0                 20   \n",
       "24  1045.627580  1680840861                       0                 20   \n",
       "25   939.758700  1680825128                       0                 20   \n",
       "26  1188.448543  1680853466                       0                 20   \n",
       "27   583.721735  1680818708                       0                 20   \n",
       "28   634.796849  1680823586                       0                 20   \n",
       "29   579.572993  1680850642                       0                 20   \n",
       "30   432.248281  1680857663                       0                 20   \n",
       "31   651.651195  1680854127                       0                 20   \n",
       "32  1608.184009  1680799548                       0                 60   \n",
       "33   887.164389  1680834269                       0                 20   \n",
       "34  1093.643809  1680851745                       0                 20   \n",
       "35   516.120235  1680850053                       0                 20   \n",
       "36   523.296041  1680838114                       0                 20   \n",
       "37   547.702003  1680838670                       0                 20   \n",
       "38   498.477462  1680843242                       0                 20   \n",
       "39   560.938578  1680833373                       0                 20   \n",
       "40  1217.675331  1680800775                       0                 20   \n",
       "41   882.185165  1680842734                       0                 20   \n",
       "42   555.786399  1680839235                       0                 20   \n",
       "43   862.685724  1680821126                       0                 20   \n",
       "44   620.068781  1680837580                       0                 20   \n",
       "45  1418.605744  1680817136                       0                 20   \n",
       "46   973.195126  1680841842                       0                 20   \n",
       "47   856.980942  1680815708                       0                 20   \n",
       "48   904.774534  1680819623                       0                 20   \n",
       "49  2801.542135  1680797930                       0                 60   \n",
       "\n",
       "    warmup_time  \n",
       "0      0.005102  \n",
       "1      0.005109  \n",
       "2      0.005227  \n",
       "3      0.005103  \n",
       "4      0.004926  \n",
       "5      0.006063  \n",
       "6      0.005236  \n",
       "7      0.004855  \n",
       "8      0.005168  \n",
       "9      0.004921  \n",
       "10     0.005152  \n",
       "11     0.004710  \n",
       "12     0.005467  \n",
       "13     0.006064  \n",
       "14     0.005155  \n",
       "15     0.006332  \n",
       "16     0.006481  \n",
       "17     0.005258  \n",
       "18     0.005084  \n",
       "19     0.004951  \n",
       "20     0.004926  \n",
       "21     0.005134  \n",
       "22     0.004910  \n",
       "23     0.005859  \n",
       "24     0.004680  \n",
       "25     0.004671  \n",
       "26     0.005352  \n",
       "27     0.005033  \n",
       "28     0.004628  \n",
       "29     0.005049  \n",
       "30     0.005467  \n",
       "31     0.005346  \n",
       "32     0.004786  \n",
       "33     0.005114  \n",
       "34     0.005296  \n",
       "35     0.004782  \n",
       "36     0.005006  \n",
       "37     0.004616  \n",
       "38     0.004631  \n",
       "39     0.005087  \n",
       "40     0.006577  \n",
       "41     0.005314  \n",
       "42     0.004510  \n",
       "43     0.005101  \n",
       "44     0.005140  \n",
       "45     0.004859  \n",
       "46     0.004810  \n",
       "47     0.007184  \n",
       "48     0.004964  \n",
       "49     0.004841  \n",
       "\n",
       "[50 rows x 46 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis = pd.read_csv('/home/chandana/projects/hominid/results/sweep_table.csv')\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b813285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chandana/ray_results/tune_hominid/tune_hominid_2e9a0_00005_5_conv1_activation=relu,conv1_batchnorm=True,conv1_channel_weight=se,conv1_filters=256,conv1_kernel_size=_2023-04-06_14-46-12\n",
      "\n",
      "\n",
      "0.8886346220970154\n",
      "\n",
      "\n",
      "/home/chandana/ray_results/tune_hominid/tune_hominid_2e9a0_00046_46_conv1_activation=relu,conv1_batchnorm=False,conv1_channel_weight=se,conv1_filters=128,conv1_kernel_siz_2023-04-07_04-13-09\n",
      "\n",
      "\n",
      "0.8885594606399536\n",
      "\n",
      "\n",
      "/home/chandana/ray_results/tune_hominid/tune_hominid_2e9a0_00022_22_conv1_activation=relu,conv1_batchnorm=False,conv1_channel_weight=se,conv1_filters=512,conv1_kernel_siz_2023-04-06_20-49-23\n",
      "\n",
      "\n",
      "0.886563241481781\n",
      "\n",
      "\n",
      "/home/chandana/ray_results/tune_hominid/tune_hominid_2e9a0_00007_7_conv1_activation=relu,conv1_batchnorm=False,conv1_channel_weight=se,conv1_filters=512,conv1_kernel_size_2023-04-06_15-56-50\n",
      "\n",
      "\n",
      "0.8856391906738281\n",
      "\n",
      "\n",
      "/home/chandana/ray_results/tune_hominid/tune_hominid_2e9a0_00036_36_conv1_activation=relu,conv1_batchnorm=False,conv1_channel_weight=se,conv1_filters=512,conv1_kernel_siz_2023-04-07_00-54-06\n",
      "\n",
      "\n",
      "0.8853170275688171\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(analysis.loc[i]['trial_log_path'])\n",
    "    print(\"\\n\")\n",
    "    print(analysis.loc[i]['pearson_r'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9ff001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1_activation': 'relu',\n",
       " 'conv1_batchnorm': True,\n",
       " 'conv1_channel_weight': 'se',\n",
       " 'conv1_dropout': 0.2,\n",
       " 'conv1_filters': 256,\n",
       " 'conv1_kernel_size': 19,\n",
       " 'conv1_max_pool': 10,\n",
       " 'conv1_pool_type': 'attention',\n",
       " 'conv1_type': 'pw',\n",
       " 'dense_activation': 'relu',\n",
       " 'dense_batchnorm': True,\n",
       " 'dense_dropout': [0.3, 0.3],\n",
       " 'dense_units': [512, 512],\n",
       " 'input_shape': None,\n",
       " 'mha_d_model': 96,\n",
       " 'mha_dropout': 0.1,\n",
       " 'mha_head_type': 'pool',\n",
       " 'mha_heads': 8,\n",
       " 'mha_layernorm': False,\n",
       " 'mha_pool_type': 'attention',\n",
       " 'output_activation': 'linear',\n",
       " 'output_shape': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_path = \"/home/chandana/ray_results/tune_hominid/tune_hominid_2e9a0_00005_5_conv1_activation=relu,conv1_batchnorm=True,conv1_channel_weight=se,conv1_filters=256,conv1_kernel_size=_2023-04-06_14-46-12\"\n",
    "json.load(open(f\"{params_path}/params.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data =  {\n",
    "        \"num_epochs\": 100,\n",
    "        \"batch_size\": 128,\n",
    "        \"lr\": 1e-3,\n",
    "        'augmentations': params['augmentations'],\n",
    "        \"wandb_log\": True,\n",
    "        \"wandb_project\": \"hominid_hepg2-v2\",\n",
    "        \"wandb_run_name\": path,\n",
    "        \"save_path\": save_path,\n",
    "        \"framework\": 'tensorflow',\n",
    "        \"dataset\": \"hepg2\",\n",
    "        \n",
    "        \"model_fn\": {\n",
    "            'conv1_activation': params[\"conv1_activation\"],\n",
    "            'conv1_batchnorm': False,\n",
    "            'conv1_dropout': 0.2,\n",
    "            'conv1_filters': 128,\n",
    "            'conv1_kernel_size': 15,\n",
    "            'conv1_pool_type': 'attention',\n",
    "            'conv1_max_pool': 10,\n",
    "            'conv1_type': params['conv1_type'],\n",
    "            'conv1_channel_weight': params['conv1_channel_weight'],\n",
    "            'dense_activation': 'relu',\n",
    "            'dense_batchnorm': True,\n",
    "            'dense_dropout': [0.4, 0.4],\n",
    "            'dense_units': [256, 256],\n",
    "            'input_shape': None,\n",
    "            'mha_d_model': 96,\n",
    "            'mha_dropout': 0.1,\n",
    "            'mha_heads': 4,\n",
    "            'mha_layernorm': False,\n",
    "            'mha_pool_type': params['mha_pool_type'],\n",
    "            'mha_head_type': params['mha_head_type'],\n",
    "            'output_activation': \"linear\",\n",
    "            'output_shape': None,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c1a7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb47b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c69532a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chandana/ray_results/tune_hominid/tune_hominid_2e9a0_00005_5_conv1_activation=relu,conv1_batchnorm=True,conv1_channel_weight=se,conv1_filters=256,conv1_kernel_size=_2023-04-06_14-46-12'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis = analysis.drop(columns=['Name', 'State', 'Notes', 'User', 'Tags', 'Created', 'Runtime', 'Sweep', 'hostname', \n",
    "                      'node_ip', 'pid', 'trial_id', 'iterations_since_restore', 'time_since_restore', 'time_this_iter_s', 'time_total_s', 'timestamp',\n",
    "       'timesteps_since_restore', 'training_iteration', 'warmup_time', 'date'])\n",
    "\n",
    "analysis.loc[0]['trial_log_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a9ddcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1_activation': <ray.tune.search.sample.Categorical at 0x7f1bb9478750>,\n",
       " 'conv1_batchnorm': <ray.tune.search.sample.Categorical at 0x7f1bb9478790>,\n",
       " 'conv1_channel_weight': <ray.tune.search.sample.Categorical at 0x7f1bb9478810>,\n",
       " 'conv1_dropout': 0.2,\n",
       " 'conv1_filters': <ray.tune.search.sample.Categorical at 0x7f1bb94788d0>,\n",
       " 'conv1_kernel_size': <ray.tune.search.sample.Categorical at 0x7f1bb9478950>,\n",
       " 'conv1_max_pool': 10,\n",
       " 'conv1_pool_type': 'attention',\n",
       " 'conv1_type': 'pw',\n",
       " 'dense_activation': 'relu',\n",
       " 'dense_batchnorm': True,\n",
       " 'dense_dropout': <ray.tune.search.sample.Categorical at 0x7f1bb94789d0>,\n",
       " 'dense_units': <ray.tune.search.sample.Categorical at 0x7f1bb9478a90>,\n",
       " 'input_shape': None,\n",
       " 'mha_d_model': <ray.tune.search.sample.Categorical at 0x7f1bb9478b10>,\n",
       " 'mha_dropout': 0.1,\n",
       " 'mha_head_type': 'pool',\n",
       " 'mha_heads': <ray.tune.search.sample.Categorical at 0x7f1bb9478b90>,\n",
       " 'mha_layernorm': False,\n",
       " 'mha_pool_type': 'attention',\n",
       " 'output_activation': 'linear',\n",
       " 'output_shape': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61763baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conv1_activation', 'conv1_batchnorm', 'conv1_channel_weight',\n",
       "       'conv1_dropout', 'conv1_filters', 'conv1_kernel_size', 'conv1_max_pool',\n",
       "       'conv1_pool_type', 'conv1_type', 'dense_activation', 'dense_batchnorm',\n",
       "       'dense_dropout', 'dense_units', 'experiment_id', 'input_shape',\n",
       "       'mha_d_model', 'mha_dropout', 'mha_head_type', 'mha_heads',\n",
       "       'mha_layernorm', 'mha_pool_type', 'output_activation', 'output_shape',\n",
       "       'trial_log_path', 'pearson_r'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e6265b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            relu\n",
       "1            relu\n",
       "2            relu\n",
       "3            relu\n",
       "4            relu\n",
       "5            relu\n",
       "6            relu\n",
       "7            relu\n",
       "8            relu\n",
       "9            relu\n",
       "10           relu\n",
       "11           relu\n",
       "12    exponential\n",
       "13           relu\n",
       "14           relu\n",
       "15           relu\n",
       "16           relu\n",
       "17           relu\n",
       "18           relu\n",
       "19           relu\n",
       "20           relu\n",
       "21           relu\n",
       "22           relu\n",
       "23           relu\n",
       "24           relu\n",
       "25    exponential\n",
       "26           relu\n",
       "27    exponential\n",
       "28           relu\n",
       "29    exponential\n",
       "30    exponential\n",
       "31    exponential\n",
       "32    exponential\n",
       "33    exponential\n",
       "34    exponential\n",
       "35    exponential\n",
       "36    exponential\n",
       "37    exponential\n",
       "38    exponential\n",
       "39    exponential\n",
       "40    exponential\n",
       "41    exponential\n",
       "42    exponential\n",
       "43    exponential\n",
       "44    exponential\n",
       "45    exponential\n",
       "46    exponential\n",
       "47    exponential\n",
       "48    exponential\n",
       "49    exponential\n",
       "Name: conv1_activation, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis['conv1_activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "746be3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='conv1_activation', ylabel='pearson_r'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# analysis[analysis[\"conv1_channel_weight\"] == weight]\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "analysis.plot.scatter(\n",
    "    x='conv1_activation',\n",
    "    y='pearson_r',\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "deb9351d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_145218/4013634541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conv1_batchnorm\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conv1_activation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exponential\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearson_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conv1_activation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearson_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot scatterplots\n",
    "\n",
    "\n",
    "ACTIVATIONS = ['relu','exponential']\n",
    "BN = [True, False]\n",
    "conv1_filters = [128, 192, 256, 512]\n",
    "conv1_channel_weight = [\"softconv\", \"se\"]\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "for k, weight in enumerate(conv1_channel_weight):\n",
    "\n",
    "    res = analysis[analysis[\"conv1_channel_weight\"] == weight]\n",
    "\n",
    "    for i, filters in enumerate(conv1_filters):\n",
    "\n",
    "        r1 = res[res[\"conv1_filters\"] == filters]\n",
    "\n",
    "        for j, bn in enumerate(BN):\n",
    "\n",
    "            d1 = r1[r1[\"conv1_batchnorm\"] == bn]\n",
    "\n",
    "            x = d1[d1[\"conv1_activation\"] == \"exponential\"].pearson_r.to_numpy()[0]\n",
    "            y = d1[d1[\"conv1_activation\"] == \"relu\"].pearson_r.to_numpy()[0]\n",
    "                       \n",
    "            if ((k == 0) and (j == 0)):\n",
    "                label = filters\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            plt.plot(x, y, '.', color=\"blue\", label=None, markersize=17, alpha=0.3)\n",
    "#             plt.plot(x1_rel, y1_rel, '>', color=\"blue\", label=None, markersize=12, alpha=0.3)\n",
    "            \n",
    "# x = entropy_df[entropy_df[\"index\"] == 266].entropy.to_numpy()\n",
    "# y = entropy_df[entropy_df[\"index\"] == 266].snr.to_numpy()\n",
    "# plt.plot(x, y, '.', color=\"red\", label=None, markersize=20, alpha=1) #alpha=0.5\n",
    "\n",
    "# x = entropy_df[entropy_df[\"index\"] == 166].entropy.to_numpy()\n",
    "# y = entropy_df[entropy_df[\"index\"] == 166].snr.to_numpy()\n",
    "# plt.plot(x, y, '>', color=\"orange\", label=None, markersize=20, alpha=1) #alpha=0.5\n",
    "\n",
    "ax.set_xlabel(\"KLD (k-attr-mean)\", fontsize=15)\n",
    "ax.set_ylabel(\"Attribution SNR\", fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0, 7)\n",
    "plt.tight_layout();\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd8c4bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64777267, 0.31937146])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66d31a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                                                 tune_hominid_2e9a0_00005\n",
       "State                                                                finished\n",
       "Notes                                                                       -\n",
       "User                                                                 ckochath\n",
       "Tags                                                                      NaN\n",
       "Created                                              2023-04-06T18:46:14.000Z\n",
       "Runtime                                                                  2557\n",
       "Sweep                                                                     NaN\n",
       "conv1_activation                                                         relu\n",
       "conv1_batchnorm                                                          True\n",
       "conv1_channel_weight                                                       se\n",
       "conv1_dropout                                                             0.2\n",
       "conv1_filters                                                             256\n",
       "conv1_kernel_size                                                          19\n",
       "conv1_max_pool                                                             10\n",
       "conv1_pool_type                                                     attention\n",
       "conv1_type                                                                 pw\n",
       "date                                                      2023-04-06_15-28-48\n",
       "dense_activation                                                         relu\n",
       "dense_batchnorm                                                          True\n",
       "dense_dropout                                                       [0.3,0.3]\n",
       "dense_units                                                         [512,512]\n",
       "experiment_id                                7e8c709550564661b8b94d17d81d8a5f\n",
       "hostname                                                                citra\n",
       "input_shape                                                           [230,4]\n",
       "mha_d_model                                                                96\n",
       "mha_dropout                                                               0.1\n",
       "mha_head_type                                                            pool\n",
       "mha_heads                                                                   8\n",
       "mha_layernorm                                                           False\n",
       "mha_pool_type                                                       attention\n",
       "node_ip                                                         143.48.44.146\n",
       "output_activation                                                      linear\n",
       "output_shape                                                                1\n",
       "pid                                                                    163917\n",
       "trial_id                                                          2e9a0_00005\n",
       "trial_log_path              /home/chandana/ray_results/tune_hominid/tune_h...\n",
       "iterations_since_restore                                                   60\n",
       "pearson_r                                                            0.888635\n",
       "time_since_restore                                                2550.752711\n",
       "time_this_iter_s                                                    42.304018\n",
       "time_total_s                                                      2550.752711\n",
       "timestamp                                                          1680809328\n",
       "timesteps_since_restore                                                     0\n",
       "training_iteration                                                         60\n",
       "warmup_time                                                          0.005102\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.sort_values('pearson_r', ascending=False).loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a460e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
